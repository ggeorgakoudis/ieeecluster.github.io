<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd"><html><head><meta charset="utf-8" /><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width, initial-scale=1"><title>IEEE Cluster 2020 Program</title><link href="includes/css/jquery-ui.css" rel="stylesheet" type="text/css" /><link href="includes/css/shared_styles.css" rel="stylesheet" type="text/css" /><link href="includes/css/block_styles.css?v=1" rel="stylesheet" type="text/css" /><link href="includes/css/jquery.qtip.min.css" rel="stylesheet" type="text/css" /><link href="includes/css/font-awesome-4.1.0/css/font-awesome.min.css" rel="stylesheet" type="text/css" /><link href="includes/css/user_generated.css" rel="stylesheet" type="text/css" /><link href="archive_styles.css" rel="stylesheet" type="text/css" /><style>

        .tabs {
            background: #e8ca9b;
        }
        .tabs .divider,
        .tabs .bg_tab {
            background-color: #e8ca9b;
            border-top-color: #e8ca9b;
            color: #000000;
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
            text-transform: none;
        }
        .tabs .fg_tab {
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
            color: #CC1A1A;
            text-transform: none;
        }
        .tab_menu_label, .tab_no_menu_label {
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
        }
        .qtip.qtip-rm-tab-menu {
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
        }
        


        .filter_bar,
        .filter_bar_w_legend {
            background-color: #C9DDF9;
        }
        .filter_bar_w_legend .instr,
        .filter_bar .instr {
            background-color: #A2C2FC;
        }
        


        .role_stype_bar {
            background-color: #E0AB76;
            color: #000000;
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
            text-transform: none;
        }
        


        #footer {
            background-color: #EAEAEA;
            color: #999999;
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
            text-transform: none;
        }
        #footer a {
            color: #777777;
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
            text-transform: none;
        }
        


        div.banner_top,
        div.banner_top .site_title,
        div.banner_top .no_logo_banner_right,
        div.logo_banner,
        div.logo_banner .user_name,
        #header {
            background-color: #0066CC;
            color: #FFFFFF;
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
            text-transform: none;
        }

        div.logo_banner .site_title a,
        div.banner_top .site_title a,
        #header #site_title a {
            color: #FFFFFF;
            text-decoration: none;
        }
        


        a:link,
        a:visited,
        a:active,
        .clickable,
        a.clickable,
        a.clickable:link,
        a.clickable:visited,
        a.clickable:active,
        .ttip_object_info_blue,
        .ttip_object_info_blue_no_clone,
        .ttip_object_info_blue_wide,
        .ttip_object_info_blue_wide_no_clone,
        .ttip_object_info_blue_very_wide,
        .ttip_object_info_blue_very_wide_no_clone,
        .ttip_object_info_blue_extra_wide,
        .ttip_object_info_blue_extra_wide_no_clone,
        .ttip_object_info_blue_modal,
        .ttip_object_info_blue_modal_no_clone,
        .colorbox_object_info,
        span.menu_item_label,
        .page_box_print .contents A,
        .page_box_print #footer a   { color: #0000EE; }

        /* Light Links */
        .light_link a,
        .light_arrow,
        .light_link a:link,
        .light_link a:active,
        .light_link a:visited,
        .light_clickable,
        a.light_clickable,
        a.light_clickable:link,
        a.light_clickable:active,
        a.light_clickable:visited   { color: #5088F0; }

        /* user hovers     */
        a:hover,
        .light_link a:hover,
        .light_arrow:hover,
        .light_clickable:hover,
        a.light_clickable:hover,
        .hover_link:hover,
        .ttip_object_info_blue:hover,
        .ttip_object_info_blue_no_clone:hover,
        .ttip_object_info_blue_wide:hover,
        .ttip_object_info_blue_wide_no_clone:hover,
        .ttip_object_info_blue_very_wide:hover,
        .ttip_object_info_blue_very_wide_no_clone:hover,
        .ttip_object_info_blue_extra_wide:hover,
        .ttip_object_info_blue_extra_wide_no_clone:hover,
        .ttip_object_info_blue_modal:hover,
        .ttip_object_info_blue_modal_no_clone:hover,
        .ttip_object_info:hover,
        .ttip_object_info_no_clone:hover,
        .ttip_object_info_wide:hover,
        .ttip_object_info_wide_no_clone:hover,
        .ttip_object_info_very_wide:hover,
        .ttip_object_info_very_wide_no_clone:hover,
        .ttip_object_info_extra_wide:hover,
        .ttip_object_info_extra_wide_no_clone:hover,
        .ttip_object_info_modal:hover,
        .ttip_object_info_modal_no_clone:hover,
        .colorbox_object_info:hover,
        .subtabs .fg_tab:hover div,
        .subtabs .fg_tab:hover A,
        .subtabs .bg_tab:hover,
        .subtabs .bg_tab:hover A        { color: #0000EE; }

        ul.rm_mega_menu li.mega > div,
        ul.rm_mega_menu > li.mega-link > a:hover,
        .disp_details_header,
        .disp_details_sub_header,
        .disp_details I,    /* This is deprecated, since it clashes with font awesome using I tags. */
        .disp_red,
        .disp_label {
            color: #B32626;
        }

        div.active_toggle_button {
            background-color: #5088F0;
        }


        /*Default button*/
        :not(.mce-btn):not(.mce-window-head):not(.ui-datepicker-buttonpane) > button,
        input[type=button],
        input[type=reset],
        input[type=submit] {
            border-color: #0000EE;
            color: #0000EE;
        }

        /*Big, save buttons*/
        :not(.mce-btn):not(.mce-window-head):not(.ui-datepicker-buttonpane) > button.big-button,
        input[type=button].big_button-button,
        input[type=reset].big-button,
        input[type=submit].big-button,
        button.save-button,
        input[type=button].save-button,
        input[type=reset].save-button,
        input[type=submit].save-button {
            background-color: #0000EE;
        }

        /*Light button*/
        :not(.mce-btn):not(.mce-window-head):not(.ui-datepicker-buttonpane) > button.light-button,
        input[type=button].light-button,
        input[type=reset].light-button,
        input[type=submit].light-button {
            border-color: #5088F0;
            color: #5088F0;
        }

        /*Light-save button*/
        :not(.mce-btn):not(.mce-window-head):not(.ui-datepicker-buttonpane) > button.light-save-button,
        input[type=button].light-save-button,
        input[type=reset].light-save-button,
        input[type=submit].light-save-button {
            border-color: #5088F0;
            background-color: #5088F0;
        }

        /*Default button hover*/
        :not(.mce-btn):not(.mce-window-head):not(.ui-datepicker-buttonpane) > button:hover,
        input[type=button]:hover,
        input[type=reset]:hover,
        input[type=submit]:hover {
            background-color: #0000EE;
        }

        /*Light button hover*/
        :not(.mce-btn):not(.mce-window-head):not(.ui-datepicker-buttonpane) > button:hover.light-button,
        input[type=button]:hover.light-button,
        input[type=reset]:hover.light-button,
        input[type=submit]:hover.light-button {
            background-color: #5088F0;
        }

        /*Default button disabled*/
        :not(.mce-btn):not(.mce-window-head):not(.ui-datepicker-buttonpane) > button[disabled],
        button[disabled]:hover,
        button[disabled]:active,
        input[type=button][disabled],
        input[type=button][disabled]:hover,
        input[type=button][disabled]:active,
        input[type=reset][disabled],
        input[type=reset][disabled]:hover,
        input[type=reset][disabled]:active,
        input[type=submit][disabled],
        input[type=submit][disabled]:hover,
        input[type=submit][disabled]:active {
            color: #0000EE;
        }

        /*Big, save buttons disabled*/
        :not(.mce-btn):not(.mce-window-head):not(.ui-datepicker-buttonpane) > button[disabled].big-button,
        button[disabled]:hover.big-button,
        button[disabled]:active.big-button,
        input[type=button][disabled].big-button,
        input[type=button][disabled]:hover.big-button,
        input[type=button][disabled]:active.big-button,
        input[type=reset][disabled].big-button,
        input[type=reset][disabled]:hover.big-button,
        input[type=reset][disabled]:active.big-button,
        input[type=submit][disabled].big-button,
        input[type=submit][disabled]:hover.big-button,
        input[type=submit][disabled]:active.big-button,
        button[disabled].save-button,
        button[disabled]:hover.save-button,
        button[disabled]:active.save-button,
        input[type=button][disabled].save-button,
        input[type=button][disabled]:hover.save-button,
        input[type=button][disabled]:active.save-button,
        input[type=reset][disabled].save-button,
        input[type=reset][disabled]:hover.save-button,
        input[type=reset][disabled]:active.save-button,
        input[type=submit][disabled].save-button,
        input[type=submit][disabled]:hover.save-button,
        input[type=submit][disabled]:active.save-button {
            background-color: #0000EE;
        }

        /*Light button disabled*/
        :not(.mce-btn):not(.mce-window-head):not(.ui-datepicker-buttonpane) > button[disabled].light-button,
        button[disabled]:hover.light-button,
        button[disabled]:active.light-button,
        input[type=button][disabled].light-button,
        input[type=button][disabled]:hover.light-button,
        input[type=button][disabled]:active.light-button,
        input[type=reset][disabled].light-button,
        input[type=reset][disabled]:hover.light-button,
        input[type=reset][disabled]:active.light-button,
        input[type=submit][disabled].light-button,
        input[type=submit][disabled]:hover.light-button,
        input[type=submit][disabled]:active.light-button {
            color: #5088F0;
        }

        /*Light-save button disabled*/
        :not(.mce-btn):not(.mce-window-head):not(.ui-datepicker-buttonpane) > button[disabled].light-save-button,
        button[disabled]:hover.light-save-button,
        button[disabled]:active.light-save-button,
        input[type=button][disabled].light-save-button,
        input[type=button][disabled]:hover.light-save-button,
        input[type=button][disabled]:active.light-save-button,
        input[type=reset][disabled].light-save-button,
        input[type=reset][disabled]:hover.light-save-button,
        input[type=reset][disabled]:active.light-save-button,
        input[type=submit][disabled].light-save-button,
        input[type=submit][disabled]:hover.light-save-button,
        input[type=submit][disabled]:active.light-save-button {
            background-color: #5088F0;
        }

        


        #related_col .block-title {
            background-color: #BBBBBB;
            color: #000000;
        }
        #related_col .block-title a {
            color: #0000FF;
        }
        #related_col .block-content {
            background-color: #E5E5E5;
        }
        #related_col .block-content .instr {
            background-color: #D0D0D0;
        }
        #related_col .block-content .odd {
            background-color: #DEDEDE;
        }
        #related_col .block-content .even {
            background: #D0D0D0;
        }
        


        .contents .output_box .title {
            background-color: #D89655;
        }
        .block-content,
        .block2-content,
        .contents .output_box table tr th,
        .contents .output_box {
            background-color: #F9F4E6;
        }
        .output_box_instr,
        .contents .output_box .instr,
        .block .instr,
        .block-content .instr {
            background-color: #F2E0C5;
        }
        .odd,
        .contents .output_box .odd,
        .block-content .odd {
            background-color: #F6EAD5;
        }
        .even,
        .contents .output_box .even,
        .block-content .even {
            background-color: #F2E0C5;
        }
        


        .tabs .fg_tab {
            background-color: #F0DBBC;
            color: #CC1A1A;
            border-bottom-color: #F0DBBC;
        }
        .tab_menu_label:hover, .active .tab_menu_label,
        .tab_no_menu_label:hover,
        .tab_no_menu_label:hover a {
            color: #CC1A1A;
        }
        .subtabs {
            background-color: #F0DBBC;
            color: #000000;
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
            text-transform: none;
        }
        .subtabs .divider,
        .subtabs .bg_tab,
        .subtabs .bg_tab a {
            background-color: #F0DBBC;
            border-top-color: #F0DBBC;
            color: #000000;
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
            text-transform: none;
        }
        .subtabs .fg_tab {
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
            color: #CC1A1A;
            text-transform: none;
        }

        /*
        uncomment this to make the subtabs follow the selected tab color instead
        of the link color

        .subtabs .bg_tab:hover a {
            color: #CC1A1A;
        }
        .subtabs .fg_tab:hover a, {
            color: #CC1A1A;
        }
        */

        .subtabs .fg_tab a {
            color: #CC1A1A;
        }
        


        .documentation_box {
            background: #F0E0BC;
            color: #000000;
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
        }
        


        body.in_iframe {
            background-color: #FFFFF7;
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
        }
        .pagedoc {
            background-color: #FFFFF7;
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
        }
        .page_box {
            background-color: #FFFFF7;
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
        }
        .page_box_in_iframe {
            background-color: #FFFFF7;
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
        }
        .contents {
            background-color: #FFFFF7;
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
        }
        .contents_options {
            background-color: #FFFFF7;
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
        }
        #top-links {
            background-color: #FFFFF7;
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
        }
        .fullscreen {
            background-color: #FFFFF7;
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
        }
        .subtabs .fg_tab {
            border-bottom-color: #FFFFF7;
        }
        .subtabs .fg_tab div {
            background-color: #FFFFF7;
            border-bottom-color: #FFFFF7;
        }
        .fullscreen_schedule {
            background-color: #FFFFF7;
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
        }
        .contents input,
        .contents input_box,
        .contents textarea {
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
        }

        /*Possible fix for buttons using the wrong font -Nathan*/
        /*:not(.mce-btn):not(.mce-window-head):not(.ui-datepicker-buttonpane) > button,
        input[type=button],
        input[type=reset],
        input[type=submit] {
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif !important;
        }*/

        .qtip.rm-qtip {
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
        }
        #cboxContent {
            background-color: #FFFFF7;
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
        }

        /* For now, use the main site background color for tool tips. */
        .qtip.qtip-rm,
        .qtip.qtip-rm .qtip-titlebar {
            background-color: #FFFFF7;
        }

        /* Not sure where this should live. */
        #actions_col .block-title-text {
            font-size: 15px;
        }
        #related_col .block-title-text {
            font-size: 15px;
        }

        /* For jquery-ui. */
        .ui-widget {
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
        }

        .arrow-slidedown {
            background-color: #FFFFF7;
            color: #000000;
        }
        


        .contents .input .title,
        .contents .input_box .title {
            background-color: #247bf4;
        }
        .contents .input,
        .contents .input_box,
        .contents .input table tr th,
        .contents .input_box table tr th,
        .form .block-content {
            background-color: #f6f9fe;
        }
        .contents .input .instr,
        .contents .input_box .instr,
        .multi_block_button,
        .form .block .instr {
            background-color: #dbe8fa;
        }
        .contents .input .odd,
        .contents .input_box .odd,
        .form .block-content .odd {
            background-color: #edf3fc;
        }
        .contents .input .even,
        .contents .input_box .even,
        .form .block-content .even {
            background-color: #dbe8fa
        }
        


        div.rm_mega_menus_container,
        ul.rm_mega_menu.darker,
        ul.rm_mega_menu > li.mega > a,
        ul.rm_mega_menu.darker > li.mega > a,
        ul.rm_mega_menu > li.mega-link > a,
        ul.rm_mega_menu.darker > li.mega-link > a,
        ul.rm_mega_menu > li.mega-label > span {
            background-color: #777777;
            border-color: #777777;
            color: #FFFFFF;
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
            text-transform: none;
        }
        ul.rm_mega_menu > li.mega.selected > a {
            background-color: #FFFFFF;
            border-color: #FFFFFF;
            color: #777777;
        }
        ul.rm_mega_menu > li.mega:hover > a {
            color: #CC1A1A;
        }
        div.rm_mega_menus_container ul.rm_mega_menu > li.mega > div.menu_dropdown {
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
        }
        .contents input,
        .contents input_box,
        .contents textarea {
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
        }
        


        #actions_col .block-title {
            background-color: #244A84;
            color: #FFFFFF;
        }
        #actions_col .block-title a {
            color: #FFFFFF;
        }
        #actions_col .block-content {
            background-color: #BDD2F8;
        }
        #actions_col .block-content .instr {
            background-color: #B0CBFC;
        }
        #actions_col .block-content .odd {
            background-color: #C2D6FB;
        }
        #actions_col .block-content .even {
            background: #B0CBFC;
        }
        </style><script src="includes/jquery/jquery-1.8.3.min.js" type="text/javascript"></script><script src="includes/jquery/jquery-ui.min.js" type="text/javascript"></script><script src="includes/jquery/jquery.hoverIntent.minified.js" type="text/javascript"></script><script src="includes/jquery/basic.js" type="text/javascript"></script><script src="includes/jquery/jquery.qtip.min.js" type="text/javascript"></script><script src="includes/jquery/jquery.highlight-4.js"></script><script src="includes/jquery/jquery.instaFilter.js"></script><script type="text/javascript">$(function() { setup_info_links(); });</script><script src="includes/jquery/user_generated.js"></script><script type="text/javascript">
    $(document).ready(function(){
        $program_sections = $("#sections-container");
        $("#program_filter").InstaFilter($program_sections, {
            typing_pause: 500,
            search_unit_selector: ".slot-entry",
            search_unit_ancestor_fields_selector: ".session-title, .session-chair, .room-name"
        });

        // this ensures there is enough vertical space below the search field to keep the
        // page from scrolling during a search
        $program_sections.css("min-height", $(window).height());
    });
    </script></head><body><a name="top" tabindex="-1"></a><div class="centered"><div><div style="float: left;"></div>
    <div style="float: left; margin-top: 15px; height: 80px;"><span class="page-title">IEEE Cluster 2020 Program</span></div>
    <div style="float: right;"><a href="https://clustercomp.org/2020/" target="_top">back to Cluster 2020 Top page</a></div>
    <div style="clear: both;"></div>
    </div></div><br /><div class="centered"><br />
    <span>All times are JST timezone.</span><br /><br />
    <span class="page-links"><a href="at_a_glance.html">Overview</a></span> | <span class="page-links"><a href="by_date.html">By Date</a></span> | <span class="page-links">By Event Type</span> | <span class="page-links"><a href="by_room.html">By Room</a></span> | <span class="page-links"><a href="by_auth.html">Author Index</a></span><br /></div><br /><div id="main-content-box"><div class="centered"><table class="cellspacing10px" role="presentation"><tr><td><div class="anchor-link"><a href="#evtt104">Break</a></div></td><td><div class="anchor-link"><a href="#evtt102">Other</a></div></td><td><div class="anchor-link"><a href="#sstype102">Posters</a></div></td></tr><tr><td><div class="anchor-link"><a href="#evtt101">Keynote</a></div></td><td><div class="anchor-link"><a href="#sstype101">Papers</a></div></td><td><div class="anchor-link"><a href="#evtt103">Workshop</a></div></td></tr></table><br /><hr /></div><div class="righted"><input id="program_filter" name="program_filter" placeholder="search" size="40" type="text" /></div><div class="centered" id="sections-container"><table role="presentation"><tr><td align="left"><div class="etype-section"><div class="centered"><a name="evtt104" tabindex="-1"></a><div class="section-title">Break</div></div><div class="section-entry"><div class="session-entry"><span class="session-date">Tuesday</span> <span class="session-time">9:10-9:20</span> <a href="calendar/sessions/sess121.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="room-name">Break<br /></span><span class="session-event-type">Break</span><br /><div class="session-title">Break</div><span class="slot-entry"></span></div><div class="session-entry"><span class="session-date">Wednesday</span> <span class="session-time">15:00-15:10</span> <a href="calendar/sessions/sess122.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="room-name">Break<br /></span><span class="session-event-type">Break</span><br /><div class="session-title">Break</div><span class="slot-entry"></span></div><div class="session-entry"><span class="session-date">Thursday</span> <span class="session-time">9:00-9:10</span> <a href="calendar/sessions/sess123.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="room-name">Break<br /></span><span class="session-event-type">Break</span><br /><div class="session-title">Break</div><span class="slot-entry"></span></div></div><div class="centered"><div class="top-link"><a href="#top">Return to Top</a></div></div><hr /></div><div class="etype-section"><div class="centered"><a name="evtt101" tabindex="-1"></a><div class="section-title">Keynote</div></div><div class="section-entry"><div class="session-entry"><span class="session-date">Tuesday</span> <span class="session-time">8:20-9:10</span> <a href="calendar/sessions/sess111.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="room-name">Zoom<br /></span><span class="session-event-type">Keynote</span><br /><div class="session-title">K1: Keynote-1</div><div class="session-chair">Chair: Pete Beckman (Argonne National Laboratory)<br /></div><div class="slot-entry"><a name="pec101" tabindex="-1"></a><div class="slot-title">Fugaku: the First `Exascale' Supercomputer (Satoshi Matsuoka)</div><div class="slot-authors">Satoshi Matsuoka (RIKEN Center for Computational Science (R-CCS))</div><div class="auth-pics-section"></div><div><a class="clickable no-decoration" id="vhsjs_view_96_1599045762_01" onclick="$('#vhsjs_view_96_1599045762_01').hide();
                $('#vhsjs_hide_96_1599045762_01').show();
                $('#95_1599045762_01').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Biography</span></a><a class="clickable no-decoration" id="vhsjs_hide_96_1599045762_01" onclick="$('#95_1599045762_01').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_96_1599045762_01').hide();
                $('#vhsjs_view_96_1599045762_01').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Biography</span></a><div data-display-control="96_1599045762_01" id="95_1599045762_01" style="display: none"><div class="arrow-slidedown"><div><div class="biography-sect">Satoshi Matsuoka (RIKEN Center for Computational Science (R-CCS))<blockquote>Satoshi Matsuoka from April 2018 has become the director of Riken CCS, the top-tier HPC center that represents HPC in Japan, developing and hosting Japan’s tier-one ‘Fugaku’ supercomputer which has become the fastest supercomputer in the world in all four major supercomputer rankings, along with multitudes of ongoing cutting edge HPC research being conducted, including investigating Post-Moore era computing. He was the leader of the TSUBAME series of supercomputers, at Tokyo Institute of Technology, where he still holds a Professor position, to continue his research activities in HPC as well as scalable Big Data and AI. His commendations include the ACM Gordon Bell Prize in 2011 and the IEEE Sidney Fernbach Award in 2014, both being one of the highest awards in the field of HPC, as well as being the Program Chair for ACM/IEEE Supercomputing 2013 (SC13).</blockquote></div></div></div></div></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_94_1599045762_01" onclick="$('#vhsjs_view_94_1599045762_01').hide();
                $('#vhsjs_hide_94_1599045762_01').show();
                $('#93_1599045762_01').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_94_1599045762_01" onclick="$('#93_1599045762_01').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_94_1599045762_01').hide();
                $('#vhsjs_view_94_1599045762_01').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="94_1599045762_01" id="93_1599045762_01" style="display: none"><div class="arrow-slidedown"><blockquote>Fugaku is the first `exascale’ supercomputer of the world, not due to its peak double precision flops, but rather, its demonstrated performance in real applications that were expected of exascale machines on their conceptions 10 years ago, as well as reaching actual exaflops in new breed of benchmarks such as HPL-AI. But the importance of Fugaku is "applications first" philosophy under which it was developed, and its resulting mission to be the centerpiece for rapid realization of the so-called Japanese `Society 5.0' as defined by the Japanese S&T national policy. As such, Fugaku’s immense power is directly applicable not only to traditional scientific simulation applications, but can be a target of Society 5.0 applications that encompasses conversion of HPC & AI & Big Data as well as Cyber (IDC & Network) vs. Physical (IoT) space, with immediate societal impact. In fact, Fugaku is already in partial operation a year ahead of schedule, primarily to obtain early Society 5.0 results including combatting COVID-19 as well as resolving other important societal issues.</blockquote></div></div></div></div><div class="slot-urls"></div></div></div><div class="session-entry"><span class="session-date">Wednesday</span> <span class="session-time">14:00-14:50</span> <a href="calendar/sessions/sess112.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="room-name">Zoom<br /></span><span class="session-event-type">Keynote</span><br /><div class="session-title">K2: Keynote-2</div><div class="session-chair">Chair: Taisuke Boku (Center for Computational Sciences, University of Tsukuba/Graduate School of Systems and Information Engineering, University of Tsukuba)<br /></div><div class="slot-entry"><a name="pec102" tabindex="-1"></a><div class="slot-title">The Price Performance of Performance Models (Felix Wolf)</div><div class="slot-authors">Felix Wolf (Technical University of Darmstadt)</div><div class="auth-pics-section"></div><div><a class="clickable no-decoration" id="vhsjs_view_100_1599045762_02" onclick="$('#vhsjs_view_100_1599045762_02').hide();
                $('#vhsjs_hide_100_1599045762_02').show();
                $('#99_1599045762_02').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Biography</span></a><a class="clickable no-decoration" id="vhsjs_hide_100_1599045762_02" onclick="$('#99_1599045762_02').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_100_1599045762_02').hide();
                $('#vhsjs_view_100_1599045762_02').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Biography</span></a><div data-display-control="100_1599045762_02" id="99_1599045762_02" style="display: none"><div class="arrow-slidedown"><div><div class="biography-sect">Felix Wolf (Technical University of Darmstadt)<blockquote>Felix Wolf is full professor at the Department of Computer Science of Technical University of Darmstadt in Germany, where he leads the Laboratory for Parallel Programming. He works on methods, tools, and algorithms that support the development and deployment of parallel software systems in various stages of their life cycle. Wolf received his Ph.D. degree from RWTH Aachen University in 2003. After working more than two years as a postdoc at the Innovative Computing Laboratory of the University of Tennessee, he was appointed research group leader at Jülich Supercomputing Centre. Between 2009 and 2015, he was head of the Laboratory for Parallel Programming at the German Research School for Simulation Sciences in Aachen and full professor at RWTH Aachen University. Wolf has made major contributions to several open-source performance tools for parallel programs, including Scalasca, Score-P, and Extra-P. Moreover, he has initiated the Virtual Institute High Productivity Supercomputing, an international initiative of HPC programming-tool builders aimed at the enhancement, integration, and deployment of their products. He has published more than a hundred refereed articles on parallel computing, several of which have received awards.</blockquote></div></div></div></div></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_98_1599045762_02" onclick="$('#vhsjs_view_98_1599045762_02').hide();
                $('#vhsjs_hide_98_1599045762_02').show();
                $('#97_1599045762_02').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_98_1599045762_02" onclick="$('#97_1599045762_02').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_98_1599045762_02').hide();
                $('#vhsjs_view_98_1599045762_02').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="98_1599045762_02" id="97_1599045762_02" style="display: none"><div class="arrow-slidedown"><blockquote>To understand the scaling behavior of HPC applications, developers often use performance models. A performance model is a formula that expresses a key performance metric, such as runtime, as a function of one or more execution parameters, such as core count and input size. Performance models offer quick insights on a very high level of abstraction, including predictions of future behavior. In view of the complexity of today’s applications, which often combine several sophisticated algorithms, creating performance models manually is extremely laborious. Empirical performance modeling, the process of learning such models from performance data, offers a convenient alternative, but comes with its own set of challenges. The two most prominent ones are noise and the cost of the experiments needed to generate the underlying data. In this talk, we will review the state of the art in empirical performance modeling and investigate how we can employ machine learning and other strategies to improve the quality and lower the cost of the resulting models.</blockquote></div></div></div></div><div class="slot-urls"></div></div></div><div class="session-entry"><span class="session-date">Thursday</span> <span class="session-time">8:00-8:50</span> <a href="calendar/sessions/sess113.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="room-name">Zoom<br /></span><span class="session-event-type">Keynote</span><br /><div class="session-title">K3: Keynote-3</div><div class="session-chair">Chair: Franck Cappello (Argonne National Laboratory)<br /></div><div class="slot-entry"><a name="pec103" tabindex="-1"></a><div class="slot-title">AI for Science (Alok N. Choudhary)</div><div class="slot-authors">Alok N. Choudhary (Northwestern University)</div><div class="auth-pics-section"></div><div><a class="clickable no-decoration" id="vhsjs_view_104_1599045762_03" onclick="$('#vhsjs_view_104_1599045762_03').hide();
                $('#vhsjs_hide_104_1599045762_03').show();
                $('#103_1599045762_03').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Biography</span></a><a class="clickable no-decoration" id="vhsjs_hide_104_1599045762_03" onclick="$('#103_1599045762_03').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_104_1599045762_03').hide();
                $('#vhsjs_view_104_1599045762_03').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Biography</span></a><div data-display-control="104_1599045762_03" id="103_1599045762_03" style="display: none"><div class="arrow-slidedown"><div><div class="biography-sect">Alok N. Choudhary (Northwestern University)<blockquote>Dr. Alok Choudhary is the Dever Professor of Electrical Engineering and Computer Science at Northwestern University. He also teaches at Kellogg School of management. He is the founder, chairman and chief scientist of 4C insights, a big data analytics and marketing technology software company (4C was recently acquired by MediaOcean). He received the National Science Foundation’s Young Investigator Award in 1993. He is a fellow of IEEE, ACM and AAAS. He was listed by Adweek in " trailblazers and pioneers in Marketing technologies" and Dr. Choudhary was also awarded with the "Technology Manager of The Year in Chicago" in 2018. Prof. Choudhary received the first award for “Excellence in Research, Teaching and Service” from the McCormick School of Engineering.

His research interests are in high-performance computing, ML/AI and scalable data mining (and their applications in science, medicine and business), and high-performance I/O systems. Alok Choudhary has published more than 400 papers in various journals and conferences and has graduated 45+ PhD students, including more than 10 women PhDs (one of the highest in the world). He serves on the board of several companies. He serves on National academies of Science roundtable to develop and recommend post-secondary data science education strategies. Dr. Choudhary currently serves on the U.S. Secretary of Energy’s Advisory Board on Artificial Intelligence.</blockquote></div></div></div></div></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_102_1599045762_03" onclick="$('#vhsjs_view_102_1599045762_03').hide();
                $('#vhsjs_hide_102_1599045762_03').show();
                $('#101_1599045762_03').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_102_1599045762_03" onclick="$('#101_1599045762_03').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_102_1599045762_03').hide();
                $('#vhsjs_view_102_1599045762_03').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="102_1599045762_03" id="101_1599045762_03" style="display: none"><div class="arrow-slidedown"><blockquote>"AI for Science" seeks to understand, explore and develop Machine Learning and Data Mining approaches for accelerating scientific discoveries as well as designs. An example of this is learning from data to build predictive models that can enable exploration of scientific questions without relying upon underlying theory. Given that modern instruments, supercomputing simulations, experiments, sensors and IoT are creating massive amounts of data at an astonishing speed and diversity, AI for Science has the potential to accelerate science discoveries by orders of magnitude. Another example is the acceleration of so called the "inverse problems" which explore the design space based on desired properties. This talk presents examples, possibilities and limitations of "AI for Science".</blockquote></div></div></div></div><div class="slot-urls"></div></div></div></div><div class="centered"><div class="top-link"><a href="#top">Return to Top</a></div></div><hr /></div><div class="etype-section"><div class="centered"><a name="evtt102" tabindex="-1"></a><div class="section-title">Other</div></div><div class="section-entry">
                <div class="session-entry"><span class="session-date">Tuesday</span> <span class="session-time">8:00-8:20</span> <a href="calendar/sessions/sess114.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="room-name">Zoom<br /></span><span class="session-event-type">Other</span><br /><div class="session-title">IEEE Cluster 2020 Opening</div><div class="session-chair">Chair: Miwako Tsuji (RIKEN)<br /></div><div class="slot-entry">Greetings from General Co-chairs (Taisuke Boku & Pete Beckman)<br />
Greetings from PC Co-chairs (Masaaki Kondo & Franck Cappello)<br />
Greetings from Steering Committee Chair (Pavan Balaji)</div></div>
                <div class="session-entry"><span class="session-date">Tuesday</span> <span class="session-time">14:50-15:40</span> <a href="calendar/sessions/sess104v.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="room-name">Zoom<br /></span><span class="session-event-type">Other</span><br /><div class="session-title">Vender Showcase</div><div class="session-chair">Chair: Katsuya Nishi (Computational Science, K.K); Shinji Sumimoto (Fujitsu)<br /></div><div class="session-description"><a href="https://sites.google.com/view/vendorshowcase-cluster2020/" target="_blank">Vendor Showcase</a></div><span class="slot-entry"></span></div><span class="slot-entry"></span></div>
                <div class="session-entry"><span class="session-date">Wednesday</span> <span class="session-time">8:50-9:40</span> <a href="calendar/sessions/sess106.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="room-name">Zoom<br /></span><span class="session-event-type">Other</span><br /><div class="session-title">Virtual Fugaku Tour</div><div class="session-chair">Chair: Taisuke Boku (Center for Computational Sciences, University of Tsukuba/Graduate School of Systems and Information Engineering, University of Tsukuba)<br /></div><span class="slot-entry"></span></div>
                <div class="session-entry"><span class="session-date">Wednesday</span> <span class="session-time">14:50-15:00</span> <a href="calendar/sessions/sess120.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="room-name">Zoom<br /></span><span class="session-event-type">Other</span><br /><div class="session-title">IEEE Cluster 2021 Announcement</div><div class="session-chair">Chair: Taisuke Boku</div><div class="session-description">
Toni Cortes, Barcelona Supercomputing Center<br />
Kathryn Mohror, Lawrence Livermore National Laboratory</div><span class="slot-entry"></span></div>
                <div class="session-entry"><span class="session-date">Thursday</span> <span class="session-time">8:50-9:00</span> <a href="calendar/sessions/sess115.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="room-name">Zoom<br /></span><span class="session-event-type">Other</span><br /><div class="session-title">Closing Remarks</div><span class="slot-entry"></span></div></div><div class="centered"><div class="top-link"><a href="#top">Return to Top</a></div></div><hr /></div><div class="etype-section"><div class="centered"><a name="sstype101" tabindex="-1"></a><div class="section-title">Papers</div></div>
                <div class="section-entry"><div class="session-entry"><span class="session-date">Tuesday</span> <span class="session-time">9:20-10:10</span> <a href="calendar/sessions/sess102.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="room-name">Zoom<br /></span><span class="session-event-type">Paper</span><br /><div class="session-title">T1: Best Papers</div><div class="session-chair">Chair: Masaaki Kondo (University of Tokyo, Riken R-CCS)<br /></div><div class="slot-entry"><a name="pap119" tabindex="-1"></a><div class="slot-title">Efficient Process-to-Node Mapping Algorithms for Stencil Computations <a href="calendar/submissions/sess102--pap119.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div><span class="BP award">Best Paper</span></div><div class="slot-authors">Konrad von Kirchbach (TU Wien/Faculty of Informatics), Markus Lehr and Sascha Hunold (TU wien/Faculty of Informatics), Christian Schulz (University of Vienna/Faculty of Computer Science), and Jesper Larsson Träff (TU wien/Faculty of Informatics)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_106_1599045762_06" onclick="$('#vhsjs_view_106_1599045762_06').hide();
                $('#vhsjs_hide_106_1599045762_06').show();
                $('#105_1599045762_06').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_106_1599045762_06" onclick="$('#105_1599045762_06').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_106_1599045762_06').hide();
                $('#vhsjs_view_106_1599045762_06').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="106_1599045762_06" id="105_1599045762_06" style="display: none"><div class="arrow-slidedown"><blockquote>Good process-to-compute-node mappings can be decisive
for well performing HPC applications. A special, important
class of process-to-node mapping problems is the problem of
mapping processes that communicate in a sparse stencil pattern
to Cartesian grids. By thoroughly exploiting the inherently
present structure in this type of problem, we devise three novel
distributed algorithms that are able to handle arbitrary stencil
communication patterns effectively. We analyze the expected
performance of our algorithms based on an abstract model of
inter- and intra-node communication. An extensive experimental
evaluation on several HPC machines shows that our algorithms
are up to two orders of magnitude faster in running time
than a (sequential) high-quality general graph mapping tool,
while obtaining similar results in communication performance.
Furthermore, our algorithms also achieve significantly better
mapping quality compared to previous state-of-the-art Cartesian
grid mapping algorithms. This results in up to a threefold
performance improvement of an MPI_Neighbor_alltoall
exchange operation. Our new algorithms can be used to implement
the MPI_Cart_create functionality.</blockquote></div></div></div></div><div class="slot-urls"></div></div><div class="slot-entry"><a name="pap154" tabindex="-1"></a><div class="slot-title">CuVPP: Filter-based Longest Prefix Matching in Software Data Planes <a href="calendar/submissions/sess102--pap154.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div><span class="BP award">Best Paper</span></div><div class="slot-authors">Minseok Kwon and Krishna Prasad Neupane (Rochester Institute of Technology); John Marshall (Cisco Systems, Inc.); and M. Mustafa Rafique (Rochester Institute of Technology)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_108_1599045762_07" onclick="$('#vhsjs_view_108_1599045762_07').hide();
                $('#vhsjs_hide_108_1599045762_07').show();
                $('#107_1599045762_07').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_108_1599045762_07" onclick="$('#107_1599045762_07').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_108_1599045762_07').hide();
                $('#vhsjs_view_108_1599045762_07').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="108_1599045762_07" id="107_1599045762_07" style="display: none"><div class="arrow-slidedown"><blockquote>Programmability in the data plane has become increasingly important as virtualization is introduced into networking and software-defined networking becomes more prevalent. Yet, the performance of programmable data planes, often on commodity hardware, is a major concern, especially in light of ever-increasing network link speed and routing table size. This paper focuses on IP lookup, specifically the longest prefix matching for IPv6 addresses, which is a major performance bottleneck in programmable switches. As a solution, the paper presents CuVPP, a programmable switch that uses packet batch processing and cache locality for both instructions and data by leveraging Vector Packet Processing (VPP). We thoroughly evaluate CuVPP with both real network traffic and file-based lookup on a commodity hardware server connected via 80~Gbps network links and compare its performance with the other popular approaches. Our evaluation shows that CuVPP can achieve up to 4.5 million lookups per second with real traffic, higher than other trie- or filter-based lookup approaches, and scales well even when the routing table size grows to 2 million prefixes.</blockquote></div></div></div></div><div class="slot-urls"></div></div><div class="slot-entry"><a name="pap135" tabindex="-1"></a><div class="slot-title">HAN: a Hierarchical AutotuNed Collective Communication Framework <a href="calendar/submissions/sess102--pap135.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div><span class="BP award">Best Paper</span></div><div class="slot-authors">Xi Luo (University of Tennessee, Knoxville); Wei Wu (Los Alamos National Laboratory); George Bosilca, Yu Pei, and Qinglei Cao (University of Tennessee, Knoxville); Thananon Patinyasakdikul (Cray); and Dong Zhong and Jack Dongarra (University of Tennessee, Knoxville)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_110_1599045762_08" onclick="$('#vhsjs_view_110_1599045762_08').hide();
                $('#vhsjs_hide_110_1599045762_08').show();
                $('#109_1599045762_08').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_110_1599045762_08" onclick="$('#109_1599045762_08').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_110_1599045762_08').hide();
                $('#vhsjs_view_110_1599045762_08').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="110_1599045762_08" id="109_1599045762_08" style="display: none"><div class="arrow-slidedown"><blockquote>High-performance computing (HPC) systems keep growing in scale and heterogeneity to satisfy the increasing computational need, and this brings new challenges to the design of MPI libraries, especially with regard to collective operations.<br><br>To address these challenges, we present "HAN," a new hierarchical autotuned collective communication framework in Open MPI, which selects suitable homogeneous collective communication modules as submodules for each hardware level, uses collective operations from the submodules as tasks, and organizes these tasks to perform efficient hierarchical collective operations. With a task-based design, HAN can easily swap out submodules, while keeping tasks intact, to adapt to new hardware. This makes HAN suitable for the current platform and provides a strong and flexible support for future HPC systems.<br><br>To provide a fast and accurate autotuning mechanism, we present a novel cost model based on benchmarking the tasks instead of a whole collective operation. This method drastically reduces tuning time, as the cost of tasks can be reused across different message sizes, and is more accurate than existing cost models. Our cost analysis suggests the autotuning component can find the optimal configuration in most cases.<br><br>The evaluation of the HAN framework suggests our design significantly improves the default Open MPI and achieves decent speedups against state-of-the-art MPI implementations on tested applications.</blockquote></div></div></div></div><div class="slot-urls"></div></div><div class="slot-entry"><a name="pap116" tabindex="-1"></a><div class="slot-title">DelveFS - An event-driven semantic file system for object stores <a href="calendar/submissions/sess102--pap116.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div><span class="BP award">Best Paper</span></div><div class="slot-authors">Marc-André Vef, Rebecca Steiner, Reza Salkhordeh, and Jörg Steinkamp (Johannes Gutenberg University Mainz); Florent Vennetier and Jean-François Smigielski (OpenIO); and André Brinkmann (Johannes Gutenberg University Mainz)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_112_1599045762_09" onclick="$('#vhsjs_view_112_1599045762_09').hide();
                $('#vhsjs_hide_112_1599045762_09').show();
                $('#111_1599045762_09').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_112_1599045762_09" onclick="$('#111_1599045762_09').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_112_1599045762_09').hide();
                $('#vhsjs_view_112_1599045762_09').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="112_1599045762_09" id="111_1599045762_09" style="display: none"><div class="arrow-slidedown"><blockquote>Data-driven applications are becoming increasingly important in numerous industrial and scientific fields, growing the need for scalable data storage, such as object storage.
Yet, many data-driven applications cannot use object interfaces directly and often have to rely on third-party file system connectors that support only a basic representation of objects as files in a flat namespace.
With sometimes millions of objects per bucket, this simple organization is insufficient for users and applications who are usually only interested in a small subset of objects.
These huge buckets are not only lacking basic semantic properties and structure, but they are also challenging to manage from a technical perspective as object store file systems cannot cope with such directory sizes. <br><br>DelveFS is the first object store file system that solves this challenge by offering the ability to compose a custom semantic file system that allows multiple unique views onto the object store.
Through flexible filters, users can specify each view's content, tailored to their unique interests or an application's requirements.
By processing object store events which describe changes in the object store, DelveFS is able to keep all views eventually consistent. 
DelveFS allows to operate concurrently through the object and file system interfaces on the same set of objects, delivering similar file system throughput compared to the native object store interfaces or other file system connectors.</blockquote></div></div></div></div><div class="slot-urls"></div></div></div><div class="session-entry"><span class="session-date">Tuesday</span> <span class="session-time">14:00-14:50</span> <a href="calendar/sessions/sess103.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="room-name">Zoom<br /></span><span class="session-event-type">Paper</span><br /><div class="session-title">T2: Performance Characterization and Scheduling</div><div class="session-chair">Chair: Johannes Langguth (Simula)<br /></div><div class="slot-entry"><a name="pap134" tabindex="-1"></a><div class="slot-title">NeoMPX: Characterizing and Improving Estimation of Multiplexing Hardware Counters for PAPI <a href="calendar/submissions/sess103--pap134.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Yi-Chao Wang, Jie Wang, Jin-Kun Chen, Si-Cheng Zuo, Xiao-Ming Su, and James Lin (Shanghai Jiao Tong University)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_114_1599045762_1" onclick="$('#vhsjs_view_114_1599045762_1').hide();
                $('#vhsjs_hide_114_1599045762_1').show();
                $('#113_1599045762_1').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_114_1599045762_1" onclick="$('#113_1599045762_1').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_114_1599045762_1').hide();
                $('#vhsjs_view_114_1599045762_1').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="114_1599045762_1" id="113_1599045762_1" style="display: none"><div class="arrow-slidedown"><blockquote>Modern processors provide hundreds of low-level hardware events (such as cache miss rate), but offer only a small number (usually 6–12) of hardware counters to collect these events due to limited register resource. Multiplexing (MPX) is an estimation-based technique designed to collect hardware events simultaneously with few hardware counters. However, the lowaccuracy of existing MPX methods prevents this technique from wide usage in real conditions. To obtain accurate and reliable hardware counter values, we conducted this work in three steps: 1) to explore the root cause of inaccuracy, we characterized the estimation errors of MPX, and found that estimation errors arise from the outliers in PAPI; 2) to eliminate these outliers and improve MPX accuracy, we proposed two non-linear growth rate gradient estimation methods: divided curved-area method (DCAM) and curved-area method (CAM); 3) based on these two methods, we developed a new MPX library for PAPI, NeoMPX. We evaluated NeoMPX with six Rodinia benchmarks on four mainstream x86 and ARM server processors, and compared the results with PAPI default MPX and two other state-of-art MPX methods, DIRA, and TAM. Evaluations show that, for collecting 16 evaluated hardware events, our methods can improve up to 59% accuracy than PAPI default MPX, and achieve 36% and 5% higher accuracy than DIRA and TAM, respectively. We have open-sourced NeoMPX and expect it to enable PAPI MPX for practical usage.</blockquote></div></div></div></div><div class="slot-urls"></div></div><div class="slot-entry"><a name="pap169" tabindex="-1"></a><div class="slot-title">Grade10: A Framework for Performance Characterization of Distributed Graph Processing <a href="calendar/submissions/sess103--pap169.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Tim Hegeman, Animesh Trivedi, and Alexandru Iosup (Vrije Universiteit Amsterdam)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_116_1599045762_11" onclick="$('#vhsjs_view_116_1599045762_11').hide();
                $('#vhsjs_hide_116_1599045762_11').show();
                $('#115_1599045762_11').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_116_1599045762_11" onclick="$('#115_1599045762_11').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_116_1599045762_11').hide();
                $('#vhsjs_view_116_1599045762_11').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="116_1599045762_11" id="115_1599045762_11" style="display: none"><div class="arrow-slidedown"><blockquote>Graph processing is one of the most important and ubiquitous classes of analytical workloads. To process large graph datasets with diverse algorithms, tens of distributed graph processing frameworks emerged. Their users are increasingly expecting high performance for diversifying workloads. Meeting this expectation depends on understanding the performance of each framework. However, performance analysis and characterization of a distributed graph processing framework is challenging. Contributing factors are the irregular nature of
graph computation across datasets and algorithms, the semantic gap between workload-level and system-level monitoring, and the lack of lightweight mechanisms for collecting fine-grained
performance data. Addressing the challenge, in this work we present Grade10, an experimental framework for fine-grained performance characterization of distributed graph processing workloads. Grade10 captures the graph workload execution as a performance graph from logs and application traces, and builds a fine-grained, unified workload-level and system-level
view of performance. Grade10 samples sparsely for lightweight monitoring and addresses the problem of accuracy through a novel approach for resource attribution. Last, it can identify automatically resource bottlenecks and common classes of performance issues. Our real-world experimental evaluation with Giraph and PowerGraph, two state-of-the-art distributed graph
processing systems, shows that Grade10 can reveal large differences in the nature and severity of bottlenecks across systems and workloads. We also show that Grade10 can be used in debugging processes, by exemplifying how we find with it a synchronization bug in PowerGraph that slows down affected phases by 1.10&#8722;2.50x. Grade10 is an open-source project available at https://github.com/atlarge-research/grade10.</blockquote></div></div></div></div><div class="slot-urls"></div></div><div class="slot-entry"><a name="pap232" tabindex="-1"></a><div class="slot-title">Evaluating Worksharing Tasks on Distributed Environments <a href="calendar/submissions/sess103--pap232.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Marcos Maroñas and Xavier Teruel (Barcelona Supercomputing Center); Mark Bull (University of Edinburgh); Eduard Ayguadé (Barcelona Supercomputing Center, Universitat Politècnica de Catalunya); and Vicenç Beltran (Barcelona Supercomputing Center)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_118_1599045762_12" onclick="$('#vhsjs_view_118_1599045762_12').hide();
                $('#vhsjs_hide_118_1599045762_12').show();
                $('#117_1599045762_12').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_118_1599045762_12" onclick="$('#117_1599045762_12').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_118_1599045762_12').hide();
                $('#vhsjs_view_118_1599045762_12').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="118_1599045762_12" id="117_1599045762_12" style="display: none"><div class="arrow-slidedown"><blockquote>Hybrid programming is a promising approach to exploit clusters of multicore systems. Our focus is on the combination of MPI and tasking. This hybrid approach combines the low-latency and high throughput of MPI with the flexibility of tasking models and their inherent ability to handle load imbalance. However, combining tasking with standard MPI implementations can be a challenge. The Task-Aware MPI library (TAMPI) eases the development of applications combining tasking with MPI. TAMPI enables developers to overlap computation and communication
phases by relying on the tasking data-flow execution model. Using this approach, the original computation that was distributed in many different MPI ranks is grouped together in fewer
MPI ranks, and split into several tasks per rank. Nevertheless, programmers must be careful with task granularity. Too fine-grained tasks introduce too much overhead, while too coarse-grained tasks lead to lack of parallelism. An adequate granularity may not always exist, especially in distributed environments where the same amount of work is distributed among many more
cores. Worksharing tasks are a special kind of tasks, recently proposed, that internally leverage worksharing techniques. By doing so, a single worksharing task may run in several cores
concurrently. Nonetheless, the task management costs remain the same than a regular task. In this work, we study the combination of worksharing tasks and TAMPI on distributed environments using two well known mini-apps: HPCCG and LULESH. Our results show significant improvements using worksharing tasks compared to regular tasks, and to other state-of-the-art alternatives such as OpenMP worksharing.</blockquote></div></div></div></div><div class="slot-urls"></div></div><div class="slot-entry"><a name="pap126" tabindex="-1"></a><div class="slot-title">Resilient Scheduling of Moldable Jobs on Failure-Prone Platforms <a href="calendar/submissions/sess103--pap126.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Anne Benoit, Valentin Le Fèvre, and Lucas Perotin (ENS Lyon); Padma Raghavan (Vanderbilt University); Yves Robert (ENS Lyon, University of Tennessee Knoxville); and Hongyang Sun (Vanderbilt University)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_120_1599045762_12" onclick="$('#vhsjs_view_120_1599045762_12').hide();
                $('#vhsjs_hide_120_1599045762_12').show();
                $('#119_1599045762_12').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_120_1599045762_12" onclick="$('#119_1599045762_12').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_120_1599045762_12').hide();
                $('#vhsjs_view_120_1599045762_12').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="120_1599045762_12" id="119_1599045762_12" style="display: none"><div class="arrow-slidedown"><blockquote>This paper focuses on the resilient scheduling of moldable parallel jobs on high-performance computing (HPC) platforms. Moldable jobs allow for choosing a processor allocation before execution, and their execution time obeys various speedup models. The objective is to minimize the overall completion time of the jobs, or makespan, assuming that jobs are subject to arbitrary failure scenarios, and hence need to be re-executed each time they fail until successful completion. This work generalizes the classical framework where jobs are known offline and do not fail. We introduce a list-based algorithm, and prove new %complexity results and
approximation ratios for three prominent speedup models (roofline, communication, Amdahl). We also introduce a batch-based algorithm, where each job is allowed a restricted number of failures per batch, and prove a new approximation ratio for the arbitrary speedup model. We conduct an extensive set of simulations to evaluate and compare different variants of the two algorithms. The results show that they consistently outperform some baseline heuristics. In particular, the list algorithm performs better for the roofline and communication models, while the batch algorithm has better performance for the Amdahl's model. Overall, our best algorithm is within a factor of 1.47 of a lower bound on average over the whole set of experiments, and within a factor of 1.8 in the worst case.</blockquote></div></div></div></div><div class="slot-urls"></div></div><div class="slot-entry"><a name="pap251" tabindex="-1"></a><div class="slot-title">Modeling the Performance of Scientific Workflow Executions on HPC Platforms with Burst Buffers <a href="calendar/submissions/sess103--pap251.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Loic Pottier and Rafael Ferreira Da Silva (USC Information Sciences Institute), Henri Casanova (University of Hawai'i at Manoa), and Ewa Deelman (USC Information Sciences Institute)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_122_1599045762_13" onclick="$('#vhsjs_view_122_1599045762_13').hide();
                $('#vhsjs_hide_122_1599045762_13').show();
                $('#121_1599045762_13').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_122_1599045762_13" onclick="$('#121_1599045762_13').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_122_1599045762_13').hide();
                $('#vhsjs_view_122_1599045762_13').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="122_1599045762_13" id="121_1599045762_13" style="display: none"><div class="arrow-slidedown"><blockquote>Scientific domains ranging from bioinformatics to astronomy and earth
science rely on traditional high-performance computing (HPC) codes, often
encapsulated in scientific workflows. In contrast to traditional HPC codes that
employ a few programming and runtime approaches that are highly
optimized for HPC platforms, scientific workflows are not necessarily
optimized for these platforms. As an effort to reduce the gap between compute 
and I/O performance, HPC platforms have adopted intermediate storage layers 
known as burst buffers. A burst buffer (BB) is a fast
storage layer positioned between the global parallel file system and the compute
nodes. Two designs currently exist: (i) shared, where the BBs
are located on dedicated nodes; and (ii) on-node, in which
each compute node embeds a private BB.
In this paper, using accurate simulations and real-world experiments, we
study how to best use these new storage layers when executing scientific
workflows. These applications are not necessarily optimized to run on HPC
systems, and thus can exhibit I/O patterns that differ from that of HPC codes.
Thus, we first characterize the I/O behaviors of a real-world workflow under
different configuration scenarios on two leadership-class HPC systems 
(Cori at NERSC and Summit at ORNL).
Then, we use these characterizations to calibrate a simulator for workflow
executions on HPC systems featuring shared and private BBs.
Last, we evaluate our approach against a large I/O-intensive workflow, 
and we provide insights on the performance levels and the potential
limitations of these two BBs architectures.</blockquote></div></div></div></div><div class="slot-urls"></div></div><div class="slot-entry"><a name="pap213" tabindex="-1"></a><div class="slot-title">Co-scheML: Interference-aware Container Co-scheduling Scheme using Machine Learning Application Profiles for GPU Clusters <a href="calendar/submissions/sess103--pap213.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Sejin Kim and Yoonhee Kim (Sookmyung Women's University)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_124_1599045762_13" onclick="$('#vhsjs_view_124_1599045762_13').hide();
                $('#vhsjs_hide_124_1599045762_13').show();
                $('#123_1599045762_13').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_124_1599045762_13" onclick="$('#123_1599045762_13').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_124_1599045762_13').hide();
                $('#vhsjs_view_124_1599045762_13').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="124_1599045762_13" id="123_1599045762_13" style="display: none"><div class="arrow-slidedown"><blockquote>Recently, efficient execution of applications on Graphic Processing Unit(GPU) has emerged as a research topic to increase overall system throughput in cluster environment. As a current cluster orchestration platform using GPUs only supports an exclusive execution of an application on a GPU, the platform may not utilize resource of GPUs fully relying on application characteristics. Nonetheless, co-execution of GPU applications leads to interference coming from resource contention among applications. If diverse resource usage characteristics of GPU applications are not deliberated, unbalanced usage of computing resources and performance degradation could be induced in a GPU cluster. This study introduces Co-scheML  for co-execution of various GPU applications such as High Performance Computing (HPC), Deep Learning (DL) Training, and DL Inference. Interference model is constructed by applying Machine Learning (ML) model with GPU metrics since predicting interference has a difficulty. Predicted interference is utilized and deployment of an application is determined by Co-scheML scheduler. Experimental results of the Co-ScheML strategy show that average job completion time is improved by 23%, and the makespan is shortened by 22% in average, as compared to baseline schedulers.</blockquote></div></div></div></div><div class="slot-urls"></div></div></div><div class="session-entry"><span class="session-date">Wednesday</span> <span class="session-time">8:00-8:50</span> <a href="calendar/sessions/sess105.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="room-name">Zoom<br /></span><span class="session-event-type">Paper</span><br /><div class="session-title">T3: Architecture and Network Support for HPC Workloads</div><div class="session-chair">Chair: Philippe Olivier Alexandre Navaux (Federal University of Rio Grande do Sul, UFRGS)<br /></div><div class="slot-entry"><a name="pap258" tabindex="-1"></a><div class="slot-title">Opportunities and limitations of Quality-of-Service in Message Passing applications on adaptively routed Dragonfly and Fat Tree networks <a href="calendar/submissions/sess105--pap258.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Jeremiah Wilke and Joseph Kenny (Sandia National Labs)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_126_1599045762_14" onclick="$('#vhsjs_view_126_1599045762_14').hide();
                $('#vhsjs_hide_126_1599045762_14').show();
                $('#125_1599045762_14').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_126_1599045762_14" onclick="$('#125_1599045762_14').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_126_1599045762_14').hide();
                $('#vhsjs_view_126_1599045762_14').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="126_1599045762_14" id="125_1599045762_14" style="display: none"><div class="arrow-slidedown"><blockquote>Avoiding communication bottlenecks remains a critical challenge in high-performance computing (HPC) as systems grow to exascale. Numerous design possibilities exist for avoiding network congestion including topology, adaptive routing, congestion control, and quality-of-service (QoS). While network design often focuses on topological features like diameter, bisection bandwidth, and routing, efficient QoS implementations will be critical for next-generation interconnects. HPC workloads are dominated by tightly-coupled mathematics, making delays in a single message manifest as delays across an entire parallel job. QoS can spread traffic onto different virtual lanes (VLs), lowering the impact of network hotspots by providing priorities or bandwidth guarantees that prevent starvation of critical traffic. Two leading topology candidates, Dragonfly and Fat Tree, are often discussed in terms of routing properties and cost, but the topology can have a major impact on QoS. While Dragonfly has attractive routing flexibility and cost relative to Fat Tree, the extra routing complexity requires several VLs to avoid deadlock. Here we discuss the special challenges of Dragonfly, proposing configurations that use different routing algorithms for different service levels (SLs) to limit VL requirements. We provide simulated results showing how each QoS strategy performs on different classes of application and different workload mixes. Despite Dragonfly's desirable characteristics for adaptive routing, Fat Tree is shown to be an attractive option when QoS is considered.</blockquote></div></div></div></div><div class="slot-urls"></div></div><div class="slot-entry"><a name="pap173" tabindex="-1"></a><div class="slot-title">MonSTer: An Out-of-the-Box Monitoring Tool for High Performance Computing Systems <a href="calendar/submissions/sess105--pap173.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Jie Li, Ghanzanfar Ali, and Ngan Nguyen (Texas Tech University); Jon Hass (Dell EMC Inc.); and Alan Sill, Tommy Dang, and Yong Chen (Texas Tech University)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_128_1599045762_15" onclick="$('#vhsjs_view_128_1599045762_15').hide();
                $('#vhsjs_hide_128_1599045762_15').show();
                $('#127_1599045762_15').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_128_1599045762_15" onclick="$('#127_1599045762_15').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_128_1599045762_15').hide();
                $('#vhsjs_view_128_1599045762_15').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="128_1599045762_15" id="127_1599045762_15" style="display: none"><div class="arrow-slidedown"><blockquote>Understanding the status of high-performance computing platforms and correlating applications to resource usage provide insight into the interactions among platform components. A lot of efforts have been devoted into developing monitoring solutions; however, a large-scale HPC system usually requires a combination of methods/tools to successfully monitor all metrics, which will lead to a huge effort in configuration and monitoring. Besides, monitoring tools are often left behind in the procurement of large-scale HPC systems. These challenges have motivated the development of a next-generation out-of-the-box monitoring tool that can be easily deployed without losing informative metrics.<br><br>In this work, we introduce MonSTer, an ``out-of-the-box'' monitoring tool for high-performance computing platforms. MonSTer uses the evolving specification Redfish to retrieve sensor data from Baseboard Management Controller (BMC), and resource management tools such as Univa Grid Engine (UGE) or Slurm to obtain application information and resource usage data. Additionally, it also uses a time-series database (e.g. InfluxDB) for data storage. MonSTer correlates applications to resource usage and reveals insightful knowledge without having additional overhead on the application and computing nodes. This paper presents the design and implementation of MonSTer, as well as experiences gained through real-world deployment on the 467-node Quanah cluster at Texas Tech University's High Performance Computing Center (HPCC) over the past year.</blockquote></div></div></div></div><div class="slot-urls"></div></div><div class="slot-entry"><a name="pap254" tabindex="-1"></a><div class="slot-title">Dynamic Kernel Fusion for Bulk Non-contiguous Data Transfer on GPU Clusters <a href="calendar/submissions/sess105--pap254.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Ching-Hsiang Chu, Kawthar Shafie Khorassani, Qinghua Zhou, Hari Subramoni, and Dhabaleswar K. Panda (The Ohio State University)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_130_1599045762_16" onclick="$('#vhsjs_view_130_1599045762_16').hide();
                $('#vhsjs_hide_130_1599045762_16').show();
                $('#129_1599045762_16').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_130_1599045762_16" onclick="$('#129_1599045762_16').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_130_1599045762_16').hide();
                $('#vhsjs_view_130_1599045762_16').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="130_1599045762_16" id="129_1599045762_16" style="display: none"><div class="arrow-slidedown"><blockquote>In the last decade, many scientific applications have been significantly accelerated by large-scale GPU systems.
However, the movement of non-contiguous GPU-resident data is one of the most challenging parts to scale these applications using communication middleware like MPI.
Although plenty of research has discussed improving non-contiguous data movement within communication middleware, the pack/unpacking operations on GPU are still expensive. They cannot be hidden due to the limitation of MPI standard and the not-well-optimized designs in existing MPI implementations for GPU-resident data.
Consequently, application developers tend to implement customized packing/unpacking kernels to improve GPU utilization by avoiding unnecessary synchronizations in MPI routines.
However, this reduces productivity as well as performance as it cannot overlap the packing/unpacking operations with communication.
In this paper, we propose a novel approach to achieve low-latency and high-bandwidth by dynamically fusing the packing/unpacking GPU kernels to reduce the expensive kernel launch overhead.
The evaluation of the proposed designs shows up to 8X and 5X performance improvement for sparse and dense non-contiguous layout, respectively, compared to the state-of-the-art approaches on the Lassen system.
Similarly, we observe up to 19X improvement over existing approaches on the ABCI system.
Furthermore, the proposed design also outperforms the production library, such as SpectrumMPI, OpenMPI, and MVAPICH2, by many orders of magnitude.</blockquote></div></div></div></div><div class="slot-urls"></div></div><div class="slot-entry"><a name="pap184" tabindex="-1"></a><div class="slot-title">Autoscaling High-Throughput Workloads on Container Orchestrators <a href="calendar/submissions/sess105--pap184.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Chao Zheng, Nathaniel Kremer-Herman, Tim Shaffer, and Douglas Thain (University of Notre Dame)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_132_1599045762_16" onclick="$('#vhsjs_view_132_1599045762_16').hide();
                $('#vhsjs_hide_132_1599045762_16').show();
                $('#131_1599045762_16').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_132_1599045762_16" onclick="$('#131_1599045762_16').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_132_1599045762_16').hide();
                $('#vhsjs_view_132_1599045762_16').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="132_1599045762_16" id="131_1599045762_16" style="display: none"><div class="arrow-slidedown"><blockquote>High-throughput computing (HTC) workloads seek to complete as many jobs as possible over a long period of time. Such workloads require efficient execution of many parallel jobs and can occupy a large number of resources for a long time, such that full utilization is the normal state of an HTC facility. The widespread use of container orchestrators eases the deployment of HTC frameworks across different platforms, which also provides an opportunity to scale up HTC workloads with almost infinite resources on the public cloud. However, the autoscaling mechanisms of container orchestrators are primarily designed to support latency-sensitive microservices, and result in unexpected behavior when presented with HTC workloads. In this paper, we design a feedback autoscaler, High Throughput Autoscaler (HTA), that leverages the unique characteristics of the HTC workload to autoscales the resource pools used by HTC workloads on container orchestrators. HTA takes into account a reference input, the real-time status of the jobs’ queue, as well as two feedback inputs, resource consumption of jobs, and the resource initialization time of the container orchestrator. We implement HTA using the Makeflow workload manager, Work Queue job scheduler, and the Kubernetes cluster manager. We evaluate its performance on both CPU-bound and IO-bound workloads. The evaluation results show that, by using HTA, we improve resource utilization by 5.6× with a slight increase in execution time (about 15%) for the CPU-bound workload, and shorten the workload execution time by up to 3.65× for the IO-bound workload.</blockquote></div></div></div></div><div class="slot-urls"></div></div><div class="slot-entry"><a name="pap233" tabindex="-1"></a><div class="slot-title">SSP: Speeding up Small Flows for Proactive Transport in Datacenters. <a href="calendar/submissions/sess105--pap233.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Yang Bai, Dezun Dong, Shan Huan, Zejia Zhou, and Xiangke Liao (National University of Defense Technology)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_134_1599045762_17" onclick="$('#vhsjs_view_134_1599045762_17').hide();
                $('#vhsjs_hide_134_1599045762_17').show();
                $('#133_1599045762_17').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_134_1599045762_17" onclick="$('#133_1599045762_17').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_134_1599045762_17').hide();
                $('#vhsjs_view_134_1599045762_17').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="134_1599045762_17" id="133_1599045762_17" style="display: none"><div class="arrow-slidedown"><blockquote>Proactive transport nowadays has drawn much attention because of fast convergence, near-zero queueing and low latency. Proactive protocols, however, need an extra RTT to allocate ideal sending rate for new flows. To solve this, some studies, such as pHost, Homa, send unscheduled packets with line rate in the first RTT, which will causes severe network congestion. To avoid queue buildup, Aeolus directly drops unscheduled packets when congestion occurs. Nevertheless, based on our experiment, a considerable part of small flows (0-100KB) will be completed in the first RTT under 100 Gbps network, so that dropping unscheduled packets will severely affect performance of the small flows.<br><br>In this paper we propose SSP, a new scheme aimed to eliminate the extra RTT delay and improve the FCT of small flows under proactive mechanisms. Like pHost and Homa, SSP sends unscheduled packets at line rate when new flow arrives. Different from Aeolus, SSP selectively drops scheduled packets once queue buildup happens in switch, thus protecting unscheduled packets which are more likely belong to small flows. Besides, based on the short-job-first principle, we give relative higher priorities for small flows at the sender. Our simulation results with realistic workloads show that SSP can improve the FCT of small flows significantly. Specifically, under Web Search workload, SSP facilitates nearly 63% of 0-100 KB flows to complete one RTT faster. Also, SSP reduces the tail FCT by 56.8% at the 99th percentile compared with Expresspass and 29.2% compared with Aeolus while not leads to large queue buildup.</blockquote></div></div></div></div><div class="slot-urls"></div></div><div class="slot-entry"><a name="pap257" tabindex="-1"></a><div class="slot-title">Quantifying the impact of network congestion on application performance and network metrics <a href="calendar/submissions/sess105--pap257.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Yijia Zhang (Boston University); Taylor Groves, Brandon Cook, and Nicholas Wright (Lawrence Berkeley National Laboratory); and Ayse K. Coskun (Boston University)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_136_1599045762_17" onclick="$('#vhsjs_view_136_1599045762_17').hide();
                $('#vhsjs_hide_136_1599045762_17').show();
                $('#135_1599045762_17').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_136_1599045762_17" onclick="$('#135_1599045762_17').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_136_1599045762_17').hide();
                $('#vhsjs_view_136_1599045762_17').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="136_1599045762_17" id="135_1599045762_17" style="display: none"><div class="arrow-slidedown"><blockquote>In modern high-performance computing (HPC) systems, network congestion is an important factor that contributes to performance degradation. However, how network congestion impacts application performance is not fully understood. As Aries network, a recent HPC network architecture featuring a dragonfly topology, is equipped with network counters measuring packet transmission statistics on each router, these network metrics can potentially be utilized to understand network performance. In this work, by experiments on a large HPC system, we quantify the impact of network congestion on various applications' performance in terms of execution time, and we correlate application performance with network metrics. Our results demonstrate diverse impacts of network congestion: while applications with intensive MPI operations (such as HACC and MILC) suffer from more than 40% extension in their execution times under network congestion, applications with less intensive MPI operations (such as Graph500 and HPCG) are mostly not affected. We also demonstrate that a stall-to-flit ratio metric derived from Aries network counters is positively correlated with performance degradation and, thus, this metric can serve as an indicator of network congestion in HPC systems.</blockquote></div></div></div></div><div class="slot-urls"></div></div><div class="slot-entry"><a name="pap275" tabindex="-1"></a><div class="slot-title">Analysis of Cooling Water Temperature Impact on Computing Performance and Energy Consumption <a href="calendar/submissions/sess105--pap275.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Jorji Nonaka (RIKEN R-CCS); Toshihiro Hanawa (The University of Tokyo, JCAHPC); and Fumiyoshi Shoji (RIKEN R-CCS)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_138_1599045762_18" onclick="$('#vhsjs_view_138_1599045762_18').hide();
                $('#vhsjs_hide_138_1599045762_18').show();
                $('#137_1599045762_18').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_138_1599045762_18" onclick="$('#137_1599045762_18').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_138_1599045762_18').hide();
                $('#vhsjs_view_138_1599045762_18').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="138_1599045762_18" id="137_1599045762_18" style="display: none"><div class="arrow-slidedown"><blockquote>The hot water cooling technique has been widely accepted as one of the standard techniques to improve the energy efficiency for the HPC and Data Centers. However, the higher operating temperature may impact the CPU power consumption due to the leakage current. Moreover, it may degrade the computational performance due to the DVFS mechanism activated to maintain the power and temperature within the TDP limit. In that sense, to fairly evaluate the efficiency of the hot water cooling technique, it becomes important to take into consideration not only the energy reduction on the HPC facility side (cooling system) but also the impact on the power consumption and the performance degradation on the HPC system side. In this paper, we utilized the Oakforest-PACS system and its facility, jointly administrated by University of Tsukuba and The University of Tokyo, in order to execute a quantitative and systematic analysis on the impact of the cooling water temperature onto the HPC system and its facility. For this purpose, we utilized lower (9oC) and higher (18oC) cooling water temperature other than the regular operational temperature (12oC). Contrary to the gain in the energy consumption, on the HPC facility side, when using higher cooling water temperature, we observed an increase in the number of nodes suffering from performance degradation on the HPC system side. As a result, it can directly increase the probability of including low-performance nodes on multiple node jobs, and thus affecting their overall performance, especially during barrier synchronizations.</blockquote></div></div></div></div><div class="slot-urls"></div></div></div><div class="session-entry"><span class="session-date">Wednesday</span> <span class="session-time">15:10-16:00</span> <a href="calendar/sessions/sess107.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="room-name">Zoom<br /></span><span class="session-event-type">Paper</span><br /><div class="session-title">T4: Framework for Data and Storage</div><div class="session-chair">Chair: Suren Byna (Lawrence Berkeley Lab)<br /></div><div class="slot-entry"><a name="pap240" tabindex="-1"></a><div class="slot-title">E2Clab: Exploring the Computing Continuum through Repeatable, Replicable and Reproducible Edge-to-Cloud Experiments <a href="calendar/submissions/sess107--pap240.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Daniel Rosendo (Inria); Pedro Silva (Hasso-Plattner Institut); Matthieu Simonin (Inria); Alexandru Costan (IRISA, INSA Rennes); and Gabriel Antoniu (Inria)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_140_1599045762_19" onclick="$('#vhsjs_view_140_1599045762_19').hide();
                $('#vhsjs_hide_140_1599045762_19').show();
                $('#139_1599045762_19').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_140_1599045762_19" onclick="$('#139_1599045762_19').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_140_1599045762_19').hide();
                $('#vhsjs_view_140_1599045762_19').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="140_1599045762_19" id="139_1599045762_19" style="display: none"><div class="arrow-slidedown"><blockquote>Distributed digital infrastructures for computation and analytics are now evolving towards an interconnected ecosystem allowing complex applications to be executed from IoT Edge devices to the HPC Cloud (aka the Computing Continuum, the Digital Continuum, or the Transcontinuum). Understanding end-to-end performance in such a complex continuum is challenging. This breaks down to reconciling many, typically contradicting application requirements and constraints with low-level infrastructure design choices. One important  challenge is to accurately reproduce relevant behaviors of a given application workflow and representative settings of the physical infrastructure underlying this complex continuum. In this paper we introduce a rigorous methodology for such a process and validate it through E2Clab.  It is the first platform to support the complete analysis cycle of an application on the Computing Continuum: (i) the configuration of the experimental environment, libraries and frameworks; (ii) the mapping between the application parts and machines on the Edge, Fog and Cloud; (iii) the deployment of the application on the infrastructure; (iv) the automated execution; and (v) the gathering of experiment metrics. We illustrate its usage with  a real-life application deployed on the Grid'5000 testbed, showing that our framework allows one to understand and improve performance, by correlating it to the parameter settings, the resource usage and the specifics of the underlying infrastructure.</blockquote></div></div></div></div><div class="slot-urls"></div></div><div class="slot-entry"><a name="pap246" tabindex="-1"></a><div class="slot-title">Streaming File Transfer Optimization for Distributed Science Workflows <a href="calendar/submissions/sess107--pap246.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Davut Ucar and Engin Arslan (University of Nevada, Reno)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_142_1599045762_19" onclick="$('#vhsjs_view_142_1599045762_19').hide();
                $('#vhsjs_hide_142_1599045762_19').show();
                $('#141_1599045762_19').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_142_1599045762_19" onclick="$('#141_1599045762_19').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_142_1599045762_19').hide();
                $('#vhsjs_view_142_1599045762_19').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="142_1599045762_19" id="141_1599045762_19" style="display: none"><div class="arrow-slidedown"><blockquote>Driven by the advancements in computing and sensing technology, scientific applications started to generate a huge volume of data which needs to be streamed to high-performance computing clusters timely for real-time (or near-real time) processing, necessitating reliable network performance to operate seamlessly. However, existing data transfer applications are predominantly designed for batch workloads in a way that transfer configurations cannot be altered once they are set. This, in turn, severely limits streaming applications from adapting to changing dataset and network conditions therefore meeting stringent performance requirements. In this paper, we propose FStream to offer performance guarantees to time-sensitive streaming applications by dynamically adjusting transfer settings when system conditions deviate from initial assumptions to sustain high network performance throughout the runtime. We evaluate the performance of FStream by transferring several synthetic and real-world workloads in high-performance production networks and show that it offers up to 9x performance improvement over state-of-the-art data transfer solutions.</blockquote></div></div></div></div><div class="slot-urls"></div></div><div class="slot-entry"><a name="pap140" tabindex="-1"></a><div class="slot-title">Exploring the Potential of Fast Delta Encoding: Marching to a Higher Compression Ratio <a href="calendar/submissions/sess107--pap140.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Haoliang Tan, Zhiyuan Zhang, Xiangyu Zou, Qing Liao, and Wen Xia (HITSZ)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_144_1599045762_2" onclick="$('#vhsjs_view_144_1599045762_2').hide();
                $('#vhsjs_hide_144_1599045762_2').show();
                $('#143_1599045762_2').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_144_1599045762_2" onclick="$('#143_1599045762_2').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_144_1599045762_2').hide();
                $('#vhsjs_view_144_1599045762_2').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="144_1599045762_2" id="143_1599045762_2" style="display: none"><div class="arrow-slidedown"><blockquote>Delta compression (or called delta encoding) is a data reduction technique capable of calculating the differences (i.e., delta) among the very similar files and chunks, and is thus widely used for optimizing synchronization replication, backup/archival storage, cache compression, etc. However, delta compression is costly because of its time-consuming wordmatching operations for delta calculation. Existing delta encoding approaches, are either at a slow encoding speed, such as Xdelta and Zdelta, or at a low compression ratio, such as Ddelta and Edelta. In this paper, we propose Gdelta, a fast delta encoding approach with a high compression ratio, that improves the delta encoding speed by employing an improved fast Gear-based rolling hash for scanning fine-grained words, and a quick arraybased indexing scheme for word-matching, and then, after wordmatching, further batch compressing the rest to improve the compression ratio. Our evaluation results driven by six real-world datasets suggest that Gdelta achieves encoding/decoding speedups of 2X&#8764;4X over the classic Xdelta and Zdelta approaches while increasing the compression ratio by about 10%&#8764;120%.</blockquote></div></div></div></div><div class="slot-urls"></div></div><div class="slot-entry"><a name="pap190" tabindex="-1"></a><div class="slot-title">A Staging Based Task Execution Framework for Data-driven Scientific Workflows <a href="calendar/submissions/sess107--pap190.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Zhe Wang and Pradeep Subedi (Rutgers University), Matthieu Dorier (Argonne National Laboratory), and Philip E. Davis and Manish Parashar (Rutgers University)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_146_1599045762_2" onclick="$('#vhsjs_view_146_1599045762_2').hide();
                $('#vhsjs_hide_146_1599045762_2').show();
                $('#145_1599045762_2').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_146_1599045762_2" onclick="$('#145_1599045762_2').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_146_1599045762_2').hide();
                $('#vhsjs_view_146_1599045762_2').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="146_1599045762_2" id="145_1599045762_2" style="display: none"><div class="arrow-slidedown"><blockquote>As scientific workflows increasingly use extreme-scale resources, the imbalance between higher computational capabilities, generated data volumes, and available I/O bandwidth is limiting the ability to translate these scales into insights. In-situ workflows (and the in-situ approach) are leveraging storage levels close to the computation in novel ways in order to reduce the required I/O. However, to be effective, it is important that the mapping and execution of such in-situ workflows adopts a data-driven approach, enabling in-situ tasks to be executed flexibly based upon data content. This paper first explores the design space for data-driven in-situ workflows. Specifically, it presents a model that captures different factors that influence the mapping, execution, and performance of data-driven in-situ workflows and experimentally studies the impact of different mapping decisions and execution patterns. The paper then presents the design, implementation, and experimental evaluation of a data-driven in-situ workflow execution framework that leverages in-memory distributed data management and user-defined task-triggers to enable efficient and scalable in-situ workflow execution.</blockquote></div></div></div></div><div class="slot-urls"></div></div><div class="slot-entry"><a name="pap196" tabindex="-1"></a><div class="slot-title">Flexible Data Redistribution in a Task-Based Runtime System <a href="calendar/submissions/sess107--pap196.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Qinglei Cao and George Bosilca (University of Tennessee); Wei Wu (Los Alamos National Laboratory); and Dong Zhong, Aurelien Bouteiller, and Jack Dongarra (University of Tennessee)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_148_1599045762_21" onclick="$('#vhsjs_view_148_1599045762_21').hide();
                $('#vhsjs_hide_148_1599045762_21').show();
                $('#147_1599045762_21').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_148_1599045762_21" onclick="$('#147_1599045762_21').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_148_1599045762_21').hide();
                $('#vhsjs_view_148_1599045762_21').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="148_1599045762_21" id="147_1599045762_21" style="display: none"><div class="arrow-slidedown"><blockquote>Data redistribution aims to reshuffle data to optimize some objective for an algorithm. The objective can be multi-dimensional, such as improving computational load balance or decreasing communication volume or cost, with the ultimate goal to increase the efficiency and therefore decrease the time-to-solution for the algorithm. The classical redistribution problem focuses on optimally scheduling communications when reshuffling data between two regular, usually block-cyclic, data distributions. Recently, task-based runtime systems have gained popularity as a potential candidate to address the programming complexity on the way to exascale. In addition to an increase in portability against complex hardware and software systems, task-based runtime systems have the potential to be able to more easily cope with less-regular data distribution, providing a more balanced computational load during the lifetime of the execution. 
In this scenario, it becomes paramount to develop a general redistribution algorithm for task-based runtime systems, which could support all types of regular and irregular data distributions. In this paper, we detail a flexible redistribution algorithm, capable of dealing with redistribution problems without constraints of data distribution and data size and implement it in a task-based runtime system, PaRSEC. Performance results show great capability compared to ScaLAPACK, and applications highlight an increased efficiency with little overhead in terms of data distribution and data size.</blockquote></div></div></div></div><div class="slot-urls"></div></div></div><div class="session-entry"><span class="session-date">Thursday</span> <span class="session-time">9:10-10:00</span> <a href="calendar/sessions/sess108.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="room-name">Zoom<br /></span><span class="session-event-type">Paper</span><br /><div class="session-title">T5: Programming, System Software and Container</div><div class="session-chair">Chair: Alfredo Goldman (USP)<br /></div><div class="slot-entry"><a name="pap268" tabindex="-1"></a><div class="slot-title">DeepClone: Scalable Live Migration of Deep Learning Models for Data Parallel Training <a href="calendar/submissions/sess108--pap268.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Bogdan Nicolae, Matthieu Dorier, Justin Wozniak, and Franck Cappello (Argonne National Laboratory)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_150_1599045762_22" onclick="$('#vhsjs_view_150_1599045762_22').hide();
                $('#vhsjs_hide_150_1599045762_22').show();
                $('#149_1599045762_22').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_150_1599045762_22" onclick="$('#149_1599045762_22').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_150_1599045762_22').hide();
                $('#vhsjs_view_150_1599045762_22').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="150_1599045762_22" id="149_1599045762_22" style="display: none"><div class="arrow-slidedown"><blockquote>Training modern deep neural network (DNN) models involves complex workflows triggered by model exploration, sensitivity analysis, explainability, etc.  A key primitive in this context is the ability to clone a model training instance, i.e. "fork" the training process in a potentially different direction, which enables comparisons of different evolution paths using variations of training data and model parameters. However, in a quest improve the training throughput, a mix of data parallel, model parallel, pipeline parallel and layer-wise parallel approaches are making the problem of cloning highly complex. In this paper, we explore the problem of efficient cloning 
under such circumstances. To this end, we leverage several properties of data-parallel training and layer-wise parallelism to design DeepClone, a cloning approach based on augmenting the execution graph to gain direct access to tensors, which are then sharded and reconstructed 
asynchronously in order to minimize runtime overhead, standby duration, readiness duration. Compared with state-of-art approaches, DeepClone shows orders of magnitude improvement for several classes of DNN models.</blockquote></div></div></div></div><div class="slot-urls"></div></div><div class="slot-entry"><a name="pap109" tabindex="-1"></a><div class="slot-title">Exploring Non-Volatility of Non-Volatile Memory for High Performance Computing Under Failures <a href="calendar/submissions/sess108--pap109.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Jie Ren, Kai Wu, and Dong Li (University of California, Merced)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_152_1599045762_23" onclick="$('#vhsjs_view_152_1599045762_23').hide();
                $('#vhsjs_hide_152_1599045762_23').show();
                $('#151_1599045762_23').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_152_1599045762_23" onclick="$('#151_1599045762_23').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_152_1599045762_23').hide();
                $('#vhsjs_view_152_1599045762_23').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="152_1599045762_23" id="151_1599045762_23" style="display: none"><div class="arrow-slidedown"><blockquote>Hardware failures and faults often result in application crash in HPC. The emergence of non-volatile memory (NVM) provides a solution to address this problem. Leveraging the non-volatility of NVM, one can build in-memory checkpoints or enable crash-consistent data objects. However, these solutions cause large memory consumption, extra writes to NVM, or disruptive changes to applications. We introduces a fundamentally new methodology to handle HPC under failures based on NVM. In particular, we attempt to use remaining data objects in NVM (possibly stale ones because of losing data updates in caches) to restart crashed applications. To address the challenge of possibly unsuccessful recomputation after the application restarts, we introduce a framework EasyCrash that uses a systematic approach to automatically decide how to selectively persist application data objects to significantly increase possibility of successful recomputation. EasyCrash enables up to 30\% improvement (20\% on average) in system efficiency at various system scales.</blockquote></div></div></div></div><div class="slot-urls"></div></div><div class="slot-entry"><a name="pap204" tabindex="-1"></a><div class="slot-title">HCL: Distributing Parallel Data Structures in Extreme Scales <a href="calendar/submissions/sess108--pap204.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Hariharan Devarajan, Anthony Kougkas, Keith Bateman, and Xian-He Sun (Illinois Institute of Technology Chicago)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_154_1599045762_23" onclick="$('#vhsjs_view_154_1599045762_23').hide();
                $('#vhsjs_hide_154_1599045762_23').show();
                $('#153_1599045762_23').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_154_1599045762_23" onclick="$('#153_1599045762_23').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_154_1599045762_23').hide();
                $('#vhsjs_view_154_1599045762_23').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="154_1599045762_23" id="153_1599045762_23" style="display: none"><div class="arrow-slidedown"><blockquote>Most parallel programs use irregular control flow and data structures, which are perfect for one-sided communication paradigms such as MPI or PGAS programming languages. However, these environments lack the presence of efficient function-based application libraries that can utilize popular communication fabrics such as TCP, IB, and RoCE. Additionally, there is a lack of high-performance data structure interfaces. We present Hermes Container Library (HCL), a high-performance distributed data structures library that offers high-level abstractions including hash-maps, sets, and queues. HCL uses a RPC over RDMA technology that implements a novel functional programming paradigm. In this paper, we present the HCL DataBox abstraction that enables fast data serialization of complex data types, data persistency on flash storage, and inter- and intra-node data access optimization via a hybrid data access model. Evaluation results from testing the performance of a set of HCL data structures shows that HCL programs are 2x to 12x faster compared to BCL, a state-of-the-art distributed data structure library.</blockquote></div></div></div></div><div class="slot-urls"></div></div><div class="slot-entry"><a name="pap221" tabindex="-1"></a><div class="slot-title">Predicting MPI Collective Communication Performance Using Machine Learning <a href="calendar/submissions/sess108--pap221.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Sascha Hunold (TU Wien), Abhinav Bhatele (University of Maryland), George Bosilca (University of Tennessee), and Peter Knees (TU Wien)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_156_1599045762_24" onclick="$('#vhsjs_view_156_1599045762_24').hide();
                $('#vhsjs_hide_156_1599045762_24').show();
                $('#155_1599045762_24').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_156_1599045762_24" onclick="$('#155_1599045762_24').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_156_1599045762_24').hide();
                $('#vhsjs_view_156_1599045762_24').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="156_1599045762_24" id="155_1599045762_24" style="display: none"><div class="arrow-slidedown"><blockquote>The Message Passing Interface (MPI) defines the semantics of data communication operations, while the implementing libraries provide several parameterized algorithms for each operation. Each algorithm of an MPI collective operation may work best on a particular system and may be dependent on the specific communication problem. Internally, MPI libraries employ heuristics to select the best algorithm for a given communication problem when being called by an MPI application. The majority of MPI libraries allow users to override the default algorithm selection, enabling the tuning of this selection process. The problem then becomes how to select the best possible algorithm for a specific case automatically.
In this paper, we address the algorithm selection problem for MPI collective communication operations. To solve this problem, we propose an auto-tuning framework for collective MPI operations based on machine-learning techniques. First, we execute a set of benchmarks of an MPI library and its entire set of collective algorithms. Second, for each algorithm, we fit a performance model by applying regression learners. Last, we use the regression models to predict the best possible (fastest) algorithm for an unseen communication problem. We evaluate our approach for different MPI libraries and several parallel machines. The experimental results show that our approach outperforms the standard algorithm selection heuristics, which are hard-coded into the MPI libraries, by a significant margin.</blockquote></div></div></div></div><div class="slot-urls"></div></div><div class="slot-entry"><a name="pap112" tabindex="-1"></a><div class="slot-title">Decomposing MPI Collectives for Exploiting Multi-lane Communication <a href="calendar/submissions/sess108--pap112.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Jesper Larsson Träff and Sascha Hunold (TU Wien)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_158_1599045762_24" onclick="$('#vhsjs_view_158_1599045762_24').hide();
                $('#vhsjs_hide_158_1599045762_24').show();
                $('#157_1599045762_24').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_158_1599045762_24" onclick="$('#157_1599045762_24').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_158_1599045762_24').hide();
                $('#vhsjs_view_158_1599045762_24').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="158_1599045762_24" id="157_1599045762_24" style="display: none"><div class="arrow-slidedown"><blockquote>Many modern, high-performance systems increase the cumulated node-bandwidth by offering more than a single communication network and/or by having multiple connections to the network, such that a single processor-core cannot by itself saturate the off-node bandwidth. Efficient algorithms and implementations for collective operations as found in, e.g., MPI, must be explicitly designed for exploiting such multi-lane capabilities. We are interested in gauging to which extent this might be the case.  We systematically decompose the MPI collectives into similar operations that can execute concurrently on and exploit multiple network lanes. Our decomposition is applicable to all standard MPI collectives (broadcast, gather, scatter, allgather, reduce allreduce, reduce-scatter, scan, alltoall), and our implementations' performance can be readily compared to the native collectives of any given MPI library. Contrary to expectation, our full-lane, performance guideline implementations in many cases show surprising performance improvements with different MPI libraries on a  dual-socket, dual-network Intel OmniPath cluster, indicating a large potential for improving the performance of native MPI library implementations. Our full-lane implementations are in many cases large factors faster than the corresponding MPI collectives. We see similar results on  a larger, dual-rail Intel InfiniBand cluster. The results indicate considerable room for improvement of the MPI collectives in current MPI libraries including a more efficient use of multi-lane capabilities.</blockquote></div></div></div></div><div class="slot-urls"></div></div><div class="slot-entry"><a name="pap151" tabindex="-1"></a><div class="slot-title">Power Budgeting of Big Data Applications in Container-based Clusters <a href="calendar/submissions/sess108--pap151.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Jonatan Enes (Universidade da Coruña (UDC), CITIC); Guillaume Fieni (University of Lille, INRIA); Roberto Rey Expósito (Universidade da Coruña (UDC), CITIC); Romain Rouvoy (University of Lille, INRIA); and Juan Touriño (Universidade da Coruña (UDC), CITIC)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_160_1599045762_25" onclick="$('#vhsjs_view_160_1599045762_25').hide();
                $('#vhsjs_hide_160_1599045762_25').show();
                $('#159_1599045762_25').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_160_1599045762_25" onclick="$('#159_1599045762_25').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_160_1599045762_25').hide();
                $('#vhsjs_view_160_1599045762_25').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="160_1599045762_25" id="159_1599045762_25" style="display: none"><div class="arrow-slidedown"><blockquote>Energy consumption is currently highly regarded on computing systems for many reasons, such as improving the environmental impact and reducing operational costs considering the rising price of energy. Previous works have analysed how to improve energy efficiency from the entire infrastructure down to individual computing instances (e.g., virtual machines). However, the research is more scarce when it comes to controlling energy consumption, specially in real time and at the software level. This paper presents a platform that manages a power budget to cap the energy along several hierarchies, from users to applications and down to individual computing instances. Using software containers as the underlying virtualization technology, the energy limitation is implemented thanks to the platform’s ability to monitor container energy consumption and dynamically adjust its CPU resources via vertical scaling as required. Several representative Big Data applications have been deployed on the proposed platform to prove the feasibility of this power budgeting approach for energy control, showing that it is possible to effectively distribute and enforce a power budget among several users and applications.</blockquote></div></div></div></div><div class="slot-urls"></div></div><div class="slot-entry"><a name="pap164" tabindex="-1"></a><div class="slot-title">Estimating Power Consumption of Containers and Virtual Machines in Data Centers <a href="calendar/submissions/sess108--pap164.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Xusheng Zhang, Ziyu Shen, Bin Xia, Zheng Liu, and Yun Li (Nanjing University of Posts and Telecommunications)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_162_1599045762_25" onclick="$('#vhsjs_view_162_1599045762_25').hide();
                $('#vhsjs_hide_162_1599045762_25').show();
                $('#161_1599045762_25').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_162_1599045762_25" onclick="$('#161_1599045762_25').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_162_1599045762_25').hide();
                $('#vhsjs_view_162_1599045762_25').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="162_1599045762_25" id="161_1599045762_25" style="display: none"><div class="arrow-slidedown"><blockquote>Virtualization technologies provide solutions of cloud computing. Virtual resource scheduling is a crucial task in data centers, and the power consumption of virtual resources is a critical foundation of virtualization scheduling. Containers are the smallest unit of virtual resource scheduling and migration. Although many effective models for estimating power consumption of virtual machines (VM) have been proposed, few power estimation models of containers have been put forth. In this paper, we offer a fast-training piecewise regression model based on decision tree to build a VM power estimation model and estimate the containers’ power by treating the container as a group of processes on the VM. In our model, we characterize the nonlinear relationship between power and features and realize the effective estimation of the containers on the VM. We evaluate the proposed model on 13 workloads in PARSEC and compare it with several models. The experimental results prove the effectiveness of our proposed model on most workloads. Moreover, the estimated power of the containers is in line with expectations.</blockquote></div></div></div></div><div class="slot-urls"></div></div></div><div class="session-entry"><span class="session-date">Thursday</span> <span class="session-time">14:00-14:50</span> <a href="calendar/sessions/sess109.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="room-name">Zoom<br /></span><span class="session-event-type">Paper</span><br /><div class="session-title">T6: HPC Applications</div><div class="session-chair">Chair: Hatem Ltaief (KAUST)<br /></div><div class="slot-entry"><a name="pap142" tabindex="-1"></a><div class="slot-title">Fast Scalable Approximate Nearest Neighbor Search for High-dimensional Data <a href="calendar/submissions/sess109--pap142.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Renga Bashyam K G and Sathish Vadhiyar (Indian Institute of Science)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_164_1599045762_27" onclick="$('#vhsjs_view_164_1599045762_27').hide();
                $('#vhsjs_hide_164_1599045762_27').show();
                $('#163_1599045762_27').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_164_1599045762_27" onclick="$('#163_1599045762_27').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_164_1599045762_27').hide();
                $('#vhsjs_view_164_1599045762_27').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="164_1599045762_27" id="163_1599045762_27" style="display: none"><div class="arrow-slidedown"><blockquote>K-Nearest Neighbor (k-NN) search is one of the most commonly used approaches for similarity search. It finds extensive applications in machine learning and data mining. This era of big data warrants efficiently scaling k-NN search algorithms for billion-scale datasets with high dimensionality. In this paper, we propose a solution towards this end where we use vantage point trees for partitioning the dataset across multiple processes and exploit an existing graph-based sequential approximate k-NN search algorithm called HNSW (Hierarchical Navigable Small World) for searching locally within a process. Our hybrid MPI-OpenMP solution employs techniques including exploiting MPI one-sided communication for reducing communication times and partition replication for better load balancing across processes. We demonstrate computation of k-NN for 10,000 queries in the order of seconds using our approach on ~8000 cores on a dataset with billion points in a 128-dimensional space. We also show 10X speedup over a completely k-d tree-based solution for the same dataset, thus demonstrating better suitability of our solution for high dimensional datasets. Our solution shows almost linear strong scaling.</blockquote></div></div></div></div><div class="slot-urls"></div></div><div class="slot-entry"><a name="pap172" tabindex="-1"></a><div class="slot-title">A Hybrid MPI+PGAS Approach to Improve Strong Scalability Limits of Finite Element Solvers <a href="calendar/submissions/sess109--pap172.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Niclas Jansson (KTH Royal Institute of Technology)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_166_1599045762_27" onclick="$('#vhsjs_view_166_1599045762_27').hide();
                $('#vhsjs_hide_166_1599045762_27').show();
                $('#165_1599045762_27').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_166_1599045762_27" onclick="$('#165_1599045762_27').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_166_1599045762_27').hide();
                $('#vhsjs_view_166_1599045762_27').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="166_1599045762_27" id="165_1599045762_27" style="display: none"><div class="arrow-slidedown"><blockquote>Current finite element codes scale reasonably well as long as each core has sufficient amount of local work that can balance communication costs. However, achieving efficient performance at exascale will require unreasonable large problem sizes, in particular for low-order methods, where the small amount of work per element already is a limiting factor on current post petascale machines. Key bottlenecks for these methods are sparse matrix assembly, where communication latency starts to limit performance as the number of cores increases, and linear solvers, where efficient overlapping is necessary to amortize communication and synchronization cost of sparse matrix vector multiplication and dot products. We present our work on improving strong scalability limits of message passing based general low-order finite element based solvers. Using lightweight one-sided communication offered by partitioned global address space languages (PGAS), we demonstrate that the scalability of performance critical, latency sensitive sparse matrix assembly can achieve almost an order of magnitude better scalability. Linear solvers are also addressed via a signaling put algorithm for low-cost point-to-point synchronization, achieving similar performance as message passing based linear solvers. We introduce a new hybrid MPI+PGAS implementation of the open source general finite element framework FEniCS, replacing the linear algebra backend with a new library written in Unified Parallel C (UPC).  A detailed description of the implementation and the hybrid interface to FEniCS is given, and the feasibility of the approach is demonstrated via a performance study of the hybrid implementation on Cray XC40 machines.</blockquote></div></div></div></div><div class="slot-urls"></div></div><div class="slot-entry"><a name="pap217" tabindex="-1"></a><div class="slot-title">Towards Data-Flow Parallelization for Adaptive Mesh Refinement Applications <a href="calendar/submissions/sess109--pap217.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Kevin Sala (Barcelona Supercomputing Center (BSC)), Alejandro Rico (Arm Research), and Vicenç Beltran (Barcelona Supercomputing Center (BSC))</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_168_1599045762_28" onclick="$('#vhsjs_view_168_1599045762_28').hide();
                $('#vhsjs_hide_168_1599045762_28').show();
                $('#167_1599045762_28').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_168_1599045762_28" onclick="$('#167_1599045762_28').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_168_1599045762_28').hide();
                $('#vhsjs_view_168_1599045762_28').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="168_1599045762_28" id="167_1599045762_28" style="display: none"><div class="arrow-slidedown"><blockquote>Adaptive Mesh Refinement (AMR) is a prevalent method used by distributed-memory simulation applications to adapt the accuracy of their solutions depending on the turbulent conditions in each of their domain regions. These applications are usually dynamic since their domain areas are refined or coarsened in various refinement stages during their execution. Thus, they periodically redistribute their workloads among processes to avoid load imbalance. Although the defacto standard for scientific computing in distributed environments is MPI, in recent years, pure MPI applications are being ported to hybrid ones, attempting to cope with modern multi-core systems. Recently, the Task-Aware MPI library was proposed to efficiently integrate MPI communications and tasking models, providing also the transparent management of communications issued by tasks.<br><br>In this paper, we demonstrate the benefits of porting AMR applications to data-flow programming models leveraging that novel hybrid approach. We exploit most of the application parallelism by taskifying all stages, allowing their natural overlap. We employ these techniques on the miniAMR proxy application, which mimics the refinement, load balancing, communication, and computation patterns of general AMR applications. We evaluate how this approach reduces the time in its computation and communication phases while achieving better programmability than other conventional hybrid techniques.</blockquote></div></div></div></div><div class="slot-urls"></div></div><div class="slot-entry"><a name="pap263" tabindex="-1"></a><div class="slot-title">Towards End-to-end SDC Detection for HPC Applications Equipped with Lossy Compression <a href="calendar/submissions/sess109--pap263.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Sihuan Li (UC, Riverside); Sheng Di (Argonne National Laboratory); Kai Zhao (UC, Riverside); Xin Liang (Oak Ridge National Laboratory); Zizhong Chen (UC, Riverside); and Franck Cappello (Argonne National Laboratory)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_170_1599045762_28" onclick="$('#vhsjs_view_170_1599045762_28').hide();
                $('#vhsjs_hide_170_1599045762_28').show();
                $('#169_1599045762_28').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_170_1599045762_28" onclick="$('#169_1599045762_28').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_170_1599045762_28').hide();
                $('#vhsjs_view_170_1599045762_28').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="170_1599045762_28" id="169_1599045762_28" style="display: none"><div class="arrow-slidedown"><blockquote>Data reduction techniques have been widely demanded and used by large-scale high performance computing (HPC) applications because of vast volumes of data to be produced and stored for post-analysis. Due to very limited compression ratios of lossless compressors, error-bounded lossy compression has become an indispensable part in many HPC applications nowadays, because it can significantly reduce science data volume with user-acceptable data distortion. Since the large- scale HPC applications equipped with lossy compression techniques always need to deal with vast volume of data, soft errors or silent data corruptions (SDC) are non-negligible. Although SDC detection techniques have been studied for years, no studies were performed toward the HPC applications with lossy compression, leaving a significant gap between these applications and confidence of execution results. To fill this gap, this paper proposes a couple of SDC detection strategies for scientific simulations with lossy compression. Experimental results on 4 widely used scientific simulation datasets show promising detection ability could be still obtained with two popular lossy compressors. Our parallel experiments with up to 1,024 cores confirm that the time overheads could be limited within 7.9%.</blockquote></div></div></div></div><div class="slot-urls"></div></div><div class="slot-entry"><a name="pap211" tabindex="-1"></a><div class="slot-title">Efficient Execution of Dynamic Programming Algorithms on Apache Spark <a href="calendar/submissions/sess109--pap211.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Mohammad Mahdi Javanmard and Zafar Ahmad (Stony Brook University), Jaroslaw Zola (University at Buffalo), Louis-Noël Pouchet (Colorado State University), and Rezaul Chowdhury and Robert Harrison (Stony Brook University)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_172_1599045762_29" onclick="$('#vhsjs_view_172_1599045762_29').hide();
                $('#vhsjs_hide_172_1599045762_29').show();
                $('#171_1599045762_29').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_172_1599045762_29" onclick="$('#171_1599045762_29').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_172_1599045762_29').hide();
                $('#vhsjs_view_172_1599045762_29').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="172_1599045762_29" id="171_1599045762_29" style="display: none"><div class="arrow-slidedown"><blockquote>One of the most important properties of distributed computing systems (e.g., Apache Spark, Apache Hadoop, etc) on clusters and computation clouds is the ability to scale out by adding more compute nodes to the cluster. This important feature can lead to performance gain provided the computation (or the algorithm) itself can scale out. In other words, the computation (or the algorithm) should be easily decomposable into smaller units of work to be distributed among the workers based on the hardware/software configuration of the cluster or the cloud. Additionally, on such clusters, there is an important trade-off between communication cost, parallelism, and memory requirement. Due to the scalability need as well as this trade-off, it is crucial to have a well-decomposable, adaptive, tunable, and scalable program. Tunability enables the programmer to find an optimal point in the trade-off spectrum to execute the program efficiently on a specific cluster. We design and implement well-decomposable and tunable dynamic programming algorithms from the Gaussian Elimination Paradigm (GEP), such as Floyd-Warshall’s all-pairs shortest path and Gaussian elimination without pivoting, for execution on Apache Spark. Our implementations are based on parametric multi-way recursive divide-&-conquer algorithms. We explain how to map implementations of those grid-based parallel algorithms to the Spark framework. Finally, we provide experimental results illustrating the performance, scalability, and portability of our Spark programs. We show that offloading the computation to an OpenMP environment (by running parallel recursive kernels) within Spark is at least partially responsible for a 2-5x speedup of the DP benchmarks.</blockquote></div></div></div></div><div class="slot-urls"></div></div></div><div class="session-entry"><span class="session-date">Thursday</span> <span class="session-time">14:50-15:40</span> <a href="calendar/sessions/sess110.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="room-name">Zoom<br /></span><span class="session-event-type">Paper</span><br /><div class="session-title">T7: IO, Visualization, and Machine Learning</div><div class="session-chair">Chair: Kento Sato (RIKEN)<br /></div><div class="slot-entry"><a name="pap262" tabindex="-1"></a><div class="slot-title">ECS2: A Fast Erasure Coding Library for GPU-Accelerated Storage Systems With Parallel & Direct IO <a href="calendar/submissions/sess110--pap262.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Chan-Jung Chang and Jerry Chou (National Tsing Hua University), Yu-Ching Chou (H3 Platform Inc.), and I-Hsin Chung (IBM T. J. Watson)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_174_1599045762_3" onclick="$('#vhsjs_view_174_1599045762_3').hide();
                $('#vhsjs_hide_174_1599045762_3').show();
                $('#173_1599045762_3').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_174_1599045762_3" onclick="$('#173_1599045762_3').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_174_1599045762_3').hide();
                $('#vhsjs_view_174_1599045762_3').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="174_1599045762_3" id="173_1599045762_3" style="display: none"><div class="arrow-slidedown"><blockquote>As data volume keeps increasing in a rapid rate, there is an urgent need for large, reliable and cost-effective storage systems. Erasure coding has drawn increasing attention because of its ability to ensure data reliability with higher storage efficiency, and it has been widely adopted in many distributed and large-scale storage systems, such as Azure cloud storage and HDFS. However, the storage efficiency of erasure code comes at the price of higher computing complexity. While many studies have shown the coding computations can be significantly accelerated using GPU, the overhead of data transfer between storage devices and GPUs become a new performance bottleneck. In this work, we designed and implemented, ECS2, a fast erasure coding library on GPU-accelerated storage to let users enhance their data protection with transparent IO performance and file system like programming interface. By taking advantage of the latest GPUDirect technology supported on Nvidia GPU, our library is able to bypass CPU and host memory copy from the IO path, so that both the computing and IO overhead from coding can be minimized. Using synthetic IO workload based on real storage system trace, we show that the IO latency can be reduced by 10%~20% with GPUDirect technology, and the overall IO throughput of a storage system can be improved up to 70%.</blockquote></div></div></div></div><div class="slot-urls"></div></div><div class="slot-entry"><a name="pap150" tabindex="-1"></a><div class="slot-title">tf-Darshan: Understanding Fine-grained I/O Performance in Machine Learning Workloads <a href="calendar/submissions/sess110--pap150.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Steven W. D. Chien and Artur Podobas (KTH Royal Institute of Technology), Ivy B. Peng (Lawrence Livermore National Laboratory), and Stefano Markidis (KTH Royal Institute of Technology)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_176_1599045762_3" onclick="$('#vhsjs_view_176_1599045762_3').hide();
                $('#vhsjs_hide_176_1599045762_3').show();
                $('#175_1599045762_3').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_176_1599045762_3" onclick="$('#175_1599045762_3').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_176_1599045762_3').hide();
                $('#vhsjs_view_176_1599045762_3').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="176_1599045762_3" id="175_1599045762_3" style="display: none"><div class="arrow-slidedown"><blockquote>Machine Learning applications on HPC systems have been gaining popularity in recent years. The upcoming large scale systems will offer tremendous parallelism for training through GPUs. However, another heavy aspect of Machine Learning is I/O, and this can potentially be a performance bottleneck. TensorFlow, one of the most popular Deep-Learning platforms, now offers a new profiler interface and allows instrumentation of TensorFlow operations. However, the current profiler only enables analysis at the TensorFlow platform level and does not provide system-level information. In this paper, we extend TensorFlow Profiler and introduce tf-Darshan, both a profiler and tracer, that performs instrumentation through Darshan. We use the same Darshan shared instrumentation library and implement a runtime attachment without using a system preload. We can extract Darshan profiling data structures during TensorFlow execution to enable analysis through the TensorFlow profiler. We visualize the performance results through TensorBoard, the web-based TensorFlow visualization tool. At the same time, we do not alter Darshan’s existing implementation. We illustrate tf-Darshan by performing two case studies on ImageNet image and Malware classification. We show that by guiding optimization using data from tf-Darshan, we increase POSIX I/O bandwidth by up to 19% by selecting data for staging on fast tier storage. We also show that Darshan has the potential of being used as a runtime library for profiling and providing information for future optimization.</blockquote></div></div></div></div><div class="slot-urls"></div></div><div class="slot-entry"><a name="pap269" tabindex="-1"></a><div class="slot-title">Extending High-Level Synthesis with High-Performance Computing Performance Visualization <a href="calendar/submissions/sess110--pap269.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Jens Huthmann (RIKEN R-CCS), Artur Podobas (Royal Institute of Technology), Lukas Sommer (TU-Darmstadt), Andreas Koch (TU Darmstadt), and Kentaro Sano (RIKEN R-CCS)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_178_1599045762_31" onclick="$('#vhsjs_view_178_1599045762_31').hide();
                $('#vhsjs_hide_178_1599045762_31').show();
                $('#177_1599045762_31').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_178_1599045762_31" onclick="$('#177_1599045762_31').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_178_1599045762_31').hide();
                $('#vhsjs_view_178_1599045762_31').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="178_1599045762_31" id="177_1599045762_31" style="display: none"><div class="arrow-slidedown"><blockquote>The recent maturity in High-Level Synthesis (HLS) has renewed the interest of using Field-Programmable Gate-Arrays (FPGAs) to accelerate High-Performance Computing (HPC) applications. Today, several studies have shown performance- and power-benefits of using FPGAs compared to existing approaches for several HPC applications with ample room for improvements. Unfortunately, tracing and visualizing the performance of applications running on FPGAs is nearly non-existent, and understanding FPGA performance is often left to intuition or expert guesses.<br><br>In this work, we hypothesize that existing profiling and visualization tools used in the HPC domain are suitable to visualize FPGA performance. Through several non-trivial hardware extensions to support both event- and state-driven trace collection, we show how modern HLS tools can support Paraver-- a state-of-the-art visualization and profiling tool well-known in HPC. Finally, we show how contribution can be used to visualize and provide unique insights into the execution of three well-known application kernels running on an FPGA, and how we can use the visualization to guide optimizations</blockquote></div></div></div></div><div class="slot-urls"></div></div><div class="slot-entry"><a name="pap132" tabindex="-1"></a><div class="slot-title">Parallel Particle Advection Bake-Off For Scientific Visualization Workloads <a href="calendar/submissions/sess110--pap132.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Roba Binyahib (University of Oregon), David Pugmire (Oak Ridge National Laboratory), and Abhishek Yenpure and Hank Childs (University of Oregon)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_180_1599045762_31" onclick="$('#vhsjs_view_180_1599045762_31').hide();
                $('#vhsjs_hide_180_1599045762_31').show();
                $('#179_1599045762_31').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_180_1599045762_31" onclick="$('#179_1599045762_31').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_180_1599045762_31').hide();
                $('#vhsjs_view_180_1599045762_31').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="180_1599045762_31" id="179_1599045762_31" style="display: none"><div class="arrow-slidedown"><blockquote>There are multiple algorithms for parallelizing particle advection for scientific visualization workloads. While many previous studies have contributed to the understanding of individual algorithms, our study aims to provide a holistic understanding of how algorithms perform relative to each other on various workloads. To accomplish this, we consider four popular parallelization algorithms and run a “bake-off” study (i.e., an empirical study) to identify the best matches for each. The study includes 216 tests, going to a concurrency of up to 8192 cores and considering data sets as large as 34 billion cells with 300 million particles. Overall, our study informs three important research questions: (1) which parallelization algorithms perform best for a given workload?, (2) why?, and (3) what are the unsolved problems in parallel particle advection? In terms of findings, we find that the seeding box is the most important factor in choosing the best algorithm, and also that there is a significant opportunity for improvement in execution time, scalability, and efficiency.</blockquote></div></div></div></div><div class="slot-urls"></div></div><div class="slot-entry"><a name="pap203" tabindex="-1"></a><div class="slot-title">Data Life Aware Model Updating Strategy for Stream-based Online Deep Learning <a href="calendar/submissions/sess110--pap203.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Wei Rang, Donglin Yang, and Dazhao Cheng (UNC Charlotte); Kun Suo (Kennesaw State University); and Wei Chen (Nvidia Corporation)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_182_1599045762_32" onclick="$('#vhsjs_view_182_1599045762_32').hide();
                $('#vhsjs_hide_182_1599045762_32').show();
                $('#181_1599045762_32').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_182_1599045762_32" onclick="$('#181_1599045762_32').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_182_1599045762_32').hide();
                $('#vhsjs_view_182_1599045762_32').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="182_1599045762_32" id="181_1599045762_32" style="display: none"><div class="arrow-slidedown"><blockquote>Many deep learning applications deployed in dynamic environments change over time, in which the training models are supposed to be continuously updated with streaming data in order to guarantee better descriptions on data trends. However, most of the state-of-the-art learning frameworks support well in offline training methods while omitting online model updating strategies. In this work, we propose and implement iDlaLayer, a thin middleware layer on top of existing training frameworks that streamlines the support and implementation of online deep learning applications. In pursuit of good model quality as well as fast data incorporation, we design a Data Life Aware model updating strategy (DLA), which builds training data samples according to contributions of data from different life stages, and considers the training cost consumed in model updating. We evaluate iDlaLayer’s performance through both simulations and experiments based on TensorflowOnSpark with three representative online learning workloads. Our experimental results demonstrate that iDlaLayer reduces the overall elapsed time of MNIST, Criteo and PageRank by 11.3%, 28.2% and 15.2% compared to the periodic update strategy, respectively. It further achieves an average 20% decrease in training cost and brings about 5% improvement in model quality against the traditional continuous training method.</blockquote></div></div></div></div><div class="slot-urls"></div></div><div class="slot-entry"><a name="pap235" tabindex="-1"></a><div class="slot-title">Optimizing GPU Memory Transactions for Convolution Operations <a href="calendar/submissions/sess110--pap235.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Gangzhao Lu and Weizhe Zhang (Harbin Institute of Technology) and Zheng Wang (University of Leeds)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_184_1599045762_32" onclick="$('#vhsjs_view_184_1599045762_32').hide();
                $('#vhsjs_hide_184_1599045762_32').show();
                $('#183_1599045762_32').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_184_1599045762_32" onclick="$('#183_1599045762_32').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_184_1599045762_32').hide();
                $('#vhsjs_view_184_1599045762_32').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="184_1599045762_32" id="183_1599045762_32" style="display: none"><div class="arrow-slidedown"><blockquote>Cnvolution computation is a common operation in deep neural networks (DNNs) and is often responsible for performance bottlenecks during training and inferencing. Existing approaches for accelerating convolution operations aim to reduce computational complexity. However, these strategies often increase the memory footprint with extra memory accesses, thereby leaving much room for performance improvement. This paper presents a novel approach to optimize memory access for convolution operations, specically targeting GPU execution. Our approach leverages two optimization techniques to reduce the number of memory operations for convolution operations performed on the width and height dimensions. For convolution computations on the width dimension, we exploit shuffle instructions to exchange the overlapped columns of the input for reducing the number of memory transactions. For convolution operations on the height dimension, we multiply each overlapped row of the input with multiple rows of a filter to compute multiple output elements to improve the data locality of row elements. We apply our approach to 2D and multi-channel 2D convolutions on an NVIDIA 2080Ti GPU. For 2D convolution, our approach delivers over 2x faster performance than the state-of-the-art image processing libraries. For multi-channel 2D convolutions, we obtain up to 2x speedups over the quickest algorithm of cuDNN.</blockquote></div></div></div></div><div class="slot-urls"></div></div></div></div><div class="centered"><div class="top-link"><a href="#top">Return to Top</a></div></div><hr /></div>
                <div class="etype-section"><div class="centered"><a name="sstype102" tabindex="-1"></a><div class="section-title">Posters</div></div><div class="section-entry"><div class="session-entry"><span class="session-date">Tuesday</span> <span class="session-time">14:50-15:40</span> <a href="calendar/sessions/sess104p.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="room-name">Zoom<br /></span><span class="session-event-type">Poster</span><br /><div class="session-title">Poster session</div><div class="session-chair">Chair: Kento Sato (RIKEN)<br /></div><div class="session-description"><a href="../posters/list.html">Accepted posters</a></div><span class="slot-entry"></span></div></div><div class="centered"><div class="top-link"><a href="#top">Return to Top</a></div></div><hr /></div>
                <div class="etype-section"><div class="centered"><a name="evtt103" tabindex="-1"></a><div class="section-title">Workshop</div></div>
                <div class="section-entry">
                <div class="session-entry"><span class="session-date">Monday</span> <span class="session-time">14:00-18:00</span> <a href="calendar/sessions/sess119.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="room-name">Zoom<br /></span><span class="session-event-type">Workshop</span><br /><div class="session-title">Intel technology for AI and Persistent memory</div><span class="slot-entry"></span></div>
                <div class="session-entry"><span class="session-date">Monday</span> <span class="session-time">16:00-19:00</span> <a href="calendar/sessions/sess116.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="room-name">Zoom<br /></span><span class="session-event-type">Workshop</span><br /><div class="session-title">Embracing Arm: a journey of porting and optimization to the latest Arm-based processors (EAHPC-2020)</div><div class="session-description"></div><span class="slot-entry"></span></div>
                <div class="session-entry"><span class="session-date">Monday</span> <span class="session-time">21:00-24:00</span> <a href="calendar/sessions/sess117.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="room-name">Zoom<br /></span><span class="session-event-type">Workshop</span><br /><div class="session-title">Energy Efficient HPC State of the Practice 2020 (EE HPC SOP 2020)</div><div class="session-description"></div><span class="slot-entry"></span></div>
                <div class="session-entry"><span class="session-date">Monday</span> <span class="session-time">22:00-26:30</span> <a href="calendar/sessions/sess118.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="room-name">Zoom<br /></span><span class="session-event-type">Workshop</span><br /><div class="session-title">Workshop on Monitoring and Analysis for HPC Systems Plus Applications (HPCMASPA 2020)</div><div class="session-description"></div><span class="slot-entry"></span></div>
                </div>
                <div class="centered"><div class="top-link"><a href="#top">Return to Top</a></div></div><hr /></div></td></tr></table></div></div><div class="created-date righted">Created 2020-9-2 6:22</div></body></html>
