<!doctype html>
<html class="no-js" lang="en">
<head>
		<meta charset="utf-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1.0" />
	<title>IEEE Cluster 2019</title>
	<link rel="stylesheet" type="text/css"
        href="https://clustercomp.org/2019/assets/css/styles_feeling_responsive.css" />
	<script src="https://clustercomp.org/2019/assets/js/modernizr.min.js"></script>

  <script src="https://ajax.googleapis.com/ajax/libs/webfont/1.5.18/webfont.js"></script>
  <script>
    WebFont.load({
      google: {
        families: [ 'Lato:400,700,400italic:latin', 'Volkhov::latin' ]
      }
    });
  </script>

  <noscript>
    <link href='http://fonts.googleapis.com/css?family=Lato:400,700,400italic|Volkhov' rel='stylesheet' type='text/css' />
  </noscript>

  
	
	<meta name="description" content="" />

	

	



	
	<link rel="icon" sizes="32x32" href="https://clustercomp.org/2019/assets/img/favicon-32x32.png" />




	
	<link rel="icon" sizes="192x192" href="https://clustercomp.org/2019/assets/img/touch-icon-192x192.png" />




	
	<link rel="apple-touch-icon-precomposed" sizes="180x180"
                                           href="https://clustercomp.org/2019/assets/img/apple-touch-icon-180x180-precomposed.png" />




	
	<link rel="apple-touch-icon-precomposed" sizes="152x152"
                                           href="https://clustercomp.org/2019/assets/img/apple-touch-icon-152x152-precomposed.png" />




	
	<link rel="apple-touch-icon-precomposed" sizes="144x144"
                                           href="https://clustercomp.org/2019/assets/img/apple-touch-icon-144x144-precomposed.png" />




	
	<link rel="apple-touch-icon-precomposed" sizes="120x120"
                                           href="https://clustercomp.org/2019/assets/img/apple-touch-icon-120x120-precomposed.png" />




	
	<link rel="apple-touch-icon-precomposed" sizes="114x114"
                                           href="https://clustercomp.org/2019/assets/img/apple-touch-icon-114x114-precomposed.png" />




	
	<link rel="apple-touch-icon-precomposed" sizes="76x76"
                                           href="https://clustercomp.org/2019/assets/img/apple-touch-icon-76x76-precomposed.png" />




	
	<link rel="apple-touch-icon-precomposed" sizes="72x72"
                                           href="https://clustercomp.org/2019/assets/img/apple-touch-icon-72x72-precomposed.png" />




	
	<link rel="apple-touch-icon-precomposed"
        href="https://clustercomp.org/2019/assets/img/apple-touch-icon-precomposed.png" />	




	
	<meta name="msapplication-TileImage"
        content="https://clustercomp.org/2019/assets/img/msapplication_tileimage.png" />




	
	<meta name="msapplication-TileColor" content="#fabb00" />



	<!-- Facebook Optimization -->
	<meta property="og:locale" content="en_EN" />
	
	<meta property="og:title" content="IEEE Cluster 2019" />
	<meta property="og:description" content="Website for IEEE Cluster 2019,
        September 2019, Albuquerque, New Mexico USA" />
	<meta property="og:url" content="https://clustercomp.org/2019//program/" />
	<meta property="og:site_name" content="IEEE Cluster 2019" />
	

	

	<!-- Search Engine Optimization -->
	

	<link type="text/plain" rel="author" href="https://clustercomp.org/2019/humansv2.txt" />

	

	

	
	<script src="https://s3-us-west-2.amazonaws.com/ieeeshutpages/gdpr/settings.js"></script>
	<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.0.3/cookieconsent.min.css"
					/>
	<script src="https://cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.0.3/cookieconsent.min.js"></script>
	<script>
	  window.addEventListener("load", function(){
	    window.cookieconsent.initialise(json)
	  });
	</script>

</head>
<body id="top-of-page" class="post">
	
	
<div id="navigation" class="sticky">
  <nav class="top-bar" role="navigation" data-topbar>
    <ul class="title-area">
      <li class="name">
      <h1 class="show-for-small-only"><a href="https://clustercomp.org/2019"
                      class="icon-tree"> IEEE Cluster 2019</a></h1>
    </li>
       <!-- Remove the class "menu-icon" to get rid of menu icon. Take out "Menu" to just have icon alone -->
      <li class="toggle-topbar menu-icon"><a href="#"><span>Navigation</span></a></li>
    </ul>
    <section class="top-bar-section">

      <ul class="right">
        

              

          
          
        

              

          
          
        

              

          
          
        

              

          
          
        

              

          
          
        

              

          
          
        

              

          
          
        

              

          
          
        

              

          
          
        

              

          
          
        

              

          
          
        
        
      </ul>

      <ul class="left">
        

              

          
          

            
            
              <li><a href="https://clustercomp.org/2019/">Home</a></li>
              <li class="divider"></li>

            
            
          
        

              

          
          

            
            
              <li><a href="https://clustercomp.org/2019/technical">Papers</a></li>
              <li class="divider"></li>

            
            
          
        

              

          
          

            
            
              <li><a href="https://clustercomp.org/2019/authors">Authors</a></li>
              <li class="divider"></li>

            
            
          
        

              

          
          

            
            
              <li><a href="https://clustercomp.org/2019/sponsors">Sponsors</a></li>
              <li class="divider"></li>

            
            
          
        

              

          
          

            
            
              <li><a href="https://clustercomp.org/2019/workshops">Workshops</a></li>
              <li class="divider"></li>

            
            
          
        

              

          
          

            
            
              <li><a href="https://clustercomp.org/2019/posters">Posters</a></li>
              <li class="divider"></li>

            
            
          
        

              

          
          

            
            
              <li><a href="https://clustercomp.org/2019/program">Program</a></li>
              <li class="divider"></li>

            
            
          
        

              

          
          

            
            
              <li><a href="https://clustercomp.org/2019/student">Mentoring</a></li>
              <li class="divider"></li>

            
            
          
        

              

          
          

            
            
              <li><a href="https://clustercomp.org/2019/registration">Registration</a></li>
              <li class="divider"></li>

            
            
          
        

              

          
          

            
            
              <li><a href="https://clustercomp.org/2019/committees">Committees</a></li>
              <li class="divider"></li>

            
            
          
        

              

          
          

            
            
              <li><a href="https://clustercomp.org/2019/venue">Venue</a></li>
              <li class="divider"></li>


              <li><a href="https://clustercomp.org/submit">Login</a></li>
              <li class="divider"></li>
            
            
          
        
        
      </ul>
    </section>
  </nav>
</div><!-- /#navigation -->

	

	

<div id="masthead">
	<div class="row">
		<div class="small-12 columns">
			<a id="logo" href="https://clustercomp.org/2019" title="IEEE Cluster
                        2019– International Conference on Cluster Computing">
				<img src="https://clustercomp.org/2019/assets/img/logo.png" alt="IEEE
                                Cluster 2019 – International Conference on Cluster Computing">
			</a>
		</div><!-- /.small-12.columns -->
	</div><!-- /.row -->
</div><!-- /#masthead -->











	<div class="row t30">
	<div class="medium-8 columns medium-offset-2 end">
		<article itemscope itemtype="https://schema.org/Article">
			<header>
				

				<span itemprop="name">
					<p class="subheadline">Technical Program</p>
					<h1>Technical Program at a Glance</h1>
				</span>
			</header>

<table align="center" class="page_box_table"><tr align="left" valign="top"><td align="left" class="page_box_print" valign="top"><a name="top"></a><div><div style="background: #284a83"></div></div><span id="async_error"></span><div style="margin: 40px;"><b style="font-size: 2em">IEEE Cluster 2019 Program</b><br /><br /><b style="font-size: 1.3em">Tuesday 24 September  8:45-9:00</b><hr /><br /><b>Cluster 2019 Opening<br />Room: Ambassador/Registry<br /></b><hr /><br /><br /><b style="font-size: 1.3em">Tuesday 24 September  9:00-10:00</b><hr /><br /><b>Keynote 1<br />Room: Ambassador/Registry<br /></b><hr /><br /><br /><b style="font-size: 1.3em">Tuesday 24 September  10:00-10:30</b><hr /><br /><b>Top 3 Papers of Cluster 2019 (1/3)<br />Room: Ambassador/Registry<br /></b><hr /><table class="cellpadding3px"><tr><td valign="top"><span style="white-space: nowrap">10:00-10:30</span></td><td valign="top"><b>Evaluating Burst Buffer Placement in HPC Systems </b><br /><i>Harsh Khetawat, Christopher Zimmer, Frank Mueller, Scott Atchley, Sudharshan Vazhkudai, Misbah Mubarak</i><br /></td></tr></table><br />Burst buffers are increasingly exploited in contemporary supercomputers to bridge the performance gap between compute and storage systems. The design of burst buffers, particularly the placement of these devices and the underlying network topology, impacts both performance and cost. As the cost of other components such as memory and accelerators is increasing, it is becoming more important that HPC centers provision burst buffers tailored to their workloads.
This work contributes a provisioning system to provide accurate, multi-tenant simulations that model realistic application and storage workloads from HPC systems. The framework aids HPC centers in modeling their workloads against multiple network and burst buffer configurations rapidly. In experiments with our framework, we provide acomparison of representative OLCF I/O workloads against multiple burst buffer designs. We analyze the impact of these designs on latency, I/O phase lengths, contention for network and storage devices, and choice of network topology.<br /><hr /><br /><br /><b style="font-size: 1.3em">Tuesday 24 September  10:30-11:00</b><hr /><br /><b>Break<br />Room: Wurlitzer<br /></b><hr /><br /><br /><b style="font-size: 1.3em">Tuesday 24 September  11:00-12:30</b><hr /><br /><b>Deep Learning 1<br />Room: Ambassador/Registry<br /></b><hr /><table class="cellpadding3px"><tr><td valign="top"><span style="white-space: nowrap">11:00-11:30</span></td><td valign="top"><b>Efficient User-Level Storage Disaggregation for Deep Learning </b><br /><i>Yue Zhu, Weikuan Yu, Bing Jiao, Kathryn Mohror, Adam Moody, Fahim Chowdhury</i><br /></td></tr></table><br />On large-scale High-Performance Computing (HPC) systems, applications are provisioned with aggregated resources to meet their peak demands for brief periods. This results in resource underutilization because application requirements vary a lot during execution. This problem is particularly pronounced for deep learning applications that are running on leadership HPC systems with a large pool of burst buffers in the form of flash or non-volatile memory (NVM) devices. In this paper, we examine the I/O patterns of deep neural networks and reveal their critical need of loading many small samples randomly for successful training. We have designed a special Deep Learning File System (DLFS) that provides a thin set of APIs. Particularly, we design the metadata management of DLFS through an in-memory tree-based sample directory and its file services through the user-level SPDK protocol that can disaggregate the capabilities of NVM Express (NVMe) devices to parallel training tasks. Our experimental results show that DLFS can dramatically
improve the throughput of training for deep neural networks on NVMe over Fabric, compared with the kernel-based Ext4 file system. Furthermore, DLFS achieves efficient user-level storage
disaggregation with very little CPU utilization.<br /><hr /><table class="cellpadding3px"><tr><td valign="top"><span style="white-space: nowrap">11:30-12:00</span></td><td valign="top"><b>FluentPS: A Parameter Server Design with Low-frequency Synchronization for Distributed Deep Learning </b><br /><i>Xin Yao, Xueyu Wu, Cho-Li Wang</i><br /></td></tr></table><br />With pursuing high accuracy on big datasets, current research prefers designing complex neural networks, which need to maximize data parallelism for short training time. Many distributed deep learning systems, such as MXNet and Petuum, widely use parameter server framework with relaxed synchronization models. Although these models could cost less on each synchronization, its frequency is still high among many workers, e.g., the soft barrier introduced by Stale Synchronous Parallel (SSP) model. In this paper, we introduce our parameter server design, namely FluentPS, which can reduce frequent synchronization and optimize communication overhead in a large-scale cluster. Different from using a single scheduler to manage all parameters synchronization in some previous designs, our system allows each server to independently adjust schemes for synchronizing its own parameter shard and overlaps the push and pull processes of different servers. We also explore two methods to improve the SSP model: (1) lazy execution of buffered pull requests to reduce the synchronization frequency and (2) a probability-based strategy to pause the fast worker at a probability under SSP condition, which avoids unnecessary waiting of fast workers. We evaluate ResNet-56 with the same large batch size at different cluster scales. While guaranteeing robust convergence, FluentPS gains up to 6× speedup and reduce 93.7% communication costs than PS-Lite. The raw SSP model causes up to 131× delayed pull requests than our improved synchronization model, which can provide fine-tuned staleness controls and achieve higher accuracy.<br /><hr /><table class="cellpadding3px"><tr><td valign="top"><span style="white-space: nowrap">12:00-12:30</span></td><td valign="top"><b>Performance Characterization of DNN Training using TensorFlow and PyTorch on Modern Clusters </b><br /><i>Arpan Jain, Ammar Ahmad Awan, Quentin Anthony, Hari Subramoni, Dhabaleswar K. Panda</i><br /></td></tr></table><br />The recent surge of Deep Learning (DL) models and applications can be attributed to the rise in computational resources, availability of large-scale datasets, and accessible DL frameworks such as TensorFlow and PyTorch. Because these frameworks have been heavily optimized for NVIDIA GPUs, several performance characterization studies exist for GPU-based training. However, there exist very few research studies that focus on CPU-based DNN training. In this paper, we provide an in-depth performance characterization of state-of-the-art Deep Neural Networks (DNNs) such as ResNet(s) and Inception-v3/v4 on multiple CPU architectures including Intel Xeon Broadwell, three variants of the Intel Xeon Skylake, AMD EPYC, and NVIDIA GPUs like K80, P100, and V100. We provide three key insights: 1) Multi-process (MP) training should be used even for a single-node as single-process (SP) approach cannot fully exploit all the cores, 2) Performance of both SP and MP depend on various features such as the number of cores, the processes per node (ppn), and DNN architecture, and 3) There is a non-linear and complex relationship between CPU/system characteristics (core-count, ppn, hyper-threading, etc) and DNN specifications such as inherent parallelism between layers. We further provide a comparative analysis for CPU and GPU based training as well as profiling analysis for Horovod. The fastest Skylake we had access to is up to 2.35x better than a K80 GPU but up to 3.32x slower than a V100 GPU. For ResNet-152 training, we observed that MP is up to 1.47x faster than SP and achieves 125x speedup on 128 Skylake nodes.<br /><hr /><br /><b>Parallel Applications Using Alternate Models<br />Room: Regal<br /></b><hr /><table class="cellpadding3px"><tr><td valign="top"><span style="white-space: nowrap">11:00-11:30</span></td><td valign="top"><b>Leveraging Task-Based Polar Decomposition Using PARSEC on Massively Parallel Systems </b><br /><i>Dalal Sukkari, Mathieu Faverge, Hatem Ltaief, David Keyes</i><br /></td></tr></table><br />This paper describes how to leverage task-based implementation of the polar decomposition on massively parallel systems using the PARSEC dynamic runtime system. Based on a formulation of the iterative QR Dynamically-Weighted Halley (QDWH) algorithm, our novel implementation reduces data traffic while exploiting high concurrency from the underlying hardware architecture. First, we replace the most time-consuming classical QR factorization phase with a new hierarchical variant, customized for the specific structure of the matrix during the QDWH iterations. The newly developed hierarchical QR for QDWH exploits not only the matrix structure, but also shortens the length of the critical path to maximize hardware occupancy. We then deploy PARSEC to seamlessly orchestrate, pipeline, and track the data dependencies of the various linear algebra building blocks involved during the iterative QDWH algorithm. PARSEC enables to overlap communications with computations thanks to its asynchronous scheduling of fine-grained computational tasks. It employs look-ahead techniques to further expose parallelism, while actively pursuing the critical path. In addition, we identify synergistic opportunities between the task-based QDWH algorithm and the PARSEC framework. We exploit them during the hierarchical QR factorization to enforce a locality- aware task execution. The latter feature permits to minimize the expensive inter-node communication, which represents one of the main bottlenecks for scaling up applications on challenging distributed-memory systems. We report numerical accuracy and performance results using well and ill-conditioned matrices. The benchmarking campaign reveals up to 2X performance speedup against the existing state-of-the-art implementation for the polar decomposition on 36,864 cores.<br /><hr /><table class="cellpadding3px"><tr><td valign="top"><span style="white-space: nowrap">11:30-12:00</span></td><td valign="top"><b>Engineering a Distributed Histogram Sort </b><br /><i>Roger Kowalewski, Pascal Jungblut, Karl Fuerlinger</i><br /></td></tr></table><br />Sorting is one of the most critical non-numerical algorithms and covers use cases in a wide spectrum of scientific applications. Although we can build upon excellent research over the last decades, scaling to thousands of processing units on modern many-core architectures reveals a gap between theory and practice.  We adopt ideas of the well-known quickselect and sample sort algorithms to minimize data movement. Our evaluation demonstrates that we can keep up with recently proposed distribution sort algorithms in large-scale experiments, without any assumptions on the input keys. Additionally, our implementation outperforms an efficient multi-threaded merge sort on a single node. Our implementation is based on a C++ PGAS approach with an STL-like interface and can easily be integrated into many application codes.  As part of the presented experiments, we further reveal challenges with multi-threaded MPI and one-sided communication.<br /><hr /><table class="cellpadding3px"><tr><td valign="top"><span style="white-space: nowrap">12:00-12:30</span></td><td valign="top"><b>Asynchronous Task-Based Execution of the Reverse Time Migration for the Oil and Gas Industry </b><br /><i>Amani Alonazi, Hatem Ltaief, Issam Said, Samuel Thibault, David Keyes</i><br /></td></tr></table><br />We propose a new framework for deploying Reverse Time Migration (RTM) simulations
on distributed-memory systems equipped with multiple GPUs. The software infrastructure engine relies on the StarPU dynamic runtime system to orchestrate the asynchronous scheduling of RTM computational tasks on the underlying resources. Besides dealing with the challenging hardware heterogeneity, StarPU has to support tasks with different workload characteristics, which stress disparate components of the hardware system. RTM is challenging in that it operates intensively at both ends of the memory hierarchy, with compute kernels running at the highest level of the memory system, possibly in GPU main memory, while I/O kernels are saving solution data to fast storage. We consider how to span the wide performance gap between the two extreme ends of the memory system, i.e., GPU memory and fast storage, on which large-scale RTM simulations routinely execute. To maximize hardware occupancy while maintaining high memory bandwidth
throughout the memory subsystem, our framework relies on the new out-of-core (OOC) feature from StarPU to prefetch data solutions in and out not only from/to the GPU/CPU main memory but also from/to the fast storage system. The OOC technique may trigger opportunities for overlapping expensive data movement with computations. StarPU enables to address this challenging problem of heterogeneity with a systematic approach that is oblivious to the targeted hardware architectures. Our resulting RTM framework can effectively be deployed on massively parallel GPU-based systems, while delivering performance scalability up to 500 GPUs.<br /><hr /><br /><br /><b style="font-size: 1.3em">Tuesday 24 September  12:30-14:00</b><hr /><br /><b>Lunch (on your own)<br />Room: Other<br /></b><hr /><br /><br /><b style="font-size: 1.3em">Tuesday 24 September  14:00-15:30</b><hr /><br /><b>Deep Learning 2<br />Room: Ambassador/Registry<br /></b><hr /><table class="cellpadding3px"><tr><td valign="top"><span style="white-space: nowrap">14:00-14:30</span></td><td valign="top"><b>A Quantitative Study of Deep Learning Training on Heterogeneous Supercomputers </b><br /><i>Jingoo Han, Luna Xu, M. Mustafa Rafique, Ali R. Butt, Seung-Hwan Lim</i><br /></td></tr></table><br />Deep learning (DL) has become a key technique for solving complex problems in scientific research and discovery. DL training for science is substantially challenging because it has to deal with massive quantities of multi-dimensional data. High-performance computing (HPC) supercomputers are increasingly being employed for meeting the exponentially growing demand for DL. Multiple GPUs and high-speed interconnect network are needed for supporting DL on HPC systems. However, the excessive use of GPUs without considering effective benefits leads to inefficient resource utilization of these expensive setups. In this paper, we conduct a quantitative analysis to gauge the efficacy of DL workloads on the latest HPC system and identify viability of next-generation DL-optimized heterogeneous supercomputers for enabling researchers to develop more efficient resource management and distributed DL middleware. We evaluate well-known DL models with large-scale datasets using the popular TensorFlow framework, and provide a thorough evaluation including scalability, accuracy, variability, storage resource, GPU-GPU/GPU-CPU data transfer, and GPU utilization. Our analysis reveals that the latest heterogeneous supercomputing cluster shows varying performance trend as compared to the existing literature for single- and multi-node training. To the best of our knowledge, this is the first work to conduct such a quantitative and comprehensive study of DL training on a supercomputing system with multiple GPUs.<br /><hr /><table class="cellpadding3px"><tr><td valign="top"><span style="white-space: nowrap">14:30-15:00</span></td><td valign="top"><b>Parallelizing Training of Deep Generative Models on Massive Scientific Datasets </b><br /><i>Sam Ade Jacobs, Brian Van Essen, Tim Moon, Jae Seung Yeom, David Hysom, Brian Spears, Rushil Anirudh, Jayaraman Thiagaranjan, Shusen Liu, Jim Gaffney, Peer-Timo Bremer, Tom Benson, Peter Robinson, Luc Peterson</i><br /></td></tr></table><br />Training deep neural networks on large scientific data is a challenging task that requires enormous compute power especially if no pre-trained models exist to initialize the process. We present a novel tournament method to train traditional as well as generative adversarial networks built on LBANN, a scalable deep learning framework optimized for HPC systems. LBANN combines multiple levels of parallelism and exploits some of the worlds largest supercomputers.

We demonstrate our framework by creating a complex predictive model based on multi-variate data from high-energy-density physics containing hundreds of millions of images and hundreds of millions scalar values from tens of millions of simulations of inertial confinement fusion. Our approach combines an HPC workflow and LBANN with optimized data ingestion and the new tournament- style training algorithm to produce a scalable neural network architecture using a CORAL-class supercomputer.<br /><hr /><table class="cellpadding3px"><tr><td valign="top"><span style="white-space: nowrap">15:00-15:30</span></td><td valign="top"><b>Quantifying the Impact of Memory Errors in Deep Learning </b><br /><i>Zhao Zhang, Lei Huang, Ruizhu Huang, Weijia Xu, Daniel S. Katz</i><br /></td></tr></table><br />The use of deep learning (DL) on HPC resources has become common as scientists  explore and exploit DL methods to solve domain problems.
On the other hand, in the coming exascale computing era, a high error rate is expected to be problematic for most HPC applications.
The impact of errors on DL applications, especially DL training, remains unclear given their stochastic nature. 
In this paper, we focus understanding DL training applications on HPC in the presence of silent data corruption.
Specifically, we design and perform a quantification study with three representative applications by manually injecting silent data corruption errors (SDCs) across the design space and compare training results with the error-free baseline.
The results show only 0.61--1.76% of SDCs cause training failures, and taking the SDC rate in modern hardware into account, the actual chance of a failure is one in thousands to millions of executions.
With this quantitatively measured impact, computing centers can make rational design decisions based on their application portfolio, the acceptable failure rate, and financial constraints; for example, they might determine their confidence in the correctness of training results performed processors without error correction code (ECC) RAM.
We also discover that over 75-90% of the SDCs that cause catastrophic errors can be easily detected by a training loss in the next iteration.
Thus we propose this error-aware software solution to correct catastrophic errors, as it has significantly lower time and space overhead compared to algorithm-based fault-tolerance (ABFT) and ECC.<br /><hr /><br /><b>Workflows<br />Room: Regal<br /></b><hr /><table class="cellpadding3px"><tr><td valign="top"><span style="white-space: nowrap">14:00-14:30</span></td><td valign="top"><b>NORNS: Extending Slurm to Support Data-Driven Workflows through Asynchronous Data Staging </b><br /><i>Alberto Miranda, Adrian Jackson, Tommaso Tocci, Iakovos Panourgias, Ramon Nou</i><br /></td></tr></table><br />As HPC systems move into the Exascale era, parallel file systems are struggling to keep up with the I/O requirements from data-intensive problems. While the inclusion of burst buffers has helped to alleviate this by improving I/O performance, it has also increased the complexity of the I/O hierarchy by adding additional storage layers each with its own semantics. This forces users to explicitly manage data movement between the different storage layers, which, coupled with the lack of interfaces to communicate data dependencies between jobs in a data-driven workflow, prevents resource schedulers from optimizing these transfers to benefit the cluster's overall performance. This paper proposes several extensions to job schedulers, prototyped using the Slurm scheduling system, to enable users to appropriately express the data dependencies between the different phases in their processing workflows. It also introduces a new service for asynchronous data staging called NORNS that coordinates with the job scheduler to orchestrate data transfers to achieve better resource utilization. Our evaluation shows that a workflow-aware Slurm exploits node-local storage more effectively, reducing the filesystem I/O contention and improving job running times.<br /><hr /><table class="cellpadding3px"><tr><td valign="top"><span style="white-space: nowrap">14:30-15:00</span></td><td valign="top"><b>Leveraging Machine Learning for Anticipatory Data Delivery in Extreme Scale In-situ Workflows </b><br /><i>Pradeep Subedi, Philip E. Davis, Manish Parashar</i><br /></td></tr></table><br />Extreme scale scientific workflows are composed of multiple applications that exchange data at runtime. Several data-related challenges are limiting the potential impact of such workflows. While data staging and in-situ models of execution have emerged as approaches to address data-related costs at extreme scales, increasing data volumes and complex data exchange patterns impact the effectiveness of such approaches. In this paper, we design and implement DESTINY, which is an autonomic data delivery mechanism for staging-based in-situ workflows. DESTINY dynamically learns the data access patterns of scientific workflow applications and leverages these patterns to decrease data access costs. Specifically, DESTINY uses machine learning techniques to anticipate future data accesses, proactively packages and delivers the data necessary to satisfy these requests as close to the consumer as possible and, when data staging processes and consumer processes are colocated, removes the need for inter-process communication by making these data available to the consumer as shared-memory objects. When consumer processes reside on nodes other than staging nodes, the data is packaged and stored in a format the client will likely access in future. This amortizes expensive data discovery and assembly operations typically associated with data staging. We experimentally evaluate the performance and scalability of DESTINY on leadership class platforms using synthetic applications and the S3D combustion workflow. We demonstrate that DESTINY is scalable and can achieve a reduction of up to 75% in read response time as compared to in-memory staging service for production scientific workflows.<br /><hr /><table class="cellpadding3px"><tr><td valign="top"><span style="white-space: nowrap">15:00-15:30</span></td><td valign="top"><b>Harmony: An Approach for Geo-distributed Processing of Big-Data Applications </b><br /><i>Han Zhang, Lavanya Ramapantulu, Yong Meng Teo</i><br /></td></tr></table><br />Big-data application processing is increasingly geo-distributed, a paradigm shift from the traditional cluster-based processing frameworks. As the communication time for data movement across geo-distributed data centers is not a design criterion for traditional cluster-based processing approaches, there are research gaps in the algorithms used for staging and scheduling big-data applications for geo-distributed clusters. We address these gaps by proposing Harmony, an approach consisting of both staging and scheduling strategies to minimize an application's total execution time.

	The staging strategy of Harmony exploits the intra-stage parallelism by having concurrent operators within a stage in contrast to the traditional Apache spark which uses fine-grained stages, thus reducing the computation time within each stage. Secondly, the scheduling strategy of Harmony reduces data transfers between geo-distributed data centers by exploiting data locality and thus reducing communication time and total execution time. The proposed approach Harmony achieves a speedup of two times with respect to geo-distributed Apache Spark. In addition, Harmony achieves a speedup of 1.6 times and 2.1 times when compared with the state-of-the-art framework Iridium for geo-distributed analytics over five locations with uniform and non-uniform network link bandwidths respectively.<br /><hr /><br /><br /><b style="font-size: 1.3em">Tuesday 24 September  15:30-16:00</b><hr /><br /><b>Break<br />Room: Wurlitzer<br /></b><hr /><br /><br /><b style="font-size: 1.3em">Tuesday 24 September  16:00-17:00</b><hr /><br /><b>Machine Learning<br />Room: Ambassador/Registry<br /></b><hr /><table class="cellpadding3px"><tr><td valign="top"><span style="white-space: nowrap">16:00-16:30</span></td><td valign="top"><b>HarpGBDT: Optimizing Gradient Boosting Decision Tree for Parallel Efficiency </b><br /><i>Bo Peng, Judy Qiu</i><br /></td></tr></table><br />Gradient Boosting Decision Tree (GBDT) is a widely used machine learning algorithm, whose 
training involves both irregular computation and random memory access and is challenging for system optimizations. In this paper, we conduct a comprehensive performance analysis of two state-of-the-art systems, XGBoost and LightGBM. They represent two typical parallel implementations for GBDT; one is data parallel and the other one is parallel over features. Substantial thread synchronization overhead, as well as the inefficiency of random memory access, is identified. We propose HarpGBDT, a new GBDT system designed from the perspective of parallel efficiency optimization. Firstly, we adopt a new tree growth method that selects the top K candidates of tree nodes to enable the use of more levels of parallelism without sacrificing the algorithm's accuracy. Secondly, we organize the training data and model data in blocks and propose a block-wise approach as a general model that enables the exploration of various parallelism options. Thirdly, we propose a mixed mode to utilize the advantages of a different mode of parallelism in different phases of training. By changing the configuration of the block size and parallel mode, HarpGBDT is able to attain better parallel efficiency. By extensive experiments on four datasets with different statistical characteristics on the Intel(R) Xeon(R) E5-2699 server, HarpGBDT on average performs 8x faster than XGBoost and 2.6x faster than LightGBM.<br /><hr /><table class="cellpadding3px"><tr><td valign="top"><span style="white-space: nowrap">16:30-17:00</span></td><td valign="top"><b>Training Google Neural Machine Translation on an Intel CPU Cluster </b><br /><i>Dhiraj Kalamkar, Kunal Banerjee, Sudarshan Srinivasan, Srinivas Sridharan, Evangelos Georganas, Mikhail Smorkalov, Cong Xu, Alexander Heinecke</i><br /></td></tr></table><br />Google's neural machine translation (GNMT) is state-of-the-art recurrent neural network (RNN/LSTM) based language translation application. It is computationally more demanding than well-studied convolutional neural networks (CNNs). Also, in contrast to CNNs, RNNs heavily mix compute and memory bound layers which requires careful tuning on a latency machine to optimally use fast on-die memories for best single processor performance. Additionally, due to massive compute demand, it is essential to distribute the entire workload among several processors and even compute nodes. To the best of our knowledge, this is the first work which attempts to scale this application on an Intel CPU cluster. Our CPU-based GNMT optimization, the first of its kind, achieves this by the following steps: (i) we choose a monolithic long short-term memory (LSTM) cell implementation from LIBXSMM library (specifically tuned for CPUs) and integrate it into TensorFlow, (ii) we modify GNMT code to use fused time step LSTM op for the encoding stage, (iii) we combine Horovod and Intel MLSL scaling libraries for improved performance on multiple nodes, and (iv) we extend the bucketing logic for grouping similar length sentences together to multiple nodes for achieving load balance across multiple ranks. In summary, we demonstrate that due to these changes we are able to outperform Google's stock CPU-based GNMT implementation by ~2x on single node and potentially enable more than 25x speedup using 16 node CPU cluster.<br /><hr /><br /><b>Clustering<br />Room: Regal<br /></b><hr /><table class="cellpadding3px"><tr><td valign="top"><span style="white-space: nowrap">16:00-16:30</span></td><td valign="top"><b>Volume and Surface Area-Based Cluster Validity Index </b><br /><i>Qi Li, Shihong Yue, Chang Sun, Yaru Wang, Xiaofeng Gao</i><br /></td></tr></table><br />Cluster validity index plays an important role in assessing the quality of the obtained clustering results. Existing validity indexes not only depend on special clustering algorithms and data structures, but also are very time-consuming due to a trial-and-error strategy which repeatedly performs clustering algorithms. Consequently, these validity indexes are supervised
and are very limited in practice. In this paper, a nonparametric density notation is defined and thereby the density of each point in a dataset is computed, and then all points are grouped into boundary and interior points that are used to compute the volume and surface area of all clusters. In view of the natural relationship between volume and surface area for any cluster, a new validity index is proposed to directly assess the clustering results, which is independent of clustering algorithms and data structures. A group of synthetic datasets and real datasets with various characteristics are used to test the proposed validity index. Results show that the proposed validity index can find the correct number of clusters. Since the new index is unsupervised and independent of clustering algorithms, and thus has less computational time than the existing indexes. Considering current background of big data and complex data distributions, the new validity index is very helpful to assess clustering results in real practice.<br /><hr /><table class="cellpadding3px"><tr><td valign="top"><span style="white-space: nowrap">16:30-17:00</span></td><td valign="top"><b>MuDBSCAN: An Exact Scalable DBSCAN Algorithm for Big Data Exploiting Spatial Locality </b><br /><i>Aditya Sarma, Poonam Goyal, Sonal Kumari, Anand Wani, Jagat Sesh Challa, Saiyedul Islam, Navneet Goyal</i><br /></td></tr></table><br />DBSCAN is one of the most popular and effective clustering algorithms that is capable of identifying arbitrary-shaped clusters and noise efficiently. However, its super-linear complexity makes it infeasible for applications involving clustering of Big Data. A major portion of the computation time of DBSCAN is taken up by the neighborhood queries, which becomes a bottleneck to its performance. We address this issue in our proposed micro-cluster based DBSCAN algorithm, MuDBSCAN, which identifies core-points even without performing neighbourhood queries and becomes instrumental in reducing the run-time of the algorithm. It also significantly reduces the computation time per neighbourhood query while producing exact DBSCAN clusters. Moreover, the micro-cluster based solution makes it scalable for high dimensional data. We also propose a highly scalable distributed implementation of MuDBSCAN, MuDBSCAN-D, to exploit a commodity cluster infrastructure. Experimental results demonstrate tremendous improvements in performance of our proposed algorithms as compared to their respective state-of-the-art solutions for various standard datasets. MuDBSCAN-D is an exact parallel solution for DBSCAN which is capable of processing massive amounts of data efficiently (1 billion data points in 41 minutes on a 32 node cluster), while producing a clustering that is same as that of traditional DBSCAN.<br /><hr /><br /><br /><b style="font-size: 1.3em">Tuesday 24 September  17:00-17:30</b><hr /><br /><b>Poster Blitz<br />Room: Ambassador/Registry<br /></b><hr /><br /><br /><b style="font-size: 1.3em">Tuesday 24 September  17:30-20:00</b><hr /><br /><b>Poster Session and Reception<br />Room: Wurlitzer<br /></b><hr /><br /><br /><b style="font-size: 1.3em">Wednesday 25 September  8:45-9:00</b><hr /><br /><b>Announcements<br />Room: Ambassador/Registry<br /></b><hr /><br /><br /><b style="font-size: 1.3em">Wednesday 25 September  9:00-10:00</b><hr /><br /><b>Keynote 2<br />Room: Ambassador/Registry<br /></b><hr /><br /><br /><b style="font-size: 1.3em">Wednesday 25 September  10:00-10:30</b><hr /><br /><b>Top 3 Papers of Cluster 2019 (2/3)<br />Room: Ambassador/Registry<br /></b><hr /><table class="cellpadding3px"><tr><td valign="top"><span style="white-space: nowrap">10:00-10:30</span></td><td valign="top"><b>Algorithm-Based Fault Tolerance for Parallel Stencil Computations </b><br /><i>Aurélien Cavelan, Florina M. Ciorba</i><br /></td></tr></table><br />The increase in HPC systems size and complexity, together with increasing on-chip transistor density, power limitations, and number of components, render modern HPC systems subject to soft errors. Silent data corruptions (SDCs) are typically caused by such soft errors in the form of bit-flips in the memory subsystem and hinder the correctness of scientific applications. This work addresses the problem of protecting a class of iterative computational kernels, called stencils, against SDCs when executing on parallel HPC systems. Existing SDC detection and correction methods are in general either inaccurate, inefficient, or targeting specific application classes that do not include stencils. This work proposes a novel algorithm-based fault tolerance (ABFT) method to protect scientific applications that contain arbitrary stencil computations against SDCs. The ABFT method can be applied both online and offline to accurately detect and correct SDCs in 2D and 3D parallel stencil computations. We present a formal model for the proposed method including theorems and proofs for the computation of the associated checksums as well as error detection and correction. We experimentally evaluate the use of the proposed ABFT method on a real 3D stencil-based application (HotSpot3D) via a fault-injection, detection, and correction campaign. Results show that the proposed ABFT method achieves less than 8% overhead compared to the performance of the unprotected stencil application. Moreover, it accurately detects and corrects SDCs. While the offline ABFT version corrects errors more accurately, it may incur a small additional overhead than its online counterpart.<br /><hr /><br /><br /><b style="font-size: 1.3em">Wednesday 25 September  10:30-11:00</b><hr /><br /><b>Break<br />Room: Wurlitzer<br /></b><hr /><br /><br /><b style="font-size: 1.3em">Wednesday 25 September  11:00-12:30</b><hr /><br /><b>Message Passing<br />Room: Ambassador/Registry<br /></b><hr /><table class="cellpadding3px"><tr><td valign="top"><span style="white-space: nowrap">11:00-11:30</span></td><td valign="top"><b>MPI Sessions: Evaluation of an Implementation in Open MPI </b><br /><i>Nathan Hjelm, Howard Pritchard, Samuel K. Gutiérrez, Daniel J. Holmes, Ralph Castain, Anthony Skjellum</i><br /></td></tr></table><br />The recently proposed MPI Sessions extensions to the MPI standard present a new paradigm for applications to use with MPI.  MPI Sessions has the potential to address several limitations of MPI's current specification: MPI cannot be initialized within an MPI process from different application components without a priori knowledge or coordination; MPI cannot be initialized more than once; and, MPI cannot be reinitialized after MPI finalization.  MPI Sessions also offers the possibility for more flexible ways for individual components of an application to express the capabilities they require from MPI at a finer granularity than is presently possible.

At this time, MPI Sessions has reached sufficient maturity for implementation and evaluation, which are the focuses of this paper.  This paper presents a prototype implementation of MPI Sessions, discusses certain of its performance characteristics, and describes its successful use in a large-scale production MPI application.  Overall, MPI Sessions is shown to be implementable, integrable with key infrastructure, and effective, but with certain overheads involving the initialization of MPI as well as communicator construction.  Small impacts on message-passing latency and throughput are noted.  Open MPI was used as the implementation vehicle, but results here are also relevant to other middleware stacks.<br /><hr /><table class="cellpadding3px"><tr><td valign="top"><span style="white-space: nowrap">11:30-12:00</span></td><td valign="top"><b>Give MPI Threading a Fair Chance: A Study of Multithreaded MPI Designs </b><br /><i>Thananon Patinyasakdikul, David Eberius, George Bosilca, Nathan Hjelm</i><br /></td></tr></table><br />The Message Passing Interface (MPI) has been one of the most prominent programming paradigms in highperformance computing (HPC) for the past decade. Lately, with changes in modern hardware leading to a drastic increase in the number of processor cores, developers of parallel applications are moving toward more integrated parallel programming paradigms, where MPI is used along with other, possibly node-level, programming paradigms, or MPI+X. MPI+threads emerged as one of the favorite choices in HPC community, according to a survey of the HPC community. However, threading support in MPI comes with many compromises to the overall performance delivered, and, therefore, its adoption is compromised.

This paper studies in depth the MPI multi-threaded implementation design in one of the leading MPI implementations, Open MPI, and expose some of the shortcomings of the current design. We propose, implement, and evaluate a new design of the internal handling of communication progress which allows for a significant boost in multi-threading performance, increasing the viability of MPI in the MPI+X programming paradigm.<br /><hr /><table class="cellpadding3px"><tr><td valign="top"><span style="white-space: nowrap">12:00-12:30</span></td><td valign="top"><b>Fast and Faithful Performance Prediction of MPI Applications: the HPL Case Study </b><br /><i>Tom Cornebize, Arnaud Legrand, Franz Christian Heinrich</i><br /></td></tr></table><br />Finely tuning MPI applications (number of processes, granularity, collective
operation algorithms, geometry and process placement) is critical to obtain good performance on
supercomputers.  With a rising cost of modern supercomputers, running parallel
applications at scale solely to optimize their performance is extremely
expensive. Having inexpensive but faithful predictions of expected performance
could be a great help for researchers and system administrators.
The methodology we propose captures the complexity of adaptive applications by
emulating the MPI code while skipping insignificant parts. We demonstrate its
capability with High Performance Linpack (HPL), the benchmark used to rank
supercomputers in the TOP500 and which requires a careful tuning.  We explain
how we both (1) extended the SimGrid's SMPI simulator and slightly modified the
open-source version of HPL to allow a fast emulation on a single commodity
server at the scale of a supercomputer and (2) how to model the different
components (network, BLAS, ...) of the platform.  We show that a careful
modeling of both spatial and temporal node variability allows us to obtain
predictions within a few percents of real experiments (see Figure 1).<br /><hr /><br /><b>Data Centers and Clouds<br />Room: Regal<br /></b><hr /><table class="cellpadding3px"><tr><td valign="top"><span style="white-space: nowrap">11:00-11:30</span></td><td valign="top"><b>MBECN: Enabling ECN with Micro-burst Traffic in Multi-queue Data Center </b><br /><i>Kexi Kang, Jinghui Zhang, Jiahui Jin, Dian Shen, Junzhou Luo, Wenxin Li, Zhiang Wu</i><br /></td></tr></table><br />Modern multi-queue data centers often use the standard Explicit Congestion Notification (ECN) scheme to achieve high network performance. However, one substantial drawback of this approach is that micro-burst traffic can cause the instantaneous queue length to exceed the ECNs threshold, resulting in numerous mismarkings. After enduring too many mismarkings, senders may overreact, leading to severe throughput loss. As a solution to this dilemma, we propose our own adaptation the Micro-burst ECN (MBECN) scheme-to mitigate mismarking. MBECN finds a more appropriate threshold baseline for each queue to absorb micro-bursts, based on steady-state analysis and an ideal generalized processor sharing (GPS) model. By adopting a queue-occupation-based dynamically adjusting algorithm, MBECN effectively handles packet backlog without hurting latency. Through testbed experiments, we find that MBECN improves throughput by ~20% and reduces flow completion time (FCT) by ~40%. Using large scale simulations, we find that throughput can be improved by 1.5~2.4x with DCTCP and 1.26~1.35x with ECN*. We also measure network delay and find that latency only increases by 7.36%.<br /><hr /><table class="cellpadding3px"><tr><td valign="top"><span style="white-space: nowrap">11:30-12:00</span></td><td valign="top"><b>Large-Scale Analysis of the Docker Hub Dataset </b><br /><i>Nannan Zhao, Vasily Tarasov, Hadeel Albahar, Ali Anwar, Lukas Rupprecht, Dimitrios Skourtis, Amit S. Warke, Mohamed Mohamed, Ali R. Butt</i><br /></td></tr></table><br />Docker containers have become a prominent solution for supporting modern enterprise applications due to the highly desirable features of isolation, low overhead, and efficient packaging of the execution environment. Containers are created from images which are shared between users via a Docker registry. The amount of data Docker registries store is massive; for example, Docker Hub, a popular public registry, stores at least half a million public images. In this paper, we analyze over 167 TB of uncompressed Docker Hub images, characterize them using multiple metrics and evaluate the potential of file-level deduplication in Docker Hub. Our analysis helps to make conscious decisions when designing storage for containers in general and Docker registries in particular. For example, only 3% of the files in images are unique, which means file-level deduplication has a great potential to save storage space for the registry. Our findings can motivate and help improve the design of data reduction, caching, and pulling optimizations for registries.<br /><hr /><table class="cellpadding3px"><tr><td valign="top"><span style="white-space: nowrap">12:00-12:30</span></td><td valign="top"><b>DP_Greedy: A Two-Phase Caching Algorithm for Mobile Cloud Services </b><br /><i>Yang Wang</i><br /></td></tr></table><br />In this paper, we study the data caching problem in mobile cloud environment where multiple correlated data items could be packed and migrated to serve a predefined sequence of requests. By leveraging the spatial and temporal trajectory of requests, we propose a two-phase caching algorithm. We first investigate the correlation between data items to determine whether or not two data items could be packed to transfer, and then combine the algorithm proposed in \cite{wang2017data} and a greedy strategy to design a two-phase algorithm, named \emph{DP\_Greedy}, for effectively caching these shared data items to serve a predefined sequence of requests. Under homogeneous cost model, we prove the proposed algorithm is at most $2/\alpha$ times worse than the optimal one in terms of the total service cost, where $\alpha$ is the discount factor we defined, and also show that the algorithm can achieve this results within $O(mn^2)$ time and $O(mn)$ space complexity for $m$ caches to serve a $n$-length sequence. We evaluate our algorithm by effectively implementing it and comparing it with the non-packing case, the result show the proposed DP\_Greedy algorithm not only presents excellent performances but is also more in line with the actual situation.<br /><hr /><br /><br /><b style="font-size: 1.3em">Wednesday 25 September  12:30-14:00</b><hr /><br /><b>Lunch (on your own)<br />Room: Other<br /></b><hr /><br /><br /><b style="font-size: 1.3em">Wednesday 25 September  14:00-15:30</b><hr /><br /><b>Cluster Communication<br />Room: Ambassador/Registry<br /></b><hr /><table class="cellpadding3px"><tr><td valign="top"><span style="white-space: nowrap">14:00-14:30</span></td><td valign="top"><b>X-RDMA: Effective RDMA Middleware in Large-scale Production Environments </b><br /><i>Teng Ma, Tao Ma, Zhuo Song, Jingxuan Li, Huaixin Chang, Kang Chen, Hai Jiang, Yongwei Wu</i><br /></td></tr></table><br />X-RDMA is a communication middleware deployed and heavily used in Alibabas large-scale cluster hosting cloud storage and database systems. Unlike recent research projects which purely focus on squeezing out the raw hardware performance, it puts emphasis on robustness, scalability, and maintainability of large-scale production clusters. X-RDMA integrates necessary features, not available in current RDMA ecosystem, to release the developers from complex and imperfect details. X-RDMA simplifies the programming model, extends RDMA protocols for application awareness, and proposes mechanisms for resource management with thousands of connections per machine. It also reduces the work for administration and performance tuning with built-in tracing, tuning and monitoring tools.

X-RDMA has been deployed in several large-scale clusters with over 4000 servers in Alibaba cloud since 2016. It can save at least 70% development and maintenance time over RDMA, effectively improve performance and reduce network jitter especially when production servers are under pressure. It also helped locate over 30 issues in different layers of productions with over 5000 connections for each server on average.<br /><hr /><table class="cellpadding3px"><tr><td valign="top"><span style="white-space: nowrap">14:30-15:00</span></td><td valign="top"><b>Delay Propagation and Overlapping Mechanisms on Clusters: A Case Study of Idle Periods based on Workload, Communication, and Delay Granularity </b><br /><i>Ayesha Afzal, Georg Hager, Gerhard Wellein</i><br /></td></tr></table><br />Analytic, first-principles performance modeling of distributed-memory
applications is difficult due to a wide spectrum of random
disturbances caused by the application and the system.  These
disturbances (commonly called "noise") run contrary to the assumptions
about regularity that one usually employs when constructing simple
analytic models.  Despite numerous efforts to quantify, categorize,
and reduce such effects, a comprehensive quantitative understanding of
their performance impact is not available, especially for
long, one-off delays of execution periods
that have global consequences for the parallel application.
In this work, we
investigate various traces collected from synthetic
benchmarks that mimic real applications on simulated and real message-passing
systems in order to pinpoint the mechanisms behind delay propagation.
We analyze the dependence of the propagation speed of "idle
waves," i.e., propagating phases of inactivity,
emanating from injected delays
with respect to the execution and communication properties of the
application, study how such delays decay under increased noise levels,
and how they interact with each other.
We also show how fine-grained noise can make a system immune
against the adverse effects of propagating idle waves.
Our results contribute to a better understanding of the
collective phenomena that manifest themselves in distributed-memory
parallel applications.<br /><hr /><table class="cellpadding3px"><tr><td valign="top"><span style="white-space: nowrap">15:00-15:30</span></td><td valign="top"><b>An Empirical Study of Cryptographic Libraries for MPI Communications </b><br /><i>Abu Naser, Mohsen Gavahi, Cong Wu, Viet Tung Hoang, Zhi Wang, Xin Yuan</i><br /></td></tr></table><br />As High Performance Computing (HPC) applications with data security requirements are increasingly moving to execute in the public cloud, there is a demand that the cloud infrastructure for HPC should support privacy and integrity. Incorporating privacy and integrity mechanisms in the communication infrastructure of todays public cloud is challenging because recent advances in the networking infrastructure in data centers have shifted the communication bottleneck from the network links to the network end points and because encryption is computationally intensive.

In this work, we consider incorporating cryptographic schemes to support privacy and integrity in the Message Passing Interface (MPI) library, which is widely used in HPC applications. We empirically study four contemporary cryptographic libraries, OpenSSL, BoringSSL, Libsodium, and CryptoPP using micro-benchmarks and NAS parallel benchmarks to evaluate their overheads for securing the MPI library on two different networking technologies, 10Gbps Ethernet and 40Gbps InfiniBand. The results indicate that (1) different cryptographic libraries result in very different overheads in MPI communications, and (2) effectively supporting privacy and integrity in MPI communications running on high speed data center networks is challengingeven
with the most efficient cryptographic library, encryption can still introduce very significant overheads in MPI communications. Our study also indicates that although secure communication can introduce significant overhead for individual communication, the overall overhead may not be prohibitive for practical uses when there are multiple concurrent communications.<br /><hr /><br /><b>Efficient Storage<br />Room: Regal<br /></b><hr /><table class="cellpadding3px"><tr><td valign="top"><span style="white-space: nowrap">14:00-14:30</span></td><td valign="top"><b>RE-Store: Reliable and Efficient KV-Store with Erasure Coding and Replication </b><br /><i>Yuzhe Li, Jiang Zhou, Weiping Wang, Yong Chen</i><br /></td></tr></table><br />In-memory key/value store (KV-store) is a key building block for numerous applications running on a cluster. With the increase of cluster scale, efficiency and availability have become two critical demanding features. Traditional replication provides redundancy but is inefficient due to its high storage cost. Erasure coding can provide data reliability with significantly low storage requirements but primarily used for long-term archival data due to the limitation of write performance. Recent studies attempt to combine these two techniques, e.g. using replication
for frequently-updated metadata and using erasure coding for large, read-only data. In this study, we propose RE-Store, an in-memory key/value system with a novel, hybrid scheme of replication and erasure coding to achieve both efficiency and reliability. RE-Store introduces replication into erasure coding by making one copy for each encoded data and replacing partial parity with replicas for storage-efficiency. When failures occur, it uses replicas to ensure data availability, avoiding the inefficiency of erasure coding during repair. A fast online recovery is achieved
for fault tolerance at different failure scenarios, with little performance degradation. We have implemented RE-Store on a real key/value system and conducted extensive evaluations
to validate its design and to study its performance, efficiency, and reliability. Experimental results show that RE-Store has a similar performance with erasure coding and replication under
normal operations, yet saves 18% to 34% memory compared to replication when tolerating 2 to 4 failures.<br /><hr /><table class="cellpadding3px"><tr><td valign="top"><span style="white-space: nowrap">14:30-15:00</span></td><td valign="top"><b>Compact Filters for Fast Online Data Partitioning </b><br /><i>Qing Zheng, Charles Cranor, Ankush Jain, Gregory Ganger, Garth Gibson, George Amvrosiadis, Bradley Settlemyer, Gary Grider</i><br /></td></tr></table><br />We are approaching a point in time when it will be infeasible to catalog and query data after it has been generated. This trend has fueled research on in-situ data processing (i.e. operating on data as it is streamed to storage). One important example of this approach is in-situ data indexing. Prior work has shown the feasibility of indexing at scale as a two-step process: first by partitioning data by key across CPU cores, and then by having each core produce indexes on its subset as data is persisted. Online partitioning requires transferring data over the network so that it can be indexed and stored by the core responsible for the data. This approach is becoming increasingly costly as new computing platforms emphasize parallelism instead of individual core performance that is crucial for communication libraries and systems software in general. In addition to indexing, scalable online data partitioning is also useful in other contexts such as efficient compression and load balancing.

We present FilterKV, an efficient data management scheme for fast online data partitioning of key-value (KV) pairs. FilterKV reduces the total amount of data sent over the network and to storage. We achieve this by: (a) partitioning pointers to KV pairs instead of the KV pairs themselves, and (b) using a compact format to represent and store KV pointers. Results from LANL show that FilterKV can reduce total write slowdown (including partitioning overhead) by up to 3x across 4096 CPU cores.<br /><hr /><table class="cellpadding3px"><tr><td valign="top"><span style="white-space: nowrap">15:00-15:30</span></td><td valign="top"><b>Exploring Declustered Software RAID for Enhanced Reliability and Recovery Performance in HPC Storage Systems </b><br /><i>Zhi Qiao, Song Fu, Hsing-Bung Chen, Bradley Settlemyer</i><br /></td></tr></table><br />Redundant array of independent disks (RAID) has been widely used to address the reliability and performance issues of storage systems. As the scale of modern storage systems continuous growing, disk failure becomes the norm. With the ever-increasing disk capacity, RAID recovery based on disk rebuild becomes more and more costly, which causes significant performance degradation and even unavailability of storage systems. Declustered data layout enables parallel RAID reconstruction by shuffling data and parity blocks among all drives (including spares) in a RAID group. However, the reliability and performance of declustered RAID in real-world storage environments have not been thoroughly studied. With the popularity of ZFS file system and software RAID used in production data centers, in this paper, we extensively evaluate declustered RAID with regard to the RAID recovery time and I/O performance on an high performance storage platform at Los Alamos National Laboratory. Our empirical study reveals advantages and disadvantages of the declustered RAID technology. We qualitatively characterize the recovery performance of declustered RAID and compare with that of ZFS RAIDZ under various I/O workloads and access patterns. The experimental results show that the speedup of declustered RAID over traditional RAID is sub-linear to the parallelism of recovery I/O. Furthermore, we formally model and analyze the reliability of declustered RAID in terms of the mean-time-to-data-loss (MTTDL) and discover that the improved recovery performance leads to higher storage reliability compared with the traditional RAID.<br /><hr /><br /><br /><b style="font-size: 1.3em">Wednesday 25 September  15:30-16:00</b><hr /><br /><b>Break<br />Room: Wurlitzer<br /></b><hr /><br /><br /><b style="font-size: 1.3em">Wednesday 25 September  16:00-17:15</b><hr /><br /><b>Compression<br />Room: Regal<br /></b><hr /><table class="cellpadding3px"><tr><td valign="top"><span style="white-space: nowrap">16:00-16:25</span></td><td valign="top"><b>Analyzing the Impact of Lossy Compressor Variability on Checkpointing Scientific Simulations </b><br /><i>Pavlo D. Triantafyllides, Tasmia Reza, Jon C. Calhoun</i><br /></td></tr></table><br />Lossy compression algorithms are effective tools to reduce the size of high-performance computing data sets. As established lossy compressors such as SZ and ZFP evolve, they seek to improve the compression/decompression bandwidth and the compression ratio. Algorithm improvements may alter the spatial distribution of errors in the compressed data even when using the same error bound and error bound type. If HPC applications are to compute on lossy compressed data, application users require an understanding of how the performance and spatial distribution of error changes. We explore how spatial distributions of error, compression/decompression bandwidth, and compression ratio change for HPC data sets from the applications PlasComCM and Nek5000 between various versions of SZ and ZFP. In addition, we explore how the spatial distribution of error impacts application correctness when restarting from lossy compressed checkpoints. We verify that known approaches to selecting error tolerances for lossy compressed checkpointing are robust to compressor selection and in the face of changes in the distribution of error.<br /><hr /><table class="cellpadding3px"><tr><td valign="top"><span style="white-space: nowrap">16:25-16:50</span></td><td valign="top"><b>Improving Performance of Data Dumping with Lossy Compression for Scientific Simulation </b><br /><i>Xin Liang, Sheng Di, Dingwen Tao, Sihuan Li, Bogdan Nicolae, Zizhong Chen, Franck Cappello</i><br /></td></tr></table><br />Because of the ever-increasing  data being produced by today's high performance computing (HPC) scientific simulations, I/O performance is becoming a significant bottleneck for their executions. An efficient error-controlled lossy compressor is a promising solution to significantly reduce data writing time for scientific simulations running on supercomputers. In this paper, we explore how to optimize the data dumping performance for scientific simulation by leveraging error-bounded lossy compression techniques. The contributions of the paper are threefold. (1) We propose a novel I/O performance profiling model that can effectively represent the I/O performance with different execution scales and data sizes, and optimize the  estimation accuracy of data dumping performance using least square method. (2) We develop an adaptive lossy compression framework that can select the bestfit compressor (between two leading lossy compressors SZ and ZFP) with optimized parameter settings with respect to overall data dumping performance. (3) We evaluate our adaptive lossy compression framework with up to 32k cores on a supercomputer facilitated with fast I/O systems and using real-world scientific simulation datasets. Experiments show that our solution can mostly always lead the data dumping performance to the optimal level with very accurate selection of the bestfit lossy compressor and settings. The data dumping performance can be improved by up to 27% at different scales.<br /><hr /><table class="cellpadding3px"><tr><td valign="top"><span style="white-space: nowrap">16:50-17:15</span></td><td valign="top"><b>Efficient Distributed Graph Analytics using Triply Compressed Sparse Format </b><br /><i>Mohammad Hasanzadeh Mofrad, Rami Melhem, Yousuf Ahmad, Mohammad Hammoud</i><br /></td></tr></table><br />This paper presents Triply Compressed Sparse Column (TCSC), a novel compression technique designed specifically for matrix-vector operations where the matrix as well as the input and output vectors are sparse. We refer to these operations as SpMSpV$^2$. TCSC compresses the nonzero columns and rows of a highly sparse input matrix representing a large real-world graph. During this compression process, it encodes the sparsity patterns of the input and output vectors within the compressed representation of the sparse matrix itself. Consequently, it aligns the compressed indices of the input and output vectors with those of the compressed matrix columns and rows, thus eliminating the need for extra indirections when SpMSpV$^2$ operations access the vectors. This results in fewer cache misses, greater space efficiency and faster execution times. We evaluate TCSC's performance and show that it is more space and time efficient compared to CSC and DCSC, with up to $11 \times$ speedup. We integrate TCSC into GraphTap, our suggested linear algebra-based distributed graph analytics system. We compare GraphTap against GraphPad and LA3, two state-of-the-art linear algebra-based distributed graph analytics systems, using different dataset scales and numbers of processes. We demonstrate that GraphTap is up to $7\times$ faster than these systems due to TCSC and the resulting communication efficiency.<br /><hr /><br /><br /><b style="font-size: 1.3em">Wednesday 25 September  16:00-17:30</b><hr /><br /><b>Resource Allocation<br />Room: Ambassador/Registry<br /></b><hr /><table class="cellpadding3px"><tr><td valign="top"><span style="white-space: nowrap">16:00-16:22</span></td><td valign="top"><b>SMQoS: Improving Utilization and Power Efficiency with QoS Awareness on GPUs </b><br /><i>Qingxiao Sun, Yi Liu, Hailong Yang, Zhongzhi Luan, Depei Qian</i><br /></td></tr></table><br />Meeting the Quality of Service (QoS) requirement under task consolidation on the GPU is extremely challenging. Previous work mostly relies on static task or resource scheduling and cannot handle the QoS violation during runtime. In addition, the existing work fails to exploit the computing characteristics of batch tasks, and thus wastes the opportunities to reduce power consumption while improving GPU utilization. To address the above problems, we propose a new runtime mechanism SMQoS that can dynamically adjust the resource allocation during runtime to satisfy the QoS of latency-sensitive tasks and determine the optimal resource allocation for batch tasks to improve GPU utilization and power efficiency. The experimental results show that with SMQoS, 2.27% and 7.58% more task co-runnings reach the 95% QoS target than Spart and Rollover respectively. In addition, SMQoS achieves 23.9% and 32.3% higher throughput, and reduces the power consumption by 25.7% and 10.1%, compared to Spart and Rollover respectively.<br /><hr /><table class="cellpadding3px"><tr><td valign="top"><span style="white-space: nowrap">16:22-16:45</span></td><td valign="top"><b>Mitigating Inter-Job Interference via Process-Level Quality-of-Service </b><br /><i>Lee Savoie, David Lowenthal, Bronis de Supinski, Kathryn Mohror, Nikhil Jain</i><br /></td></tr></table><br />Jobs on most high-performance computing (HPC) systems share the network with other concurrently executing jobs. This sharing creates contention that can severely degrade performance. We investigate the use of Quality of Service (QoS) mechanisms to reduce the negative impacts of network contention. Our results show that careful use of QoS reduces the impact of contention for specific jobs, resulting in up to a 27% performance improvement. In some cases the impact of contention is completely eliminated. These improvements are achieved with limited negative impact to other jobs; any job that experiences performance loss typically degrades less than 5%, often much less. Our approach can help ensure that HPC machines maintain high throughput as per-node compute power continues to increase faster than network bandwidth.<br /><hr /><table class="cellpadding3px"><tr><td valign="top"><span style="white-space: nowrap">16:45-17:07</span></td><td valign="top"><b>Kube-Knots: Resource Harvesting through Dynamic Container Orchestration in GPU-based Datacenters </b><br /><i>Prashanth Thinakaran, Jashwant Raj Gunasekaran, Bikash Sharma, Mahmut Kandemir, Chita Das</i><br /></td></tr></table><br />Compute heterogeneity is increasingly gaining prominence in modern datacenters due to the addition of accelerators like GPUs and FPGAs. We observe that datacenter schedulers are agnostic of these emerging accelerators, especially their resource utilization footprints, and thus, not well equipped to dynamically provision them based on the application needs. We observe that the state-of-the-art datacenter schedulers fail to provide fine-grained resource guarantees for latency-sensitive tasks that are GPU-bound. Specifically for GPUs, this results in resource fragmentation and interference leading to poor utilization of allocated resources. Furthermore, GPUs exhibit highly linear energy efficiency with respect to utilization and hence proactive management of these resources is essential to keep the operational costs low while ensuring the end-to-end Quality of Service (QoS).

We build Knots, a GPU-aware resource orchestration layer and integrate it with the Kubernetes container orchestrator to build Kube-Knots. Kube-Knots can dynamically harvest spare compute cycles through dynamic container orchestration enabling co-location of latency-critical and batch workloads together while improving the overall resource utilization. We design and evaluate two GPU-based schedulers for datacenter-scale workloads on a ten node GPU cluster. Our proposed Correlation Based Prediction (CBP) and Peak Prediction (PP) schemes together improve both average and 99th percentile cluster-wide GPU utilization by up to 80%. This leads to 33% cluster-wide energy savings on an average for three different workloads compared to state-of-the-art GPU-agnostic schedulers. Further, the PP scheduler guarantees the end-to-end QoS for latency-critical GPU-bound queries by reducing QoS violations by up to 53% when compared to GPU-agnostic policies.<br /><hr /><table class="cellpadding3px"><tr><td valign="top"><span style="white-space: nowrap">17:07-17:30</span></td><td valign="top"><b>Scheduling Independent Stochastic Tasks on Heterogeneous Cloud Platforms </b><br /><i>Yiqin Gao, Louis-Claude Canon, Yves Robert, Frédéric Vivien</i><br /></td></tr></table><br />This work introduces scheduling strategies to  maximize the
expected number of independent tasks that can be executed on a cloud platform with budget and deadline constraints. The cloud platform is composed of several types of virtual machines (VMs), where each type has a unit execution cost that depends upon its characteristics. 
The amount of budget spent during the execution of a task on a given VM  is the product of its execution length by the unit execution cost of that VM. The execution lengths of  tasks 
follow a variety of standard probability distributions (exponential, uniform, half-normal, lognormal, gamma, inverse-gamma and Weibull) whose mean and 
standard deviation both depend upon the VM type. Finally, there is a global available budget
and a deadline constraint,
and the goal is to successfully execute as many tasks as possible before the deadline is reached or the budget is exhausted (whichever comes first).
On each VM,
the scheduler can decide at any instant to interrupt the execution of a (long) running task 
and to launch a new one,
but the budget already spent for the interrupted task is lost. 
The main
questions are which\VMs to enroll, and whether and when to
interrupt tasks that have been executing for some time.
We assess the complexity of the problem by showing its NP-completeness and providing a 2-approximation for the asymptotic case where budget and deadline both tend to infinity.
Then we introduce several heuristics and compare their performance by  running 
an extensive set of simulations.<br /><hr /><br /><br /><b style="font-size: 1.3em">Wednesday 25 September  18:30-22:00</b><hr /><br /><b>Social Event<br />Room: Ambassador/Registry<br /></b><hr /><br /><br /><b style="font-size: 1.3em">Thursday 26 September  8:45-9:00</b><hr /><br /><b>Cluster 2020 Presentation<br />Room: Ambassador/Registry<br /></b><hr /><br /><br /><b style="font-size: 1.3em">Thursday 26 September  9:00-10:00</b><hr /><br /><b>Keynote 3<br />Room: Ambassador/Registry<br /></b><hr /><br /><br /><b style="font-size: 1.3em">Thursday 26 September  10:00-10:30</b><hr /><br /><b>Top 3 Papers of Cluster 2019 (3/3)<br />Room: Ambassador/Registry<br /></b><hr /><table class="cellpadding3px"><tr><td valign="top"><span style="white-space: nowrap">10:00-10:30</span></td><td valign="top"><b>STASH : Fast Hierarchical Aggregation Queries for Effective Visual Spatiotemporal Explorations </b><br /><i>Saptashwa Mitra, Paahuni Khandelwal, Shrideep Pallickara, Sangmi Lee Pallickara</i><br /></td></tr></table><br />The proliferation of spatiotemporal datasets has lead to a demand for scalable real-time analytics over these datasets to help scientists make inferences and inform decision-making. However, the data is voluminous and combined with incessant user queries adversely impacts the latency of the visualization queries over such datasets stored over large clusters.

In this paper, we introduce STASH, a distributed in-memory cache for hierarchical aggregation query evaluation.  STASH is a middleware which can be loaded on top of a distributed spatiotemporal file system.  Users perform queries from a front-end lightweight visualization interface and the evaluations occur over the back-end storage system housing the raw data over which summarization and subsequent visualization are performed. STASH facilitates fast exploratory analytics by caching relevant past query results based on their frequency and freshness to assist similar, future queries and avoid expensive disk I/O and network usage, thus reducing their latency. Additionally, STASH also handles any hotspot that might result from a spike in user requests due to the spatial and temporal locality of their access patterns.

Our empirical benchmarks show that a STASH-enabled system reduces query latency of a basic system by over 5 folds and brings it down to interactive speed even for large country-sized spatial queries. We have contrasted STASH with existing cache-enabled analytics engines, such as ElasticSearch, and it shows that our STASH-enabled system reduces the aggregation query latency up to ~71%. STASH also alleviates skewed workloads through its dynamic replication scheme and improves throughput by ~40% in hotspot scenarios.<br /><hr /><br /><br /><b style="font-size: 1.3em">Thursday 26 September  10:30-11:00</b><hr /><br /><b>Break<br />Room: Wurlitzer<br /></b><hr /><br /><br /><b style="font-size: 1.3em">Thursday 26 September  11:00-12:30</b><hr /><br /><b>Tools and Optimization<br />Room: Ambassador/Registry<br /></b><hr /><table class="cellpadding3px"><tr><td valign="top"><span style="white-space: nowrap">11:00-11:30</span></td><td valign="top"><b>DiffTrace: Efficient Whole-Program Trace Analysis and Diffing for Debugging </b><br /><i>Saeed Taheri, Ian Briggs, Martin Burtscher, Ganesh Gopalakrishnan</i><br /></td></tr></table><br />We present a tool DiffTrace that approaches debugging
via whole program tracing and diffing of normal and
erroneous traces. After collecting these traces, a user-configurable
front-end filters out irrelevant function calls, and then summarizes
loops of executions of retained function calls based on state-of-
the-art loop extraction algorithms. Information about these
loops is inserted into concept lattices, which helps computes
salient dissimilarities to help narrow down bugs. DiffTrace is
a clean start that addresses many debugging features missing
in existing approaches. Our experiments on a heterogeneous
MPI/OpenMP program called ILCS and initial measurements
on LULESH, a DOE mini-app, helps demonstrate the advantages
of the proposed debugging approach.<br /><hr /><table class="cellpadding3px"><tr><td valign="top"><span style="white-space: nowrap">11:30-12:00</span></td><td valign="top"><b>FSMonitor: Scalable File System Monitoring for Arbitrary Storage Systems </b><br /><i>Arnab K. Paul, Ryan Chard, Kyle Chard, Steven Tuecke, Ali R. Butt, Ian Foster</i><br /></td></tr></table><br />Data automation, monitoring, and management tools are reliant on being able to detect, report, and respond to file system events. Various data event reporting tools exist for specific operating systems and storage devices, such as inotify for Linux, kqueue for BSD, and FSEvents for macOS. However, these tools are not designed to monitor distributed file systems. Indeed, many cannot scale to monitor many thousands of directories, or simply cannot be applied to distributed file systems. Moreover, each tool implements a custom API and event representation, making the development of generalized and portable event-based applications challenging. As file systems grow in size and become increasingly diverse, there is a need for scalable monitoring solutions that can be applied to a wide range of both distributed and local systems. We present here a generic and scalable file system monitor and event reporting tool, FSMonitor, that provides a file-system-independent event representation and event capture interface. FSMonitor uses a modular Data Storage Interface (DSI) architecture to enable the selection and application of appropriate event monitoring tools to detect and report events from a target file system, and implements efficient and fault-tolerant mechanisms that can detect and report events even on large file systems. We describe and evaluate DSIs for common UNIX, macOS, and Windows storage systems, and for the Lustre distributed file system. Our experiments on a 897 TB Lustre file system show that FSMonitor can capture and process almost 38000 events per second.<br /><hr /><table class="cellpadding3px"><tr><td valign="top"><span style="white-space: nowrap">12:00-12:30</span></td><td valign="top"><b>On the Benefits of Anticipating Load Imbalance for Performance Optimization of Parallel Applications </b><br /><i>Anthony Boulmier, Franck Raynaud, Nabil Abdennadher, Bastien Chopard</i><br /></td></tr></table><br />In parallel iterative applications, computational efficiency is essential for addressing large problems. Load imbalance is one of the major performance degradation factors of parallel applications. Therefore, distributing, cleverly, and as evenly as possible, the workload among processing elements (PE) maximizes application performance. So far, the standard load balancing method consists in distributing the workload evenly between PEs and, when load imbalance appears, redistributing the extra load from overloaded PEs to underloaded PEs. However, this does not anticipate the load imbalance growth that may continue during the next iterations. In this paper, we present a first step toward a novel philosophy of load balancing that unloads the PEs that will be overloaded in the near future to let the application rebalance itself via its own dynamics. 
Herein, we present a formal definition of our new approach using a simple mathematical model and discuss its advantages compared to the standard load balancing method. In addition to the theoretical study, we apply our method to an application that reproduces the computation of a fluid model with non-uniform erosion. The performance validates the benefit of anticipating load imbalance. We observed up to 16% performance improvement compared to the standard load balancing method.<br /><hr /><br /><b>Applications<br />Room: Regal<br /></b><hr /><table class="cellpadding3px"><tr><td valign="top"><span style="white-space: nowrap">11:00-11:30</span></td><td valign="top"><b>Multi-physics simulations of particle tracking in arterial geometries with a scalable moving window algorithm </b><br /><i>Gregory J. Herschlag, John Gounley, Sayan Roychowdhury, Erik W. Draeger, Amanda Randles</i><br /></td></tr></table><br />In arterial systems, cancer cell trajectories determine metastatic cancer locations; similarly, particle trajectories determine drug delivery distribution. Predicting trajectories is challenging, as the dynamics are affected by local interactions with red blood cells, complex hemodynamic flow structure, and downstream factors such as stenoses or blockages. Direct simulation is not possible, as even a single simulation of a large arterial domain with explicit red blood cells is currently intractable on even the largest supercomputers. To overcome this limitation, we present a multi-physics adaptive window algorithm, in which individual red blood cells are explicitly modeled in a small region of interest moving through a coupled arterial fluid domain. We describe the coupling between the window and fluid domains, including automatic insertion and deletion of explicit cells and dynamic tracking of cells of interest by the window. We show that this algorithm scales efficiently on heterogeneous architectures and enables us to perform large, highly-resolved particle-tracking simulations that would otherwise be intractable.<br /><hr /><table class="cellpadding3px"><tr><td valign="top"><span style="white-space: nowrap">11:30-12:00</span></td><td valign="top"><b>Fast and Scalable Implementations of Influence Maximization Algorithms </b><br /><i>Marco Minutoli, Mahantesh Halappanavar, Ananth Kalyanaraman, Arun Sathanur, Ryan Mcclure, Jason McDermott</i><br /></td></tr></table><br />The Influence Maximization problem has been extensively studied in the past
  decade because of its practical applications in finding the key influencers in
  social networks.  Due to the hardness of the underlying problem, existing
  algorithms have tried to trade off practical efficiency with approximation
  guarantees.  However, approximate solutions take several hours of compute time
  on modest sized real world inputs. On the other hand, there is a lack of
  effective parallel and distributed algorithms to solve this problem. In this
  paper, we present efficient parallel algorithms for multithreaded and
  distributed systems to solve the influence maximization with approximation
  guarantee. Our algorithms extend state-of-the-art sequential approach based on
  computing reverse reachability sets.  We present a detailed experimental
  evaluation, and analyze their performance and their sensitivity to input
  parameters, using real world inputs.  Our experimental results demonstrate
  significant speedup on parallel architectures. We further show a speedup of up
  to 586X relative to the state-of-the-art sequential baseline using
  1024 nodes of a supercomputer at far greater accuracy and twice the seed set
  size.  To the best of our knowledge, this is the first effort in parallelizing
  the influence maximization operation at scale.<br /><hr /><table class="cellpadding3px"><tr><td valign="top"><span style="white-space: nowrap">12:00-12:30</span></td><td valign="top"><b>Scalable, High-Order Continuity Across Block Boundaries of   Functional Approximations Computed in Parallel </b><br /><i>Iulian Grindeanu, Tom Peterka, Vijay Mahadevan, Youssef S. Nashed</i><br /></td></tr></table><br />We investigate the representation of discrete scientific data with a $C^k$-continuous functional
 model, where $C^k$ denotes k-th-order continuity, in a distributed-memory parallel setting.
 The MFA  Multivariate Functional Approximation  model is a piecewise-continuous functional
approximation based on multivariate high-dimensional B-splines. When computing
 an MFA approximation in parallel over multiple blocks in a spatial domain
decomposition, the interior of each block will be $C^k$-continuous,
k being the B-spline polynomial degree, but discontinuities exist across
neighboring block boundaries. We present an efficient and scalable solution
to ensure $C^k$ continuity across blocks, by blending neighboring approximations.
We show that after decomposing the domain in structured, overlapping
blocks and approximating blocks independently to high degrees of accuracy,
 we can extend the local solution to the global domain by using compact,
multidimensional smooth-step functions. We prove that this approach,
which can be viewed as an extended partition of unity approximation method,
is highly scalable on modern architectures.<br /><hr /><br /><br /><b style="font-size: 1.3em">Thursday 26 September  12:30-12:45</b><hr /><br /><b>Closing Remarks<br />Room: Ambassador/Registry<br /></b><hr /></div></div>
</td></tr></table>
			
    </article>
  </div> <!-- /.medium-8.columns -->
</div><!-- /.row -->


	
	    <div id="up-to-top" class="row">
      <div class="small-12 columns" style="text-align: right;">
        <a class="iconfont" href="#top-of-page">&#xf108;</a>
      </div><!-- /.small-12.columns -->
    </div><!-- /.row -->


    <footer id="footer-content" class="bg-grau">
      <div id="footer">
        <div class="row">
          <div class="medium-6 large-5 columns">
            <h5 class="shadow-black">About This Site</h5>

            <p class="shadow-black">
              Website for IEEE Cluster 2019, September 2019, Albuquerque, New
              Mexico USA<br>
              <a href="https://twitter.com/ieeecluster"><img src="/2019/images/Twitter_Social_Icon_Rounded_Square_Color.png" width="30"></a>
            </p>
            <p>Image Credit: Ron Behrmann</p>
          </div><!-- /.large-6.columns -->


          <div class="small-6 medium-3 large-3 large-offset-1 columns">
            
              
                <h5 class="shadow-black">Services</h5>
              
            
              
            
              
            
              
            
              
            

              <ul class="no-bullet shadow-black">
              
                
                  <li >
                    <a href=""  title=""></a>
                  </li>
              
                
                  <li >
                    <a href="/2019/contact/"  title="Contact">Contact</a>
                  </li>
              
                
                  <li >
                    <a href="/2019/feed.xml"  title="Subscribe to RSS Feed">RSS</a>
                  </li>
              
                
                  <li >
                    <a href="/2019/atom.xml"  title="Subscribe to Atom Feed">Atom</a>
                  </li>
              
                
                  <li >
                    <a href="/2019/sitemap.xml"  title="Sitemap for Google Webmaster Tools">sitemap.xml</a>
                  </li>
              
              </ul>
          </div><!-- /.large-4.columns -->


          <div class="small-6 medium-3 large-3 columns">
            
              
                <h5 class="shadow-black">Related Links</h5>
              
            
              
            
              
            
              
            
              
            

            <ul class="no-bullet shadow-black">
            
              
                <li >
                  <a href=""  title=""></a>
                </li>
            
              
                <li class="network" >
                  <a href="http://www.ieee.org/" target="_blank"  title="Institute of Electrical and Electronics Engineers">IEEE Home</a>
                </li>
            
              
                <li class="network" >
                  <a href="http://cluster2018.github.io" target="_blank"  title="IEEE Cluster 2018">IEEE Cluster 2018</a>
                </li>
            
              
                <li class="network" >
                  <a href="https://clustercomp.org/committee/Committees.html" target="_blank"  title="IEEE Cluster Steering Committee">IEEE Cluster Steering Committee</a>
                </li>


                <li class="network" >
                  <a href="https://clustercomp.org" target="_blank"  title="IEEE Cluster Conference Series">IEEE Cluster Conference Series</a>
                </li>
            
              
                <li class="network" >
                  <a href="http://www.computer.org" target="_blank"  title="The Community for Technology Leaders">IEEE Computer Society</a>
                </li>

                <li class="network" >
                  <a href="/2019/ieee-policy-statements" target="_blank" title="IEEE Policy Statements">
                  IEEE Policy Statements
                  </a>
                </li>
            
            </ul>
          </div><!-- /.large-3.columns -->
        </div><!-- /.row -->

      </div><!-- /#footer -->

    </footer>

	

	


<script src="https://clustercomp.org/2019/assets/js/javascript.min.js"></script>



<script>
    $("#masthead").backstretch("/2019/images/albuquerque.jpg", {fade: 700});
    $("#masthead-with-text").backstretch("/2019/images/albuquerque.jpg", {fade: 700});
</script>







</body>
</html>

