BEGIN:VCALENDAR
VERSION:2.0
PRODID:Linklings LLC
BEGIN:VTIMEZONE
TZID:Europe/London
X-LIC-LOCATION:Europe/London
BEGIN:DAYLIGHT
TZNAME:BST
DTSTART:19710101T010000
TZOFFSETFROM:+0000
TZOFFSETTO:+0100
RRULE:FREQ=YEARLY;BYMONTH=3;BYDAY=-1SU
END:DAYLIGHT
BEGIN:STANDARD
TZNAME:GMT
DTSTART:19710101T020000
TZOFFSETFROM:+0100
TZOFFSETTO:+0000
RRULE:FREQ=YEARLY;BYMONTH=10;BYDAY=-1SU
END:STANDARD
END:VTIMEZONE
BEGIN:VEVENT
DTSTAMP:20180913T134851Z
LOCATION:Assembly Hall
DTSTART;TZID=Europe/London:20180913T143000
DTEND;TZID=Europe/London:20180913T150000
UID:ieeecluster_IEEE Cluster 2018_sess125_pap277@linklings.com
SUMMARY:swCaffe: a Parallel Framework for Accelerating Deep Learning Appli
 cations on Sunway TaihuLight
DESCRIPTION:swCaffe: a Parallel Framework for Accelerating Deep Learning A
 pplications on Sunway TaihuLight\n\nLi, Fang, Fu...\n\n\nThis paper report
 s our efforts on swCaffe, a high-efficient parallel framework for accelera
 ting deep neural networks (DNNs) training on Sunway TaihuLight, one of the
  fastest supercomputers in the world that adopts a unique heterogeneous ma
 ny-core architecture. First, we point out some insightful principles to fu
 lly exploit the performance of the innovative many-core architecture. Seco
 nd, we propose a set of optimization strategies for redesigning a variety 
 of neural network layers based on Caffe. Third, we put forward a topology-
 aware parameter synchronization scheme to scale the synchronous Stochastic
  Gradient Descent (SGD) method to multiple processors efficiently. We eval
 uate our framework by training a variety of widely used neural networks wi
 th the ImageNet dataset. On a single node, swCaffe can achieve 23% ̃1
 19% overall performance compared with Caffe running on K40m GPU. As compar
 ed with Caffe on CPU, swCaffe runs 3.04 ̃7.84× faster on all networks
 . When training ResNet50 and AlexNet with 1024 nodes, swCaffe can achieve 
 up to 715.45× and 928.15× speedup.
END:VEVENT
END:VCALENDAR

