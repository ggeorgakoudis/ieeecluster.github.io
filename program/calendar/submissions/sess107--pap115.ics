BEGIN:VCALENDAR
VERSION:2.0
PRODID:Linklings LLC
BEGIN:VTIMEZONE
TZID:Europe/London
X-LIC-LOCATION:Europe/London
BEGIN:DAYLIGHT
TZNAME:BST
DTSTART:19710101T010000
TZOFFSETFROM:+0000
TZOFFSETTO:+0100
RRULE:FREQ=YEARLY;BYMONTH=3;BYDAY=-1SU
END:DAYLIGHT
BEGIN:STANDARD
TZNAME:GMT
DTSTART:19710101T020000
TZOFFSETFROM:+0100
TZOFFSETTO:+0000
RRULE:FREQ=YEARLY;BYMONTH=10;BYDAY=-1SU
END:STANDARD
END:VTIMEZONE
BEGIN:VEVENT
DTSTAMP:20180913T134850Z
LOCATION:Assembly Hall
DTSTART;TZID=Europe/London:20180911T153000
DTEND;TZID=Europe/London:20180911T160000
UID:ieeecluster_IEEE Cluster 2018_sess107_pap115@linklings.com
SUMMARY:Whole Program Generation of Massively Parallel Shallow Water Equat
 ion Solvers
DESCRIPTION:Whole Program Generation of Massively Parallel Shallow Water E
 quation Solvers\n\nKuckuk, KÃ¶stler\n\n\nThe study of ocean currents has be
 en an active area of research for decades.\nAs a model close to the water 
 surface, the shallow water equations (SWE) can be used.\nFor realistic sim
 ulations, efficient numerical solvers are necessary that exhibit a good no
 de-level performance while still maintaining scalability.\nWhen comparing 
 the discretized model and the actual implementation, one often finds that 
 they differ vastly.\nThis gap makes it hard for domain experts to implemen
 t their models and high performance computing (HPC) experts are required t
 o ensure an optimal implementation.\nUsing domain-specific languages (DSLs
 ) and code generation techniques can be a useful tool to bridge this gap.\
 nIn recent years, ExaStencils and its DSL ExaSlang have proven to provide 
 a suitable platform for this.\nWe present an extension from up to now elli
 ptic to hyperbolic partial differential equations (PDEs) in this work, nam
 ely the SWE. After setting up a suitable discretization, we demonstrate ho
 w it can be mapped to ExaSlang code.\nThis code is still quite similar to 
 the original, mathematically motivated specification and can be easily wri
 tten by domain experts.\nStill, solvers generated from this abstract repre
 sentation can be run on large-scale clusters.\nWe demonstrate this by givi
 ng performance and scalability results on the state-of-the-art GPU cluster
  Piz Daint where we solve for close to a trillion unknowns on 2048 GPUs.\n
 From there, we discuss the performance impact of different optimizations s
 uch as overlapping computation and communication, or switching to a hybrid
  CPU-GPU parallelization scheme.
END:VEVENT
END:VCALENDAR

