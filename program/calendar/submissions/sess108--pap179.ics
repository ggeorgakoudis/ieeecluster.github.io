BEGIN:VCALENDAR
VERSION:2.0
PRODID:Linklings LLC
BEGIN:VTIMEZONE
TZID:Europe/London
X-LIC-LOCATION:Europe/London
BEGIN:DAYLIGHT
TZNAME:BST
DTSTART:19710101T010000
TZOFFSETFROM:+0000
TZOFFSETTO:+0100
RRULE:FREQ=YEARLY;BYMONTH=3;BYDAY=-1SU
END:DAYLIGHT
BEGIN:STANDARD
TZNAME:GMT
DTSTART:19710101T020000
TZOFFSETFROM:+0100
TZOFFSETTO:+0000
RRULE:FREQ=YEARLY;BYMONTH=10;BYDAY=-1SU
END:STANDARD
END:VTIMEZONE
BEGIN:VEVENT
DTSTAMP:20180913T134851Z
LOCATION:Minor Hall
DTSTART;TZID=Europe/London:20180911T160000
DTEND;TZID=Europe/London:20180911T163000
UID:ieeecluster_IEEE Cluster 2018_sess108_pap179@linklings.com
SUMMARY:Applying Pwrake Workflow System and Gfarm File System to Telescope
  Data Processing
DESCRIPTION:Applying Pwrake Workflow System and Gfarm File System to Teles
 cope Data Processing\n\nTanaka, Tatebe, Kawashima\n\n\nIn this paper, we d
 escribe a use case applying a scientific workflow system and a distributed
  file system to improve the performance of telescope data processing. The 
 application is pipeline processing of data generated by Hyper Suprime-Cam 
 (HSC) which is a focal plane camera mounted on the Subaru telescope. In th
 is paper, we focus on the scalability of parallel I/O and core utilization
 . The IBM Spectrum Scale (GPFS) used for actual operation has a limit on s
 calability due to the configuration using storage servers. Therefore, we i
 ntroduce the Gfarm file system which uses the storage of the worker node f
 or parallel I/O performance. To improve core utilization, we introduce the
  Pwrake workflow system instead of the parallel processing framework devel
 oped for the HSC pipeline. Descriptions of task dependencies are necessary
  to further improve core utilization by overlapping different types of tas
 ks. We discuss the usefulness of the workflow description language with th
 e function of scripting language for defining complex task dependency. In 
 the experiment, the performance of the pipeline is evaluated using a quart
 er of the observation data per night (input files: 80 GB, output files: 1.
 2 TB). Measurements on strong scaling from 48 to 576 cores show that the p
 rocessing with Gfarm file system is more scalable than that with GPFS. Mea
 surement using 576 cores shows that our method improves the processing spe
 ed of the pipeline by 2.2 times compared with the method used in actual op
 eration.
END:VEVENT
END:VCALENDAR

