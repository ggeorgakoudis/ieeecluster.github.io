<html>

<head>
<meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
<meta name="GENERATOR" content="Microsoft FrontPage 4.0">
<meta name="ProgId" content="FrontPage.Editor.Document">
<title>Cluster2001: Abstracts</title>
<link rel="stylesheet" type="text/css" href="../../cluster2001.css">
</head>

<body>

<h1>Stingray: Cone tracing using a software DSM for SCI clusters &nbsp;</h1>
<p><b>      
      Emmanuel Cecchet&nbsp;<br>
</b>Alexandre Meyer<b>      <br>
</b><a href="mailto:Alexandre.Meyer@imag.fr">Alexandre.Meyer@imag.fr</a>&nbsp;<b>      <br>
<br>
INRIA</b><br>
<br>
In this paper we compare the efficiency of a supercomputer with<br>
hardware shared memory with that of a cluster of workstations using<br>
software Distributed Shared Memory (DSM). The difference in performance<br>
is studied by running ray tracing applications on both architectures.<br>
We have ported Stingray, a parallel cone tracer developed on a SGI<br>
Origin 2000 supercomputer, on a cluster interconnected by a Scalable<br>
Coherent Interface (SCI) network and a software DSM called SciFS.  We<br>
present the implementation issues and compare the results obtained with<br>
each architecture. Further we discuss the trade­off -<br>
price/performance/programming ease - of both architectures.  We show<br>
using Stingray that a modest 12 nodes SCI cluster with an efficient<br>
software DSM is 5 times cheaper and can perform up to 2.3 times better<br>
than a SGI Origin 2000 with 6 processors. We therefore consider that<br>
software DSM is well suited for this kind of application and provides<br>
both ease in programming and scalable performance.</p>
<p><a href="../index.html">Back to Program</a></p>

</body>

</html>
